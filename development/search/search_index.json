{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"continuiti Learning function operators with neural networks. <p>continuiti is a Python package for deep learning on function operators with a focus on elegance and generality. It provides a unified interface for neural operators (such as DeepONet or FNO) to be used in a plug and play fashion. As operator learning is particularly useful in scientific machine learning, continuiti also includes physics-informed loss functions and a collection of relevant benchmarks.</p> <p>Tutorials</p><p>Getting started with continuiti</p> <p>How-to Guides</p><p>Solve your problem</p> <p>Background</p><p>More details on operator learning</p> <p>Reference</p><p>API documentation</p>"},{"location":"CHANGELOG/","title":"CHANGELOG","text":""},{"location":"CHANGELOG/#020","title":"0.2.0","text":"<ul> <li>Add <code>Attention</code> base class, <code>MultiHeadAttention</code>, and <code>ScaledDotProductAttention</code> classes.</li> <li>Add <code>branch_network</code> and <code>trunk_network</code> arguments to <code>DeepONet</code> to allow for custom network architectures.</li> <li>Add <code>MaskedOperator</code> base class.</li> <li>Add <code>DeepCatOperator</code>.</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Move all content of <code>__init__.py</code> files to sub-modules.</li> <li>Add <code>Trainer</code> class to replace <code>operator.fit</code> method.</li> <li>Implement <code>BelNet</code>.</li> <li>Add <code>Sampler</code>, <code>BoxSampler</code>, <code>UniformBoxSampler</code>, and <code>RegularGridSampler</code> classes.</li> <li>Moved <code>DataLoader</code> into the <code>fit</code> method of the <code>Trainer</code>.   Therefore, <code>Trainer.fit</code> expects an <code>OperatorDataset</code> now.</li> <li>A <code>Criterion</code> now enables stopping the training loop.</li> <li>The <code>plotting</code> module has been removed.</li> <li>Add <code>timeseries.ipynb</code> example.</li> <li>Add <code>Function</code>, <code>FunctionSet</code>, and <code>FunctionOperatorDataset</code> classes.</li> <li>Add <code>function.ipynb</code> example.</li> <li>Add <code>Benchmark</code> base class.</li> <li>Add <code>SineBenchmark</code>.</li> <li>Implement <code>DeepNeuralOperator</code>.</li> <li>Generalize <code>NeuralOperator</code> to take a list of operators.</li> <li>The <code>data.DatasetShapes</code> class becomes <code>operators.OperatorShapes</code> without <code>num_observations</code> attribute.</li> <li>Change <code>torch</code> dependency from \"==2.1.0\" to \"&gt;=2.1.0,&lt;3.0.0\".</li> <li>Change <code>optuna</code> dependency from \"3.5.0\" to \"&gt;=3.5.0,&lt;4.0.0\".</li> <li>Add <code>FourierLayer</code> and <code>FourierNeuralOperator</code> with example.</li> <li>Add <code>benchmarks</code> infrastructure.</li> <li>An <code>Operator</code> now takes a <code>device</code> argument.</li> <li>Add <code>QuantileScaler</code> class.</li> </ul>"},{"location":"CHANGELOG/#000-2024-02-22","title":"0.0.0 (2024-02-22)","text":"<ul> <li>Set up project structure.</li> <li>Implement basic functionality.</li> <li>Build documentation.</li> <li>Create first notebooks.</li> <li>Introduce neural operators.</li> <li>Add CI/CD.</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>continuiti aims to be a repository of architectures and benchmarks for operator learning with neural networks and its applications.</p> <p>Contributions are welcome from anyone in the form of pull requests, bug reports and feature requests.</p>"},{"location":"CONTRIBUTING/#local-development","title":"Local development","text":"<p>In order to contribute to the library, you will need to set up your local development environment. First, clone the repository:</p> <pre><code>git clone https://github.com/aai-institute/continuiti.git\ncd continuiti\n</code></pre>"},{"location":"CONTRIBUTING/#setting-up-your-environment","title":"Setting up your environment","text":"<p>We strongly suggest using some form of virtual environment for working with the library, e.g., with venv:</p> <pre><code>python3 -m venv ./venv\nsource venv/bin/activate\n</code></pre>"},{"location":"CONTRIBUTING/#installing-in-editable-mode","title":"Installing in editable mode","text":"<p>A very convenient way of working with your library during development is to install it in editable mode into your environment by running:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>The <code>[dev]</code> extra installs all dependencies needed for development, including testing, documentation and benchmarking.</p>"},{"location":"CONTRIBUTING/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>This project uses black to format code and pre-commit to invoke it as a git pre-commit hook.</p> <p>Run the following to set up the pre-commit git hook to run before pushes:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"CONTRIBUTING/#build-documentation","title":"Build documentation","text":"<p>API documentation is built with mkdocs. Notebooks are an integral part of the documentation as well.</p> <p>You can use this command to continuously rebuild documentation on changes to the <code>docs</code> and <code>src</code> folder:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will rebuild the documentation on changes to <code>.md</code> files inside <code>docs</code>, notebooks and python files.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>Automated builds, tests, generation of documentation and publishing are handled by CI pipelines. Before pushing your changes to the remote we recommend to execute <code>pytest</code> locally in order to detect mistakes early on and to avoid failing pipelines.</p> <p>To run all tests, use: <pre><code>pytest\n</code></pre></p> <p>To run specific tests, use: <pre><code>pytest -k test_pattern\n</code></pre></p> <p>Slow tests (&gt; 5s) are marked by the <code>@pytest.mark.slow</code> decorator. To run all tests except the slow ones, use:</p> <pre><code>pytest -m \"not slow\"\n</code></pre>"},{"location":"CONTRIBUTING/#notebooks","title":"Notebooks","text":"<p>We use notebooks both as documentation and as integration tests. Because we want documentation to include the full dataset, we commit notebooks with their outputs running with full datasets to the repo.</p>"},{"location":"CONTRIBUTING/#hiding-cells-in-notebooks","title":"Hiding cells in notebooks","text":"<p>You can isolate boilerplate code into separate cells which are then hidden in the documentation. In order to do this, cells are marked with tags understood by the mkdocs plugin <code>mkdocs-jupyter</code>, namely adding the following to the metadata of the relevant cells:</p> <pre><code>\"tags\": [\n\"hide\"\n]\n</code></pre> <p>To hide the cell's input and output.</p> <p>Or:</p> <pre><code>\"tags\": [\n\"hide-input\"\n]\n</code></pre> <p>To only hide the input and</p> <p><pre><code>\"tags\": [\n\"hide-output\"\n]\n</code></pre> for hiding the output only.</p> <p>If a cell should be skipped in CI (e.g. because the full data set is missing), you can use:</p> <pre><code>\"tags\": [\n\"skip-execution\"\n]\n</code></pre>"},{"location":"CONTRIBUTING/#plots-in-notebooks","title":"Plots in Notebooks","text":"<p>If you add a plot to a notebook, which should also render nicely in browser dark mode, add the tag invertible-output, i.e.</p> <p><pre><code>\"tags\": [\n\"invertible-output\"\n]\n</code></pre> This applies a simple CSS-filter to the output image of the cell.</p>"},{"location":"CONTRIBUTING/#release-process","title":"Release process","text":"<p>In order to create a new release, make sure that the project's venv is active and the repository is clean and on the main branch.</p> <p>Create a new release using the script <code>build_scripts/release.sh</code>. This script will create a release tag on the repository and bump the version number:</p> <pre><code>./build_scripts/release.sh\n</code></pre> <p>Afterwards, create a GitHub release for that tag. That will a trigger a CI pipeline that will automatically create a package and publish it from CI to PyPI.</p>"},{"location":"api/continuiti/","title":"API","text":"<p>continuiti is a Python package for machine learning on function operators.</p> <p>The package is structured into the following modules:</p> <p>Benchmarks</p><p>Benchmarks for testing operator architectures.</p> <p>Data</p><p>Data sets and data utility functions.</p> <p>Discrete</p><p>Discretization utilities like samplers.</p> <p>Networks</p><p>Neural network implementations.</p> <p>Operators</p><p>Neural operator implementations.</p> <p>PDE</p><p>Loss functions for physics-informed training.</p> <p>Trainer</p><p>Default training loop for operator models.</p> <p>Transforms</p><p>Transformations for operator inputs/outputs.</p>"},{"location":"api/continuiti/benchmarks/","title":"Benchmarks","text":"<p><code>continuiti.benchmarks</code></p> <p>Benchmarks for operator learning.</p>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.Benchmark","title":"<code>Benchmark(train_dataset, test_dataset, losses=lambda: [MSELoss()]())</code>  <code>dataclass</code>","text":"<p>Benchmark class.</p> <p>A Benchmark object encapsulates two distinct datasets: a train and a test dataset. The training dataset is used to train an operator to fit the dataset. The test dataset, in contrast, is utilized solely for evaluating the performance. The evaluation is done by measuring the loss on the test set.</p>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.Benchmark.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the benchmark.</p> Source code in <code>src/continuiti/benchmarks/benchmark.py</code> <pre><code>def __str__(self):\n\"\"\"Return string representation of the benchmark.\"\"\"\nreturn self.__class__.__name__\n</code></pre>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.SineRegular","title":"<code>SineRegular()</code>","text":"<p>             Bases: <code>SineBenchmark</code></p> <p>Sine benchmark with the domain and co-domain sampled on a regular grid.</p> <p>The <code>SineRegular</code> benchmark is a <code>SineBenchmark</code> with the following properties:</p> <ul> <li><code>n_sensors</code> is 32.</li> <li><code>n_evaluations</code> is 32.</li> <li><code>n_train</code> is 1024.</li> <li><code>n_test</code> is 1024.</li> <li><code>uniform</code> is <code>False</code>.</li> </ul> <p> Visualizations of a few samples. </p> Source code in <code>src/continuiti/benchmarks/sine.py</code> <pre><code>def __init__(self):\nsuper().__init__(\nn_sensors=32,\nn_evaluations=32,\nn_train=1024,\nn_test=1024,\nuniform=False,\n)\n</code></pre>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.SineUniform","title":"<code>SineUniform()</code>","text":"<p>             Bases: <code>SineBenchmark</code></p> <p>Sine benchmark with the domain and co-domain sampled random uniformly.</p> <p>The <code>SineRegular</code> benchmark is a <code>SineBenchmark</code> with the following properties:</p> <ul> <li><code>n_sensors</code> is 32.</li> <li><code>n_evaluations</code> is 32.</li> <li><code>n_train</code> is 4096.</li> <li><code>n_test</code> is 4096.</li> <li><code>uniform</code> is <code>True</code>.</li> </ul> <p> Visualizations of a few samples. </p> Source code in <code>src/continuiti/benchmarks/sine.py</code> <pre><code>def __init__(self):\nsuper().__init__(\nn_sensors=32,\nn_evaluations=32,\nn_train=4096,\nn_test=4096,\nuniform=True,\n)\n</code></pre>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.Flame","title":"<code>Flame(flame_dir=None, train_size=None, val_size=None, normalize=True, upsample=False)</code>","text":"<p>             Bases: <code>Benchmark</code></p> <p>Flame benchmark.</p> <p>The <code>Flame</code> benchmark contains the dataset of the 2023 FLAME AI Challenge on super-resolution for turbulent flows.</p> PARAMETER  DESCRIPTION <code>flame_dir</code> <p>Path to FLAME data set. Default is <code>data/flame</code> in the root directory of the repository.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>train_size</code> <p>Limit size of training set. By default use the full data set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>val_size</code> <p>Limit size of validation set. By default use the full data set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>normalize</code> <p>Normalize data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>upsample</code> <p>Upsample training set.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/continuiti/benchmarks/flame.py</code> <pre><code>def __init__(\nself,\nflame_dir: Optional[str] = None,\ntrain_size: int = None,\nval_size: int = None,\nnormalize: bool = True,\nupsample: bool = False,\n):\nif flame_dir is None:\n# Get root dir relative to this file\nroot_dir = pathlib.Path(continuiti.__file__).parent.parent.parent\nflame_dir = root_dir / \"data\" / \"flame\"\nelse:\nflame_dir = pathlib.Path(flame_dir)\nkwargs = {\n\"flame_dir\": flame_dir,\n\"normalize\": normalize,\n\"upsample\": upsample,\n}\ntrain_dataset = FlameDataset(split=\"train\", size=train_size, **kwargs)\ntest_dataset = FlameDataset(split=\"val\", size=val_size, **kwargs)\nsuper().__init__(train_dataset, test_dataset, [MSELoss()])\n</code></pre>"},{"location":"api/continuiti/benchmarks/#continuiti.benchmarks.NavierStokes","title":"<code>NavierStokes(dir=None)</code>","text":"<p>             Bases: <code>Benchmark</code></p> <p>Navier-Stokes benchmark.</p> <p>This benchmark contains a dataset of turbulent flow samples taken from neuraloperator/graph-pde that was used as illustrative example in the FNO paper:</p> <p>Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" arXiv preprint arXiv:2010.08895 (2020).</p> <p>The dataset loads the <code>NavierStokes_V1e-5_N1200_T20</code> file which contains 1200 samples of Navier-Stokes flow simulations at a spatial resolution of 64x64 and 20 time steps.</p> <p>The benchmark exports operator datasets where both input and output function are defined on the space-time domain (periodic in space), i.e., \\((x, y, t) \\in [-1, 1] \\times [-1, 1] \\times (-1, 0]\\) for the input function and \\((x, y, t) \\in [-1, 1] \\times [-1, 1] \\times (0, 1]\\) for the output function.</p> <p>The input function is given by the vorticity field at the first ten time steps \\((-0.9, -0.8, ..., 0.0)\\) and the output function by the vorticity field at the following ten time steps \\((0.1, 0.2, ..., 1.0)\\).</p> <p> Visualization of first training sample. </p> <p>The datasets have the following shapes:</p> <pre><code>    len(benchmark.train_dataset) == 1000\n    len(benchmark.test_dataset) == 200\n\n    x.shape == (3, 64, 64, 10)\n    u.shape == (1, 64, 64, 10)\n    y.shape == (3, 64. 64, 10)\n    v.shape == (1, 64, 64, 10)\n</code></pre> PARAMETER  DESCRIPTION <code>dir</code> <p>Path to data set. Default is <code>data/navierstokes</code> in the root directory of the repository.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/benchmarks/navierstokes.py</code> <pre><code>def __init__(self, dir: Optional[str] = None):\nif dir is None:\n# Get root dir relative to this file\nroot_dir = pathlib.Path(continuiti.__file__).parent.parent.parent\ndir = root_dir / \"data\" / \"navierstokes\"\nelse:\ndir = pathlib.Path(dir)\n# Create space-time grids (x_1, x_2, t)\nls = torch.linspace(-1, 1, 64)\ntx = torch.linspace(-0.9, 0.0, 10)\ngrid_x = torch.meshgrid(ls, ls, tx, indexing=\"ij\")\nx = torch.stack(grid_x, axis=0).unsqueeze(0).expand(1200, -1, -1, -1, -1)\nx = x.reshape(1200, 3, 64, 64, 10)\nty = torch.linspace(0.1, 1.0, 10)\ngrid_y = torch.meshgrid(ls, ls, ty, indexing=\"ij\")\ny = torch.stack(grid_y, axis=0).unsqueeze(0).expand(1200, -1, -1, -1, -1)\ny = y.reshape(1200, 3, 64, 64, 10)\n# Load vorticity\ndata = scipy.io.loadmat(dir / \"NavierStokes_V1e-5_N1200_T20.mat\")\nvort0 = torch.tensor(data[\"a\"], dtype=torch.float32)\nvort = torch.tensor(data[\"u\"], dtype=torch.float32)\nassert vort0.shape == (1200, 64, 64)\nassert vort.shape == (1200, 64, 64, 20)\n# Input is vorticity for t \\in [0, 10]\nu = torch.cat(\n(vort0.reshape(-1, 64, 64, 1), vort[:, :, :, :9]),\naxis=3,\n).reshape(1200, 1, 64, 64, 10)\n# Output is vorticity for t \\in [10, 20]\nv = vort[:, :, :, 10:].reshape(1200, 1, 64, 64, 10)\n# Split train/test\ntrain_indices = torch.arange(1000)\ntest_indices = torch.arange(1000, 1200)\ntrain_dataset = OperatorDataset(\nx=x[train_indices],\nu=u[train_indices],\ny=y[train_indices],\nv=v[train_indices],\n)\ntest_dataset = OperatorDataset(\nx=x[test_indices],\nu=u[test_indices],\ny=y[test_indices],\nv=v[test_indices],\n)\nsuper().__init__(train_dataset, test_dataset, [RelativeL1Error()])\n</code></pre>"},{"location":"api/continuiti/benchmarks/benchmark/","title":"Benchmark","text":"<p><code>continuiti.benchmarks.benchmark</code></p> <p>Benchmark base class.</p>"},{"location":"api/continuiti/benchmarks/benchmark/#continuiti.benchmarks.benchmark.Benchmark","title":"<code>Benchmark(train_dataset, test_dataset, losses=lambda: [MSELoss()]())</code>  <code>dataclass</code>","text":"<p>Benchmark class.</p> <p>A Benchmark object encapsulates two distinct datasets: a train and a test dataset. The training dataset is used to train an operator to fit the dataset. The test dataset, in contrast, is utilized solely for evaluating the performance. The evaluation is done by measuring the loss on the test set.</p>"},{"location":"api/continuiti/benchmarks/benchmark/#continuiti.benchmarks.benchmark.Benchmark.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the benchmark.</p> Source code in <code>src/continuiti/benchmarks/benchmark.py</code> <pre><code>def __str__(self):\n\"\"\"Return string representation of the benchmark.\"\"\"\nreturn self.__class__.__name__\n</code></pre>"},{"location":"api/continuiti/benchmarks/flame/","title":"Flame","text":"<p><code>continuiti.benchmarks.flame</code></p> <p>Flame benchmark.</p>"},{"location":"api/continuiti/benchmarks/flame/#continuiti.benchmarks.flame.Flame","title":"<code>Flame(flame_dir=None, train_size=None, val_size=None, normalize=True, upsample=False)</code>","text":"<p>             Bases: <code>Benchmark</code></p> <p>Flame benchmark.</p> <p>The <code>Flame</code> benchmark contains the dataset of the 2023 FLAME AI Challenge on super-resolution for turbulent flows.</p> PARAMETER  DESCRIPTION <code>flame_dir</code> <p>Path to FLAME data set. Default is <code>data/flame</code> in the root directory of the repository.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>train_size</code> <p>Limit size of training set. By default use the full data set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>val_size</code> <p>Limit size of validation set. By default use the full data set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>normalize</code> <p>Normalize data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>upsample</code> <p>Upsample training set.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/continuiti/benchmarks/flame.py</code> <pre><code>def __init__(\nself,\nflame_dir: Optional[str] = None,\ntrain_size: int = None,\nval_size: int = None,\nnormalize: bool = True,\nupsample: bool = False,\n):\nif flame_dir is None:\n# Get root dir relative to this file\nroot_dir = pathlib.Path(continuiti.__file__).parent.parent.parent\nflame_dir = root_dir / \"data\" / \"flame\"\nelse:\nflame_dir = pathlib.Path(flame_dir)\nkwargs = {\n\"flame_dir\": flame_dir,\n\"normalize\": normalize,\n\"upsample\": upsample,\n}\ntrain_dataset = FlameDataset(split=\"train\", size=train_size, **kwargs)\ntest_dataset = FlameDataset(split=\"val\", size=val_size, **kwargs)\nsuper().__init__(train_dataset, test_dataset, [MSELoss()])\n</code></pre>"},{"location":"api/continuiti/benchmarks/flame/#continuiti.benchmarks.flame.FlameDataset","title":"<code>FlameDataset(flame_dir, split, size=None, channels=all_channels, normalize=True, upsample=False)</code>","text":"<p>             Bases: <code>OperatorDatasetBase</code></p> <p>Flame data set.</p> PARAMETER  DESCRIPTION <code>flame_dir</code> <p>Path to data set, e.g. \"data/flame/\".</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>Split. Either \"train\", \"val\" or \"test\".</p> <p> TYPE: <code>str</code> </p> <code>size</code> <p>Limit size of data set. If None, use all data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>channels</code> <p>channels to load, e.g. [\"rho\", \"ux\", \"uy\", \"uz\"].</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>all_channels</code> </p> <code>normalize</code> <p>Normalize data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>upsample</code> <p>Upsample input to 128x128 using bilinear interpolation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/continuiti/benchmarks/flame.py</code> <pre><code>def __init__(\nself,\nflame_dir: str,\nsplit: str,\nsize: int = None,\nchannels: List[str] = all_channels,\nnormalize: bool = True,\nupsample: bool = False,\n):\nself.path = os.path.join(flame_dir, \"\")\nassert os.path.exists(self.path), f\"Path '{self.path}' does not exist.\"\nself.split = split\nassert split in [\"train\", \"val\", \"test\"], f\"Invalid split '{split}'.\"\nself.size = self._get_size()\nif size is not None:\nself.size = min(self.size, size)\nself.channels = channels\nassert len(channels) &gt; 0, \"Must load at least one channel.\"\nassert all([q in self.all_channels for q in channels]), \"Invalid channel.\"\nself.normalize = normalize\nself.upsample = upsample\nself.upsample_layer = torch.nn.Upsample(scale_factor=8, mode=\"bilinear\")\nsize = (16, 16) if not upsample else (128, 128)\nself.shapes = OperatorShapes(\nx=TensorShape(dim=2, size=size),\nu=TensorShape(dim=len(channels), size=size),\ny=TensorShape(dim=2, size=(128, 128)),\nv=TensorShape(dim=len(channels), size=(128, 128)),\n)\n</code></pre>"},{"location":"api/continuiti/benchmarks/flame/#continuiti.benchmarks.flame.FlameDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples.</p> RETURNS DESCRIPTION <code>int</code> <p>number of samples in the data set.</p> Source code in <code>src/continuiti/benchmarks/flame.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return the number of samples.\n    Returns:\n        number of samples in the data set.\n    \"\"\"\nreturn self.size\n</code></pre>"},{"location":"api/continuiti/benchmarks/flame/#continuiti.benchmarks.flame.FlameDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the input-output pair at the specified index and applies transformations.</p> PARAMETER  DESCRIPTION <code>-</code> <p>The index of the sample to retrieve.</p> <p> TYPE: <code>idx</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple containing the three input tensors and the output tensor for the given index.</p> Source code in <code>src/continuiti/benchmarks/flame.py</code> <pre><code>def __getitem__(\nself, idx\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\"\"\"Retrieves the input-output pair at the specified index and applies transformations.\n    Parameters:\n        - idx: The index of the sample to retrieve.\n    Returns:\n        A tuple containing the three input tensors and the output tensor for the given index.\n    \"\"\"\nassert idx &gt;= 0 and idx &lt; self.size, f\"Invalid index '{idx}'.\"\n# Load data\nu_i = self._load(index=idx, res=\"LR\")\nv_i = self._load(index=idx, res=\"HR\")\n# Normalize\nif self.normalize:\ndef normalized(x):\nmean = x.mean(dim=0, keepdim=True)\nstd = x.std(dim=0, keepdim=True) + 1e-3\nreturn (x - mean) / std\nu_i = normalized(u_i)\nv_i = normalized(v_i)\n# Positions and (optional) upsampling\nif self.upsample:\n# Upsample\nu_i = u_i.reshape(16, 16, -1)\nu_i = u_i.unsqueeze(0).swapaxes(1, -1)\nu_i = self.upsample_layer(u_i)\nu_i = u_i.swapaxes(1, -1).reshape(128 * 128, -1)\nx_i = self._create_position_grid(128)\nelse:\nx_i = self._create_position_grid(16)\ny_i = self._create_position_grid(128)\nreturn x_i, u_i, y_i, v_i\n</code></pre>"},{"location":"api/continuiti/benchmarks/navierstokes/","title":"Navierstokes","text":"<p><code>continuiti.benchmarks.navier_stokes</code></p> <p>Navier-Stokes benchmark.</p>"},{"location":"api/continuiti/benchmarks/navierstokes/#continuiti.benchmarks.navierstokes.NavierStokes","title":"<code>NavierStokes(dir=None)</code>","text":"<p>             Bases: <code>Benchmark</code></p> <p>Navier-Stokes benchmark.</p> <p>This benchmark contains a dataset of turbulent flow samples taken from neuraloperator/graph-pde that was used as illustrative example in the FNO paper:</p> <p>Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" arXiv preprint arXiv:2010.08895 (2020).</p> <p>The dataset loads the <code>NavierStokes_V1e-5_N1200_T20</code> file which contains 1200 samples of Navier-Stokes flow simulations at a spatial resolution of 64x64 and 20 time steps.</p> <p>The benchmark exports operator datasets where both input and output function are defined on the space-time domain (periodic in space), i.e., \\((x, y, t) \\in [-1, 1] \\times [-1, 1] \\times (-1, 0]\\) for the input function and \\((x, y, t) \\in [-1, 1] \\times [-1, 1] \\times (0, 1]\\) for the output function.</p> <p>The input function is given by the vorticity field at the first ten time steps \\((-0.9, -0.8, ..., 0.0)\\) and the output function by the vorticity field at the following ten time steps \\((0.1, 0.2, ..., 1.0)\\).</p> <p> Visualization of first training sample. </p> <p>The datasets have the following shapes:</p> <pre><code>    len(benchmark.train_dataset) == 1000\n    len(benchmark.test_dataset) == 200\n\n    x.shape == (3, 64, 64, 10)\n    u.shape == (1, 64, 64, 10)\n    y.shape == (3, 64. 64, 10)\n    v.shape == (1, 64, 64, 10)\n</code></pre> PARAMETER  DESCRIPTION <code>dir</code> <p>Path to data set. Default is <code>data/navierstokes</code> in the root directory of the repository.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/benchmarks/navierstokes.py</code> <pre><code>def __init__(self, dir: Optional[str] = None):\nif dir is None:\n# Get root dir relative to this file\nroot_dir = pathlib.Path(continuiti.__file__).parent.parent.parent\ndir = root_dir / \"data\" / \"navierstokes\"\nelse:\ndir = pathlib.Path(dir)\n# Create space-time grids (x_1, x_2, t)\nls = torch.linspace(-1, 1, 64)\ntx = torch.linspace(-0.9, 0.0, 10)\ngrid_x = torch.meshgrid(ls, ls, tx, indexing=\"ij\")\nx = torch.stack(grid_x, axis=0).unsqueeze(0).expand(1200, -1, -1, -1, -1)\nx = x.reshape(1200, 3, 64, 64, 10)\nty = torch.linspace(0.1, 1.0, 10)\ngrid_y = torch.meshgrid(ls, ls, ty, indexing=\"ij\")\ny = torch.stack(grid_y, axis=0).unsqueeze(0).expand(1200, -1, -1, -1, -1)\ny = y.reshape(1200, 3, 64, 64, 10)\n# Load vorticity\ndata = scipy.io.loadmat(dir / \"NavierStokes_V1e-5_N1200_T20.mat\")\nvort0 = torch.tensor(data[\"a\"], dtype=torch.float32)\nvort = torch.tensor(data[\"u\"], dtype=torch.float32)\nassert vort0.shape == (1200, 64, 64)\nassert vort.shape == (1200, 64, 64, 20)\n# Input is vorticity for t \\in [0, 10]\nu = torch.cat(\n(vort0.reshape(-1, 64, 64, 1), vort[:, :, :, :9]),\naxis=3,\n).reshape(1200, 1, 64, 64, 10)\n# Output is vorticity for t \\in [10, 20]\nv = vort[:, :, :, 10:].reshape(1200, 1, 64, 64, 10)\n# Split train/test\ntrain_indices = torch.arange(1000)\ntest_indices = torch.arange(1000, 1200)\ntrain_dataset = OperatorDataset(\nx=x[train_indices],\nu=u[train_indices],\ny=y[train_indices],\nv=v[train_indices],\n)\ntest_dataset = OperatorDataset(\nx=x[test_indices],\nu=u[test_indices],\ny=y[test_indices],\nv=v[test_indices],\n)\nsuper().__init__(train_dataset, test_dataset, [RelativeL1Error()])\n</code></pre>"},{"location":"api/continuiti/benchmarks/sine/","title":"Sine","text":"<p><code>continuiti.benchmarks.sine</code></p> <p>Sine benchmarks.</p>"},{"location":"api/continuiti/benchmarks/sine/#continuiti.benchmarks.sine.SineBenchmark","title":"<code>SineBenchmark(n_sensors=32, n_evaluations=32, n_train=1024, n_test=32, uniform=False)</code>","text":"<p>             Bases: <code>Benchmark</code></p> <p>Sine benchmark.</p> <p>The <code>SineBenchmark</code> contains a dataset of trigonometric functions</p> \\[     f_k(x) = \\sin(kx), \\quad x \\in [-1, 1], \\quad k \\in [\\pi, 2\\pi], \\] <p>with the following properties:</p> <ul> <li>Input and output function spaces are the same.</li> <li>The input space is mapped to the output space with the identity operator.</li> <li>Both the the domain and the co-domain are sampled on a regular grid, if   <code>uniform</code> is <code>False</code>, or uniformly, otherwise.</li> <li>The parameter \\(k\\) is sampled uniformly from \\([\\pi, 2\\pi]\\).</li> </ul> PARAMETER  DESCRIPTION <code>n_sensors</code> <p>number of sensors.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>n_evaluations</code> <p>number of evaluations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>n_train</code> <p>number of observations in the train dataset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>n_test</code> <p>number of observations in the test dataset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>uniform</code> <p>whether to sample the domain and co-domain random uniformly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/continuiti/benchmarks/sine.py</code> <pre><code>def __init__(\nself,\nn_sensors: int = 32,\nn_evaluations: int = 32,\nn_train: int = 1024,\nn_test: int = 32,\nuniform: bool = False,\n):\nsine_set = FunctionSet(lambda k: Function(lambda x: torch.sin(k * x)))\nif uniform:\nx_sampler = y_sampler = UniformBoxSampler([-1.0], [1.0])\nelse:\nx_sampler = y_sampler = RegularGridSampler([-1.0], [1.0])\nparameter_sampler = UniformBoxSampler([torch.pi], [2 * torch.pi])\ndef get_dataset(n_obs: int):\nreturn FunctionOperatorDataset(\ninput_function_set=sine_set,\nx_sampler=x_sampler,\nn_sensors=n_sensors,\noutput_function_set=sine_set,\ny_sampler=y_sampler,\nn_evaluations=n_evaluations,\nparameter_sampler=parameter_sampler,\nn_observations=n_obs,\n)\ntrain_dataset = get_dataset(n_train)\ntest_dataset = get_dataset(n_test)\nsuper().__init__(train_dataset, test_dataset, [MSELoss()])\n</code></pre>"},{"location":"api/continuiti/benchmarks/sine/#continuiti.benchmarks.sine.SineRegular","title":"<code>SineRegular()</code>","text":"<p>             Bases: <code>SineBenchmark</code></p> <p>Sine benchmark with the domain and co-domain sampled on a regular grid.</p> <p>The <code>SineRegular</code> benchmark is a <code>SineBenchmark</code> with the following properties:</p> <ul> <li><code>n_sensors</code> is 32.</li> <li><code>n_evaluations</code> is 32.</li> <li><code>n_train</code> is 1024.</li> <li><code>n_test</code> is 1024.</li> <li><code>uniform</code> is <code>False</code>.</li> </ul> <p> Visualizations of a few samples. </p> Source code in <code>src/continuiti/benchmarks/sine.py</code> <pre><code>def __init__(self):\nsuper().__init__(\nn_sensors=32,\nn_evaluations=32,\nn_train=1024,\nn_test=1024,\nuniform=False,\n)\n</code></pre>"},{"location":"api/continuiti/benchmarks/sine/#continuiti.benchmarks.sine.SineUniform","title":"<code>SineUniform()</code>","text":"<p>             Bases: <code>SineBenchmark</code></p> <p>Sine benchmark with the domain and co-domain sampled random uniformly.</p> <p>The <code>SineRegular</code> benchmark is a <code>SineBenchmark</code> with the following properties:</p> <ul> <li><code>n_sensors</code> is 32.</li> <li><code>n_evaluations</code> is 32.</li> <li><code>n_train</code> is 4096.</li> <li><code>n_test</code> is 4096.</li> <li><code>uniform</code> is <code>True</code>.</li> </ul> <p> Visualizations of a few samples. </p> Source code in <code>src/continuiti/benchmarks/sine.py</code> <pre><code>def __init__(self):\nsuper().__init__(\nn_sensors=32,\nn_evaluations=32,\nn_train=4096,\nn_test=4096,\nuniform=True,\n)\n</code></pre>"},{"location":"api/continuiti/benchmarks/run/","title":"Run","text":"<p><code>continuiti.benchmarks.run</code></p> <p>This module contains the functionality to run benchmarks in continuiti.</p>"},{"location":"api/continuiti/benchmarks/run/#continuiti.benchmarks.run.RunConfig","title":"<code>RunConfig(benchmark_factory, operator_factory, seed=0, lr=0.001, tol=0, max_epochs=100, batch_size=8, device=get_device(), verbose=True)</code>  <code>dataclass</code>","text":"<p>Run configuration.</p> PARAMETER  DESCRIPTION <code>benchmark_factory</code> <p>Benchmark factory. Callable that returns a Benchmark.</p> <p> TYPE: <code>Callable[[], Benchmark]</code> </p> <code>operator_factory</code> <p>Operator factory. Callable that takes OperatorShapes and returns an Operator.</p> <p> TYPE: <code>Callable[[OperatorShapes], Operator]</code> </p> <code>seed</code> <p>Random seed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>lr</code> <p>Learning rate.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>tol</code> <p>Threshold for stopping criterion.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>max_epochs</code> <p>Maximum number of epochs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Union[device, str]</code> DEFAULT: <code>get_device()</code> </p> <code>verbose</code> <p>Verbose training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"api/continuiti/benchmarks/run/#continuiti.benchmarks.run.BenchmarkRunner","title":"<code>BenchmarkRunner</code>","text":"<p>Benchmark runner.</p>"},{"location":"api/continuiti/benchmarks/run/#continuiti.benchmarks.run.BenchmarkRunner.run","title":"<code>run(config, params_dict=None)</code>  <code>staticmethod</code>","text":"<p>Run a benchmark.</p> PARAMETER  DESCRIPTION <code>config</code> <p>run configuration.</p> <p> TYPE: <code>RunConfig</code> </p> <code>params_dict</code> <p>dictionary of parameters to log.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Test loss.</p> Source code in <code>src/continuiti/benchmarks/run/runner.py</code> <pre><code>@staticmethod\ndef run(config: RunConfig, params_dict: Optional[dict] = None) -&gt; float:\n\"\"\"Run a benchmark.\n    Args:\n        config: run configuration.\n        params_dict: dictionary of parameters to log.\n    Returns:\n        Test loss.\n    \"\"\"\n# Device\ndevice = torch.device(config.device) or get_device()\n# Rank\nrank = device.index or 0\n# Benchmark\nbenchmark = config.benchmark_factory()\n# Operator\nshapes = benchmark.train_dataset.shapes\noperator = config.operator_factory(shapes, device=device)\n# Log parameters\nif rank == 0:\nif params_dict is None:\nparams_dict = {}\nparam_str = \" \".join(f\"{key}={value}\" for key, value in params_dict.items())\n# MLFLow\nmlflow.set_experiment(f\"{benchmark}\")\ntimestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nrun_name = f\"{operator} {timestamp}\"\ntags = {\n\"benchmark\": str(benchmark),\n\"operator\": str(operator),\n\"device\": str(config.device),\n}\nmlflow.start_run(run_name=run_name, tags=tags)\n# Log parameters\nif params_dict is not None:\nfor key, value in params_dict.items():\nmlflow.log_param(key, value)\nif \"seed\" not in params_dict:\nmlflow.log_param(\"seed\", config.seed)\nif \"lr\" not in params_dict:\nmlflow.log_param(\"lr\", config.lr)\nif \"batch_size\" not in params_dict:\nmlflow.log_param(\"batch_size\", config.batch_size)\nif \"tol\" not in params_dict:\nmlflow.log_param(\"tol\", config.tol)\nif \"max_epochs\" not in params_dict:\nmlflow.log_param(\"max_epochs\", config.max_epochs)\nmlflow.log_metric(\"num_params\", operator.num_params())\n# Seed\nrandom.seed(config.seed)\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nif torch.cuda.is_available():\ntorch.cuda.manual_seed_all(config.seed)\n# For now, take the sum of all losses in benchmark\ndef loss_fn(*args):\nreturn sum(loss(*args) for loss in benchmark.losses)\n# Trainer\noptimizer = torch.optim.Adam(operator.parameters(), lr=config.lr)\ntrainer = Trainer(\noperator,\noptimizer,\nloss_fn=loss_fn,\ndevice=config.device,\nverbose=config.verbose,\n)\nif rank == 0:\nprint(f\"&gt; {benchmark} {operator} {param_str}\")\n# Train\ncallbacks = None\nif rank == 0:\ncallbacks = [MLFlowLogger(operator)]\nlogs = trainer.fit(\nbenchmark.train_dataset,\ntol=config.tol,\nepochs=config.max_epochs,\ncallbacks=callbacks,\nbatch_size=config.batch_size,\ntest_dataset=benchmark.test_dataset,\n)\n# Return test loss\nreturn logs.loss_test\n</code></pre>"},{"location":"api/continuiti/benchmarks/run/run_config/","title":"Run config","text":""},{"location":"api/continuiti/benchmarks/run/run_config/#continuiti.benchmarks.run.run_config.RunConfig","title":"<code>RunConfig(benchmark_factory, operator_factory, seed=0, lr=0.001, tol=0, max_epochs=100, batch_size=8, device=get_device(), verbose=True)</code>  <code>dataclass</code>","text":"<p>Run configuration.</p> PARAMETER  DESCRIPTION <code>benchmark_factory</code> <p>Benchmark factory. Callable that returns a Benchmark.</p> <p> TYPE: <code>Callable[[], Benchmark]</code> </p> <code>operator_factory</code> <p>Operator factory. Callable that takes OperatorShapes and returns an Operator.</p> <p> TYPE: <code>Callable[[OperatorShapes], Operator]</code> </p> <code>seed</code> <p>Random seed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>lr</code> <p>Learning rate.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>tol</code> <p>Threshold for stopping criterion.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>max_epochs</code> <p>Maximum number of epochs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Union[device, str]</code> DEFAULT: <code>get_device()</code> </p> <code>verbose</code> <p>Verbose training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"api/continuiti/benchmarks/run/runner/","title":"Runner","text":""},{"location":"api/continuiti/benchmarks/run/runner/#continuiti.benchmarks.run.runner.BenchmarkRunner","title":"<code>BenchmarkRunner</code>","text":"<p>Benchmark runner.</p>"},{"location":"api/continuiti/benchmarks/run/runner/#continuiti.benchmarks.run.runner.BenchmarkRunner.run","title":"<code>run(config, params_dict=None)</code>  <code>staticmethod</code>","text":"<p>Run a benchmark.</p> PARAMETER  DESCRIPTION <code>config</code> <p>run configuration.</p> <p> TYPE: <code>RunConfig</code> </p> <code>params_dict</code> <p>dictionary of parameters to log.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Test loss.</p> Source code in <code>src/continuiti/benchmarks/run/runner.py</code> <pre><code>@staticmethod\ndef run(config: RunConfig, params_dict: Optional[dict] = None) -&gt; float:\n\"\"\"Run a benchmark.\n    Args:\n        config: run configuration.\n        params_dict: dictionary of parameters to log.\n    Returns:\n        Test loss.\n    \"\"\"\n# Device\ndevice = torch.device(config.device) or get_device()\n# Rank\nrank = device.index or 0\n# Benchmark\nbenchmark = config.benchmark_factory()\n# Operator\nshapes = benchmark.train_dataset.shapes\noperator = config.operator_factory(shapes, device=device)\n# Log parameters\nif rank == 0:\nif params_dict is None:\nparams_dict = {}\nparam_str = \" \".join(f\"{key}={value}\" for key, value in params_dict.items())\n# MLFLow\nmlflow.set_experiment(f\"{benchmark}\")\ntimestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nrun_name = f\"{operator} {timestamp}\"\ntags = {\n\"benchmark\": str(benchmark),\n\"operator\": str(operator),\n\"device\": str(config.device),\n}\nmlflow.start_run(run_name=run_name, tags=tags)\n# Log parameters\nif params_dict is not None:\nfor key, value in params_dict.items():\nmlflow.log_param(key, value)\nif \"seed\" not in params_dict:\nmlflow.log_param(\"seed\", config.seed)\nif \"lr\" not in params_dict:\nmlflow.log_param(\"lr\", config.lr)\nif \"batch_size\" not in params_dict:\nmlflow.log_param(\"batch_size\", config.batch_size)\nif \"tol\" not in params_dict:\nmlflow.log_param(\"tol\", config.tol)\nif \"max_epochs\" not in params_dict:\nmlflow.log_param(\"max_epochs\", config.max_epochs)\nmlflow.log_metric(\"num_params\", operator.num_params())\n# Seed\nrandom.seed(config.seed)\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nif torch.cuda.is_available():\ntorch.cuda.manual_seed_all(config.seed)\n# For now, take the sum of all losses in benchmark\ndef loss_fn(*args):\nreturn sum(loss(*args) for loss in benchmark.losses)\n# Trainer\noptimizer = torch.optim.Adam(operator.parameters(), lr=config.lr)\ntrainer = Trainer(\noperator,\noptimizer,\nloss_fn=loss_fn,\ndevice=config.device,\nverbose=config.verbose,\n)\nif rank == 0:\nprint(f\"&gt; {benchmark} {operator} {param_str}\")\n# Train\ncallbacks = None\nif rank == 0:\ncallbacks = [MLFlowLogger(operator)]\nlogs = trainer.fit(\nbenchmark.train_dataset,\ntol=config.tol,\nepochs=config.max_epochs,\ncallbacks=callbacks,\nbatch_size=config.batch_size,\ntest_dataset=benchmark.test_dataset,\n)\n# Return test loss\nreturn logs.loss_test\n</code></pre>"},{"location":"api/continuiti/data/","title":"Data","text":"<p><code>continuiti.data</code></p> <p>Data sets in continuiti. Every data set is a list of <code>(x, u, y, v)</code> tuples.</p>"},{"location":"api/continuiti/data/#continuiti.data.OperatorDataset","title":"<code>OperatorDataset(x, u, y, v, x_transform=None, u_transform=None, y_transform=None, v_transform=None)</code>","text":"<p>             Bases: <code>OperatorDatasetBase</code></p> <p>A dataset for operator training.</p> <p>In operator training, at least one function is mapped onto a second one. To fulfill the properties discretization invariance, domain independence and learn operators with physics-based loss access to at least four different discretized spaces is necessary. One on which the input is sampled (x), the input function sampled on these points (u), the discretization of the output space (y), and the output of the operator (v) sampled on these points. Not all loss functions and/or operators need access to all of these attributes.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of shape (num_observations, x_dim, num_sensors...) with sensor positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of shape (num_observations, u_dim, num_sensors...) with evaluations of the input functions at sensor positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of shape (num_observations, y_dim, num_evaluations...) with evaluation positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of shape (num_observations, v_dim, num_evaluations...) with ground truth operator mappings.</p> <p> TYPE: <code>Tensor</code> </p> ATTRIBUTE DESCRIPTION <code>shapes</code> <p>Shape of all tensors.</p> <p> </p> <code>transform</code> <p>Transformations for each tensor.</p> <p> </p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __init__(\nself,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\nx_transform: Optional[Transform] = None,\nu_transform: Optional[Transform] = None,\ny_transform: Optional[Transform] = None,\nv_transform: Optional[Transform] = None,\n):\nassert all([t.ndim &gt;= 3 for t in [x, u, y, v]]), \"Wrong number of dimensions.\"\nassert (\nx.size(0) == u.size(0) == y.size(0) == v.size(0)\n), \"Inconsistent number of observations.\"\n# get dimensions and sizes\nx_dim, x_size = x.size(1), x.size()[2:]\nu_dim, u_size = u.size(1), u.size()[2:]\ny_dim, y_size = y.size(1), y.size()[2:]\nv_dim, v_size = v.size(1), v.size()[2:]\nassert x_size == u_size, \"Inconsistent number of sensors.\"\nassert y_size == v_size, \"Inconsistent number of evaluations.\"\nsuper().__init__()\nself.x = x\nself.u = u\nself.y = y\nself.v = v\n# used to initialize architectures\nself.shapes = OperatorShapes(\nx=TensorShape(dim=x_dim, size=x_size),\nu=TensorShape(dim=u_dim, size=u_size),\ny=TensorShape(dim=y_dim, size=y_size),\nv=TensorShape(dim=v_dim, size=v_size),\n)\nself.transform = {\ndim: tf\nfor dim, tf in [\n(\"x\", x_transform),\n(\"u\", u_transform),\n(\"y\", y_transform),\n(\"v\", v_transform),\n]\nif tf is not None\n}\n</code></pre>"},{"location":"api/continuiti/data/#continuiti.data.OperatorDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of samples in the entire set.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return the number of samples.\n    Returns:\n        Number of samples in the entire set.\n    \"\"\"\nreturn self.x.size(0)\n</code></pre>"},{"location":"api/continuiti/data/#continuiti.data.OperatorDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the input-output pair at the specified index and applies transformations.</p> PARAMETER  DESCRIPTION <code>idx</code> <p>The index of the sample to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple containing the three input tensors and the output tensor for the given index.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __getitem__(\nself,\nidx: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\"\"\"Retrieves the input-output pair at the specified index and applies transformations.\n    Parameters:\n        idx: The index of the sample to retrieve.\n    Returns:\n        A tuple containing the three input tensors and the output tensor for the given index.\n    \"\"\"\nreturn self._apply_transformations(\nself.x[idx], self.u[idx], self.y[idx], self.v[idx]\n)\n</code></pre>"},{"location":"api/continuiti/data/#continuiti.data.split","title":"<code>split(dataset, split=0.5, seed=None)</code>","text":"<p>Split data set into two parts.</p> PARAMETER  DESCRIPTION <code>split</code> <p>Split fraction.</p> <p> DEFAULT: <code>0.5</code> </p> Source code in <code>src/continuiti/data/utility.py</code> <pre><code>def split(dataset, split=0.5, seed=None):\n\"\"\"\n    Split data set into two parts.\n    Args:\n        split: Split fraction.\n    \"\"\"\nassert 0 &lt; split &lt; 1, \"Split fraction must be between 0 and 1.\"\ngenerator = torch.Generator()\nif seed is not None:\ngenerator.manual_seed(seed)\nsize = len(dataset)\nsplit = int(size * split)\nreturn torch.utils.data.random_split(\ndataset,\n[split, size - split],\ngenerator=generator,\n)\n</code></pre>"},{"location":"api/continuiti/data/#continuiti.data.dataset_loss","title":"<code>dataset_loss(dataset, operator, loss_fn=None, device=None, batch_size=32)</code>","text":"<p>Evaluate operator performance on data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> </p> <code>operator</code> <p>Operator.</p> <p> </p> <code>loss_fn</code> <p>Loss function. Default is MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to evaluate on. Default is CPU.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size. Default is 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> Source code in <code>src/continuiti/data/utility.py</code> <pre><code>def dataset_loss(\ndataset,\noperator,\nloss_fn: Optional[Loss] = None,\ndevice: Optional[torch.device] = None,\nbatch_size: int = 32,\n):\n\"\"\"Evaluate operator performance on data set.\n    Args:\n        dataset: Data set.\n        operator: Operator.\n        loss_fn: Loss function. Default is MSELoss.\n        device: Device to evaluate on. Default is CPU.\n        batch_size: Batch size. Default is 32.\n    \"\"\"\nloss_fn = loss_fn or MSELoss()\ndevice = device or torch.device(\"cpu\")\n# Move operator to device\noperator.to(device)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\nloss = 0\nfor x, u, y, v in dataloader:\nx, u, y, v = x.to(device), u.to(device), y.to(device), v.to(device)\nloss += loss_fn(operator, x, u, y, v).item()\nreturn loss / len(dataloader)\n</code></pre>"},{"location":"api/continuiti/data/dataset/","title":"Dataset","text":"<p><code>continuiti.data.dataset</code></p> <p>Data sets in continuiti. Every data set is a list of <code>(x, u, y, v)</code> tuples.</p>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDatasetBase","title":"<code>OperatorDatasetBase</code>","text":"<p>             Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Abstract base class of a dataset for operator training.</p>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDatasetBase.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the number of samples.</p> RETURNS DESCRIPTION <code>int</code> <p>number of samples in the entire set.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n\"\"\"Return the number of samples.\n    Returns:\n        number of samples in the entire set.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDatasetBase.__getitem__","title":"<code>__getitem__(idx)</code>  <code>abstractmethod</code>","text":"<p>Retrieves the input-output pair at the specified index and applies transformations.</p> PARAMETER  DESCRIPTION <code>-</code> <p>The index of the sample to retrieve.</p> <p> TYPE: <code>idx</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple containing the three input tensors and the output tensor for the given index.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>@abstractmethod\ndef __getitem__(\nself, idx\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\"\"\"Retrieves the input-output pair at the specified index and applies transformations.\n    Parameters:\n        - idx: The index of the sample to retrieve.\n    Returns:\n        A tuple containing the three input tensors and the output tensor for the given index.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDataset","title":"<code>OperatorDataset(x, u, y, v, x_transform=None, u_transform=None, y_transform=None, v_transform=None)</code>","text":"<p>             Bases: <code>OperatorDatasetBase</code></p> <p>A dataset for operator training.</p> <p>In operator training, at least one function is mapped onto a second one. To fulfill the properties discretization invariance, domain independence and learn operators with physics-based loss access to at least four different discretized spaces is necessary. One on which the input is sampled (x), the input function sampled on these points (u), the discretization of the output space (y), and the output of the operator (v) sampled on these points. Not all loss functions and/or operators need access to all of these attributes.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of shape (num_observations, x_dim, num_sensors...) with sensor positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of shape (num_observations, u_dim, num_sensors...) with evaluations of the input functions at sensor positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of shape (num_observations, y_dim, num_evaluations...) with evaluation positions.</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of shape (num_observations, v_dim, num_evaluations...) with ground truth operator mappings.</p> <p> TYPE: <code>Tensor</code> </p> ATTRIBUTE DESCRIPTION <code>shapes</code> <p>Shape of all tensors.</p> <p> </p> <code>transform</code> <p>Transformations for each tensor.</p> <p> </p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __init__(\nself,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\nx_transform: Optional[Transform] = None,\nu_transform: Optional[Transform] = None,\ny_transform: Optional[Transform] = None,\nv_transform: Optional[Transform] = None,\n):\nassert all([t.ndim &gt;= 3 for t in [x, u, y, v]]), \"Wrong number of dimensions.\"\nassert (\nx.size(0) == u.size(0) == y.size(0) == v.size(0)\n), \"Inconsistent number of observations.\"\n# get dimensions and sizes\nx_dim, x_size = x.size(1), x.size()[2:]\nu_dim, u_size = u.size(1), u.size()[2:]\ny_dim, y_size = y.size(1), y.size()[2:]\nv_dim, v_size = v.size(1), v.size()[2:]\nassert x_size == u_size, \"Inconsistent number of sensors.\"\nassert y_size == v_size, \"Inconsistent number of evaluations.\"\nsuper().__init__()\nself.x = x\nself.u = u\nself.y = y\nself.v = v\n# used to initialize architectures\nself.shapes = OperatorShapes(\nx=TensorShape(dim=x_dim, size=x_size),\nu=TensorShape(dim=u_dim, size=u_size),\ny=TensorShape(dim=y_dim, size=y_size),\nv=TensorShape(dim=v_dim, size=v_size),\n)\nself.transform = {\ndim: tf\nfor dim, tf in [\n(\"x\", x_transform),\n(\"u\", u_transform),\n(\"y\", y_transform),\n(\"v\", v_transform),\n]\nif tf is not None\n}\n</code></pre>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of samples in the entire set.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return the number of samples.\n    Returns:\n        Number of samples in the entire set.\n    \"\"\"\nreturn self.x.size(0)\n</code></pre>"},{"location":"api/continuiti/data/dataset/#continuiti.data.dataset.OperatorDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the input-output pair at the specified index and applies transformations.</p> PARAMETER  DESCRIPTION <code>idx</code> <p>The index of the sample to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple containing the three input tensors and the output tensor for the given index.</p> Source code in <code>src/continuiti/data/dataset.py</code> <pre><code>def __getitem__(\nself,\nidx: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\"\"\"Retrieves the input-output pair at the specified index and applies transformations.\n    Parameters:\n        idx: The index of the sample to retrieve.\n    Returns:\n        A tuple containing the three input tensors and the output tensor for the given index.\n    \"\"\"\nreturn self._apply_transformations(\nself.x[idx], self.u[idx], self.y[idx], self.v[idx]\n)\n</code></pre>"},{"location":"api/continuiti/data/mesh/","title":"Mesh","text":"<p><code>continuiti.data.mesh</code></p> <p>Mesh file readers.</p>"},{"location":"api/continuiti/data/mesh/#continuiti.data.mesh.Gmsh","title":"<code>Gmsh(filename)</code>","text":"<p>Gmsh is an open source 3D finite element mesh generator.</p> <p>You can find example <code>.msh</code> files in the <code>data/meshes</code> directory.</p> <p>A <code>Gmsh</code> object reads and provides the content of a Gmsh file in a tensor format that can also be used with <code>matplotlib</code>.</p> Example <pre><code>mesh = Gmsh(\"path/to/file.msh\")\nvertices = mesh.get_vertices()\ncells = mesh.get_cells()\nfrom matplotlib.tri import Triangulation\ntri = Triangulation(vertices[:, 0], vertices[:, 1], cells)\n</code></pre> PARAMETER  DESCRIPTION <code>filename</code> <p>Path to <code>.msh</code> file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/continuiti/data/mesh.py</code> <pre><code>def __init__(\nself,\nfilename: str,\n):\nimport gmsh\nself.filename = filename\ngmsh.initialize()\ngmsh.option.setNumber(\"General.Verbosity\", 0)\ngmsh.open(self.filename)\nself.dim = gmsh.model.getDimension()\nself.nodes = gmsh.model.mesh.getNodes()\nself.elements = gmsh.model.mesh.getElements()\ngmsh.finalize()\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/data/mesh/#continuiti.data.mesh.Gmsh.get_vertices","title":"<code>get_vertices()</code>","text":"<p>Get vertices (nodes) as tensor.</p> Source code in <code>src/continuiti/data/mesh.py</code> <pre><code>def get_vertices(self) -&gt; torch.Tensor:\n\"\"\"Get vertices (nodes) as tensor.\"\"\"\nv = torch.tensor(self.nodes[1], dtype=torch.get_default_dtype())\nv = v.reshape(-1, 3)  # Gmsh vertices are always 3D\nv = v.transpose(0, 1)\nreturn v\n</code></pre>"},{"location":"api/continuiti/data/mesh/#continuiti.data.mesh.Gmsh.get_cells","title":"<code>get_cells()</code>","text":"<p>Get cells (elements) as tensor.</p> Source code in <code>src/continuiti/data/mesh.py</code> <pre><code>def get_cells(self) -&gt; torch.Tensor:\n\"\"\"Get cells (elements) as tensor.\"\"\"\ncells = np.array(self.elements[2], dtype=int)\ne = torch.tensor(cells)\ne = e.reshape(-1, 3)\ne = e - 1  # Gmsh uses 1-based indexing\nreturn e\n</code></pre>"},{"location":"api/continuiti/data/selfsupervised/","title":"Selfsupervised","text":"<p><code>continuiti.data.selfsupervised</code></p> <p>Self-supervised data set.</p>"},{"location":"api/continuiti/data/selfsupervised/#continuiti.data.selfsupervised.SelfSupervisedOperatorDataset","title":"<code>SelfSupervisedOperatorDataset(x, u)</code>","text":"<p>             Bases: <code>OperatorDataset</code></p> <p>A <code>SelfSupervisedOperatorDataset</code> is a data set that contains data for self-supervised learning. Every data point is created by taking one sensor as a label.</p> <p>Every observation consists of tuples <code>(x, u, y, v)</code>, where <code>x</code> contains the sensor positions, <code>u</code> the sensor values, and <code>y = x_i</code> and <code>v = u_i</code> are the label's coordinate its value for all <code>i</code>.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (num_observations, coordinate_dim, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Sensor values of shape (num_observations, num_channels, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/data/selfsupervised.py</code> <pre><code>def __init__(self, x: torch.Tensor, u: torch.Tensor):\nself.num_observations = u.shape[0]\nself.coordinate_dim = x.shape[1]\nself.num_channels = u.shape[1]\n# Check consistency across observations\nfor i in range(self.num_observations):\nassert (\nx[i].shape[0] == self.coordinate_dim\n), \"Inconsistent coordinate dimension.\"\nassert (\nu[i].shape[0] == self.num_channels\n), \"Inconsistent number of channels.\"\nxs, us, ys, vs = [], [], [], []\nx_flat = x.view(self.num_observations, self.coordinate_dim, -1)\nu_flat = u.view(self.num_observations, self.num_channels, -1)\nself.num_sensors = math.prod(u.shape[2:])\nfor i in range(self.num_observations):\n# Add one data point for every sensor\nfor j in range(self.num_sensors):\ny = x_flat[i, :, j].unsqueeze(0)\nv = u_flat[i, :, j].unsqueeze(0)\nxs.append(x[i])\nus.append(u[i])\nys.append(y)\nvs.append(v)\nxs = torch.stack(xs)\nus = torch.stack(us)\nys = torch.stack(ys)\nvs = torch.stack(vs)\nsuper().__init__(xs, us, ys, vs)\n</code></pre>"},{"location":"api/continuiti/data/utility/","title":"Utility","text":"<p><code>continuiti.data.utility</code></p> <p>Utility functions for data handling.</p>"},{"location":"api/continuiti/data/utility/#continuiti.data.utility.split","title":"<code>split(dataset, split=0.5, seed=None)</code>","text":"<p>Split data set into two parts.</p> PARAMETER  DESCRIPTION <code>split</code> <p>Split fraction.</p> <p> DEFAULT: <code>0.5</code> </p> Source code in <code>src/continuiti/data/utility.py</code> <pre><code>def split(dataset, split=0.5, seed=None):\n\"\"\"\n    Split data set into two parts.\n    Args:\n        split: Split fraction.\n    \"\"\"\nassert 0 &lt; split &lt; 1, \"Split fraction must be between 0 and 1.\"\ngenerator = torch.Generator()\nif seed is not None:\ngenerator.manual_seed(seed)\nsize = len(dataset)\nsplit = int(size * split)\nreturn torch.utils.data.random_split(\ndataset,\n[split, size - split],\ngenerator=generator,\n)\n</code></pre>"},{"location":"api/continuiti/data/utility/#continuiti.data.utility.dataset_loss","title":"<code>dataset_loss(dataset, operator, loss_fn=None, device=None, batch_size=32)</code>","text":"<p>Evaluate operator performance on data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> </p> <code>operator</code> <p>Operator.</p> <p> </p> <code>loss_fn</code> <p>Loss function. Default is MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to evaluate on. Default is CPU.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size. Default is 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> Source code in <code>src/continuiti/data/utility.py</code> <pre><code>def dataset_loss(\ndataset,\noperator,\nloss_fn: Optional[Loss] = None,\ndevice: Optional[torch.device] = None,\nbatch_size: int = 32,\n):\n\"\"\"Evaluate operator performance on data set.\n    Args:\n        dataset: Data set.\n        operator: Operator.\n        loss_fn: Loss function. Default is MSELoss.\n        device: Device to evaluate on. Default is CPU.\n        batch_size: Batch size. Default is 32.\n    \"\"\"\nloss_fn = loss_fn or MSELoss()\ndevice = device or torch.device(\"cpu\")\n# Move operator to device\noperator.to(device)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\nloss = 0\nfor x, u, y, v in dataloader:\nx, u, y, v = x.to(device), u.to(device), y.to(device), v.to(device)\nloss += loss_fn(operator, x, u, y, v).item()\nreturn loss / len(dataloader)\n</code></pre>"},{"location":"api/continuiti/data/function/","title":"Function","text":"<p><code>continuiti.data.function</code></p> <p>Function data set implementations.</p>"},{"location":"api/continuiti/data/function/#continuiti.data.function.Function","title":"<code>Function(mapping)</code>","text":"<p>A class for creating and manipulating functions.</p> <p>A function is a mapping between a domain X (supported on a field K) and a codomain Y (supported on a field F) denoted by $$ f: X \\rightarrow Y, x \\mapsto f(x). $$ For this class, scalar multiplication and addition with other function instances are implemented. This is done to ensure that function instances are able to fulfill properties needed to create a function space (vector space over a function set).</p> PARAMETER  DESCRIPTION <code>mapping</code> <p>A callable that accepts exactly one argument, a torch.Tensor, and returns a torch.Tensor. This</p> <p> TYPE: <code>Callable</code> </p> Example <p><pre><code>f = Function(lambda x: x**2)\ng = Function(lambda x: x + 1)\nh = f + 2 * g\nx = torch.tensor(2.0)\nh(x)\n</code></pre> Output: <pre><code>tensor(10.)\n</code></pre></p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __init__(self, mapping: Callable):\nself.mapping = mapping\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.Function.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Evaluates the encapsulated function.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>list of arguments passed to the mapping class attribute.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>dict of arguments passed to the mapping class attribute.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The result of applying the function to all arguments.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; torch.Tensor:\n\"\"\"Evaluates the encapsulated function.\n    Args:\n        *args: list of arguments passed to the mapping class attribute.\n        **kwargs: dict of arguments passed to the mapping class attribute.\n    Returns:\n        The result of applying the function to all arguments.\n    \"\"\"\nreturn self.mapping(*args, **kwargs)\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.Function.__add__","title":"<code>__add__(other)</code>","text":"<p>Creates a new Function representing the addition of this function with another.</p> PARAMETER  DESCRIPTION <code>other</code> <p>Another Function instance to add to this one. The other function needs to have the same arguments as this one.</p> <p> TYPE: <code>Function</code> </p> RETURNS DESCRIPTION <code>Function</code> <p>A new Function instance representing the addition of the two functions.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __add__(self, other: Function) -&gt; Function:\n\"\"\"Creates a new Function representing the addition of this function with another.\n    Args:\n        other: Another Function instance to add to this one. The other function needs to have the same arguments as\n            this one.\n    Returns:\n        A new Function instance representing the addition of the two functions.\n    \"\"\"\nreturn Function(mapping=lambda args: self.mapping(args) + other.mapping(args))\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.Function.__mul__","title":"<code>__mul__(scalar)</code>","text":"<p>Creates a new Function representing the multiplication of this function with a scalar.</p> PARAMETER  DESCRIPTION <code>scalar</code> <p>Scalar to multiply this function with.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Function</code> <p>A new Function instance representing the multiplication of this function with the scalar.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __mul__(self, scalar: float) -&gt; Function:\n\"\"\"Creates a new Function representing the multiplication of this function with a scalar.\n    Args:\n        scalar: Scalar to multiply this function with.\n    Returns:\n        A new Function instance representing the multiplication of this function with the scalar.\n    \"\"\"\nreturn Function(mapping=lambda args: scalar * self.mapping(args))\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.FunctionSet","title":"<code>FunctionSet(parameterized_mapping)</code>","text":"<p>Function set class.</p> <p>For a domain X supported on a field K1, and the codomain Y supported on a field K2, a function space is the set of functions F(X, Y) that map from X to Y and is closed with respect to addition \\((f+g)(x):X \\rightarrow Y, x\\mapsto f(x)+g(x)\\) and scalar multiplication \\((cf)(x) \\rightarrow Y, x\\mapsto cf(x)\\). In practice, we consider subsets of function spaces (where in general, these properties are not respected) and, therefore, this class implements a (parametrized) function set.</p> PARAMETER  DESCRIPTION <code>parameterized_mapping</code> <p>A two level nested callable that takes a single parameter in the outer callable as argument and vectors x as inputs to the second callable.</p> <p> TYPE: <code>Callable</code> </p> Example <p><pre><code>p_sine = FunctionSet(lambda a: Function(lambda x: a * torch.sin(x)))\nparam = torch.arange(5)\nfor func in sine(param):\nprint(type(func))\n</code></pre> Out: <pre><code>&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n</code></pre></p> Source code in <code>src/continuiti/data/function/function_set.py</code> <pre><code>def __init__(self, parameterized_mapping: Callable):\nself.parameterized_mapping = parameterized_mapping\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.FunctionSet.__call__","title":"<code>__call__(parameters)</code>","text":"<p>Evaluates the function set for a specific discrete instance of parameters.</p> PARAMETER  DESCRIPTION <code>parameters</code> <p>Parameters for which the mapping class argument will be evaluated of shape (n_parameters, n_functions).</p> <p> TYPE: <code>Tensor</code> </p> <p>Returns:     List of Function instances for the given parameters of this function set instance.</p> Source code in <code>src/continuiti/data/function/function_set.py</code> <pre><code>def __call__(self, parameters: torch.Tensor) -&gt; List[Function]:\n\"\"\"Evaluates the function set for a specific discrete instance of parameters.\n    Args:\n        parameters: Parameters for which the mapping class argument will be evaluated of shape\n            (n_parameters, n_functions).\n    Returns:\n        List of Function instances for the given parameters of this function set instance.\n    \"\"\"\nfuncs = []\nif not isinstance(parameters, torch.Tensor):\nparameters = torch.tensor(parameters)\nn_functions = parameters.size(1)\nfor i in range(n_functions):\nparam = parameters[:, i]\ndef mapping(x, p=param):\nreturn self.parameterized_mapping(p)(x)\nfuncs.append(Function(mapping))\nreturn funcs\n</code></pre>"},{"location":"api/continuiti/data/function/#continuiti.data.function.FunctionOperatorDataset","title":"<code>FunctionOperatorDataset(input_function_set, x_sampler, n_sensors, output_function_set, y_sampler, n_evaluations, parameter_sampler, n_observations, x_transform=None, u_transform=None, y_transform=None, v_transform=None)</code>","text":"<p>             Bases: <code>OperatorDataset</code></p> <p>A dataset class for generating samples from function sets.</p> <p>This class extends <code>OperatorDataset</code> to specifically handle scenarios where both inputs and outputs are known functions. It utilizes samplers to generate discrete representations of function spaces (input and output function sets) and physical spaces.</p> PARAMETER  DESCRIPTION <code>input_function_set</code> <p>A function set representing the input space.</p> <p> TYPE: <code>FunctionSet</code> </p> <code>x_sampler</code> <p>A sampler for generating discrete representations of the domain.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_sensors</code> <p>The number of sensors to sample in the input physical space.</p> <p> TYPE: <code>int</code> </p> <code>output_function_set</code> <p>A function set representing the output set of the underlying operator.</p> <p> TYPE: <code>FunctionSet</code> </p> <code>y_sampler</code> <p>A sampler for generating discrete representations of the codomain.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_evaluations</code> <p>The number of evaluation points to sample in the output physical space.</p> <p> TYPE: <code>int</code> </p> <code>parameter_sampler</code> <p>A sampler for sampling parameters to instantiate functions from the function sets.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_observations</code> <p>The number of observations (=different sets of parameters) to generate.</p> <p> TYPE: <code>int</code> </p> <code>x_transform,</code> <p>Optional transformation functions applied to the sampled physical spaces and function evaluations, respectively.</p> <p> TYPE: <code>(u_transform, y_transform, v_transform)</code> </p> Note <p>The input_function_set and the output_function_set are evaluated on the same set of parameters. Therefore, the order of parameters need to be taken into consideration when defining both function sets.</p> Source code in <code>src/continuiti/data/function/function_dataset.py</code> <pre><code>def __init__(\nself,\ninput_function_set: FunctionSet,\nx_sampler: Sampler,\nn_sensors: int,\noutput_function_set: FunctionSet,\ny_sampler: Sampler,\nn_evaluations: int,\nparameter_sampler: Sampler,\nn_observations: int,\nx_transform: Optional[Transform] = None,\nu_transform: Optional[Transform] = None,\ny_transform: Optional[Transform] = None,\nv_transform: Optional[Transform] = None,\n):\nself.input_function_set = input_function_set\nself.x_sampler = x_sampler\nself.output_function_set = output_function_set\nself.y_sampler = y_sampler\nself.p_sampler = parameter_sampler\nx, u, y, v = self._generate_observations(\nn_sensors=n_sensors,\nn_evaluations=n_evaluations,\nn_observations=n_observations,\n)\nsuper().__init__(\nx=x,\nu=u,\ny=y,\nv=v,\nx_transform=x_transform,\nu_transform=u_transform,\ny_transform=y_transform,\nv_transform=v_transform,\n)\n</code></pre>"},{"location":"api/continuiti/data/function/function/","title":"Function","text":"<p><code>continuiti.data.function.function</code></p> <p>Function.</p>"},{"location":"api/continuiti/data/function/function/#continuiti.data.function.function.Function","title":"<code>Function(mapping)</code>","text":"<p>A class for creating and manipulating functions.</p> <p>A function is a mapping between a domain X (supported on a field K) and a codomain Y (supported on a field F) denoted by $$ f: X \\rightarrow Y, x \\mapsto f(x). $$ For this class, scalar multiplication and addition with other function instances are implemented. This is done to ensure that function instances are able to fulfill properties needed to create a function space (vector space over a function set).</p> PARAMETER  DESCRIPTION <code>mapping</code> <p>A callable that accepts exactly one argument, a torch.Tensor, and returns a torch.Tensor. This</p> <p> TYPE: <code>Callable</code> </p> Example <p><pre><code>f = Function(lambda x: x**2)\ng = Function(lambda x: x + 1)\nh = f + 2 * g\nx = torch.tensor(2.0)\nh(x)\n</code></pre> Output: <pre><code>tensor(10.)\n</code></pre></p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __init__(self, mapping: Callable):\nself.mapping = mapping\n</code></pre>"},{"location":"api/continuiti/data/function/function/#continuiti.data.function.function.Function.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Evaluates the encapsulated function.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>list of arguments passed to the mapping class attribute.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>dict of arguments passed to the mapping class attribute.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The result of applying the function to all arguments.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; torch.Tensor:\n\"\"\"Evaluates the encapsulated function.\n    Args:\n        *args: list of arguments passed to the mapping class attribute.\n        **kwargs: dict of arguments passed to the mapping class attribute.\n    Returns:\n        The result of applying the function to all arguments.\n    \"\"\"\nreturn self.mapping(*args, **kwargs)\n</code></pre>"},{"location":"api/continuiti/data/function/function/#continuiti.data.function.function.Function.__add__","title":"<code>__add__(other)</code>","text":"<p>Creates a new Function representing the addition of this function with another.</p> PARAMETER  DESCRIPTION <code>other</code> <p>Another Function instance to add to this one. The other function needs to have the same arguments as this one.</p> <p> TYPE: <code>Function</code> </p> RETURNS DESCRIPTION <code>Function</code> <p>A new Function instance representing the addition of the two functions.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __add__(self, other: Function) -&gt; Function:\n\"\"\"Creates a new Function representing the addition of this function with another.\n    Args:\n        other: Another Function instance to add to this one. The other function needs to have the same arguments as\n            this one.\n    Returns:\n        A new Function instance representing the addition of the two functions.\n    \"\"\"\nreturn Function(mapping=lambda args: self.mapping(args) + other.mapping(args))\n</code></pre>"},{"location":"api/continuiti/data/function/function/#continuiti.data.function.function.Function.__mul__","title":"<code>__mul__(scalar)</code>","text":"<p>Creates a new Function representing the multiplication of this function with a scalar.</p> PARAMETER  DESCRIPTION <code>scalar</code> <p>Scalar to multiply this function with.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Function</code> <p>A new Function instance representing the multiplication of this function with the scalar.</p> Source code in <code>src/continuiti/data/function/function.py</code> <pre><code>def __mul__(self, scalar: float) -&gt; Function:\n\"\"\"Creates a new Function representing the multiplication of this function with a scalar.\n    Args:\n        scalar: Scalar to multiply this function with.\n    Returns:\n        A new Function instance representing the multiplication of this function with the scalar.\n    \"\"\"\nreturn Function(mapping=lambda args: scalar * self.mapping(args))\n</code></pre>"},{"location":"api/continuiti/data/function/function_dataset/","title":"Function dataset","text":"<p><code>continuiti.data.function.function_dataset</code></p> <p>Function data set implementation.</p>"},{"location":"api/continuiti/data/function/function_dataset/#continuiti.data.function.function_dataset.FunctionOperatorDataset","title":"<code>FunctionOperatorDataset(input_function_set, x_sampler, n_sensors, output_function_set, y_sampler, n_evaluations, parameter_sampler, n_observations, x_transform=None, u_transform=None, y_transform=None, v_transform=None)</code>","text":"<p>             Bases: <code>OperatorDataset</code></p> <p>A dataset class for generating samples from function sets.</p> <p>This class extends <code>OperatorDataset</code> to specifically handle scenarios where both inputs and outputs are known functions. It utilizes samplers to generate discrete representations of function spaces (input and output function sets) and physical spaces.</p> PARAMETER  DESCRIPTION <code>input_function_set</code> <p>A function set representing the input space.</p> <p> TYPE: <code>FunctionSet</code> </p> <code>x_sampler</code> <p>A sampler for generating discrete representations of the domain.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_sensors</code> <p>The number of sensors to sample in the input physical space.</p> <p> TYPE: <code>int</code> </p> <code>output_function_set</code> <p>A function set representing the output set of the underlying operator.</p> <p> TYPE: <code>FunctionSet</code> </p> <code>y_sampler</code> <p>A sampler for generating discrete representations of the codomain.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_evaluations</code> <p>The number of evaluation points to sample in the output physical space.</p> <p> TYPE: <code>int</code> </p> <code>parameter_sampler</code> <p>A sampler for sampling parameters to instantiate functions from the function sets.</p> <p> TYPE: <code>Sampler</code> </p> <code>n_observations</code> <p>The number of observations (=different sets of parameters) to generate.</p> <p> TYPE: <code>int</code> </p> <code>x_transform,</code> <p>Optional transformation functions applied to the sampled physical spaces and function evaluations, respectively.</p> <p> TYPE: <code>(u_transform, y_transform, v_transform)</code> </p> Note <p>The input_function_set and the output_function_set are evaluated on the same set of parameters. Therefore, the order of parameters need to be taken into consideration when defining both function sets.</p> Source code in <code>src/continuiti/data/function/function_dataset.py</code> <pre><code>def __init__(\nself,\ninput_function_set: FunctionSet,\nx_sampler: Sampler,\nn_sensors: int,\noutput_function_set: FunctionSet,\ny_sampler: Sampler,\nn_evaluations: int,\nparameter_sampler: Sampler,\nn_observations: int,\nx_transform: Optional[Transform] = None,\nu_transform: Optional[Transform] = None,\ny_transform: Optional[Transform] = None,\nv_transform: Optional[Transform] = None,\n):\nself.input_function_set = input_function_set\nself.x_sampler = x_sampler\nself.output_function_set = output_function_set\nself.y_sampler = y_sampler\nself.p_sampler = parameter_sampler\nx, u, y, v = self._generate_observations(\nn_sensors=n_sensors,\nn_evaluations=n_evaluations,\nn_observations=n_observations,\n)\nsuper().__init__(\nx=x,\nu=u,\ny=y,\nv=v,\nx_transform=x_transform,\nu_transform=u_transform,\ny_transform=y_transform,\nv_transform=v_transform,\n)\n</code></pre>"},{"location":"api/continuiti/data/function/function_set/","title":"Function set","text":"<p><code>continuiti.data.function.function_set</code></p> <p>Function set.</p>"},{"location":"api/continuiti/data/function/function_set/#continuiti.data.function.function_set.FunctionSet","title":"<code>FunctionSet(parameterized_mapping)</code>","text":"<p>Function set class.</p> <p>For a domain X supported on a field K1, and the codomain Y supported on a field K2, a function space is the set of functions F(X, Y) that map from X to Y and is closed with respect to addition \\((f+g)(x):X \\rightarrow Y, x\\mapsto f(x)+g(x)\\) and scalar multiplication \\((cf)(x) \\rightarrow Y, x\\mapsto cf(x)\\). In practice, we consider subsets of function spaces (where in general, these properties are not respected) and, therefore, this class implements a (parametrized) function set.</p> PARAMETER  DESCRIPTION <code>parameterized_mapping</code> <p>A two level nested callable that takes a single parameter in the outer callable as argument and vectors x as inputs to the second callable.</p> <p> TYPE: <code>Callable</code> </p> Example <p><pre><code>p_sine = FunctionSet(lambda a: Function(lambda x: a * torch.sin(x)))\nparam = torch.arange(5)\nfor func in sine(param):\nprint(type(func))\n</code></pre> Out: <pre><code>&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n&lt;class 'continuiti.data.function.function.Function'&gt;\n</code></pre></p> Source code in <code>src/continuiti/data/function/function_set.py</code> <pre><code>def __init__(self, parameterized_mapping: Callable):\nself.parameterized_mapping = parameterized_mapping\n</code></pre>"},{"location":"api/continuiti/data/function/function_set/#continuiti.data.function.function_set.FunctionSet.__call__","title":"<code>__call__(parameters)</code>","text":"<p>Evaluates the function set for a specific discrete instance of parameters.</p> PARAMETER  DESCRIPTION <code>parameters</code> <p>Parameters for which the mapping class argument will be evaluated of shape (n_parameters, n_functions).</p> <p> TYPE: <code>Tensor</code> </p> <p>Returns:     List of Function instances for the given parameters of this function set instance.</p> Source code in <code>src/continuiti/data/function/function_set.py</code> <pre><code>def __call__(self, parameters: torch.Tensor) -&gt; List[Function]:\n\"\"\"Evaluates the function set for a specific discrete instance of parameters.\n    Args:\n        parameters: Parameters for which the mapping class argument will be evaluated of shape\n            (n_parameters, n_functions).\n    Returns:\n        List of Function instances for the given parameters of this function set instance.\n    \"\"\"\nfuncs = []\nif not isinstance(parameters, torch.Tensor):\nparameters = torch.tensor(parameters)\nn_functions = parameters.size(1)\nfor i in range(n_functions):\nparam = parameters[:, i]\ndef mapping(x, p=param):\nreturn self.parameterized_mapping(p)(x)\nfuncs.append(Function(mapping))\nreturn funcs\n</code></pre>"},{"location":"api/continuiti/discrete/","title":"Discrete","text":"<p><code>continuiti.discrete</code></p> <p>Functionalities handling discretization of continuous functionals.</p>"},{"location":"api/continuiti/discrete/#continuiti.discrete.UniformBoxSampler","title":"<code>UniformBoxSampler(x_min, x_max)</code>","text":"<p>             Bases: <code>BoxSampler</code></p> <p>Sample uniformly from an n-dimensional box.</p> Example <p><pre><code>sampler = UniformBoxSampler(torch.tensor([0, 0]), torch.tensor([1, 1]))\nsamples = sampler(100)\nsamples.shape\n</code></pre> Output: <pre><code>torch.Size([2, 100])\n</code></pre></p> Note <p>Using <code>torch.rand</code> the UniformBoxSampler samples from a right-open interval in every dimension.</p> Source code in <code>src/continuiti/discrete/box_sampler.py</code> <pre><code>def __init__(\nself, x_min: Union[torch.Tensor, list], x_max: Union[torch.Tensor, list]\n):\n# Convert lists to tensors\nif isinstance(x_min, list):\nx_min = torch.tensor(x_min)\nif isinstance(x_max, list):\nx_max = torch.tensor(x_max)\nassert x_min.shape == x_max.shape\nassert x_min.ndim == x_max.ndim == 1\nsuper().__init__(len(x_min))\nself.x_min = x_min\nself.x_max = x_max\nself.x_delta = x_max - x_min\n</code></pre>"},{"location":"api/continuiti/discrete/#continuiti.discrete.UniformBoxSampler.__call__","title":"<code>__call__(n)</code>","text":"<p>Generates <code>n</code> uniformly distributed samples within the n-dimensional box.</p> PARAMETER  DESCRIPTION <code>n</code> <p>Number of samples to draw.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Samples as tensor of shape (dim, n).</p> Source code in <code>src/continuiti/discrete/uniform.py</code> <pre><code>def __call__(self, n: int) -&gt; torch.Tensor:\n\"\"\"Generates `n` uniformly distributed samples within the n-dimensional box.\n    Args:\n        n: Number of samples to draw.\n    Returns:\n        Samples as tensor of shape (dim, n).\n    \"\"\"\nsample = torch.rand((n, self.ndim))\nx = sample * self.x_delta + self.x_min\nreturn x.permute(1, 0)\n</code></pre>"},{"location":"api/continuiti/discrete/#continuiti.discrete.RegularGridSampler","title":"<code>RegularGridSampler(x_min, x_max, prefer_more_samples=True)</code>","text":"<p>             Bases: <code>BoxSampler</code></p> <p>Regular Grid sampler class.</p> <p>A class for generating regularly spaced samples within an n-dimensional box defined by its minimum and maximum corner points. This sampler creates a regular grid evenly spaced in each dimension, trying to create sub-boxes that are as close to cubical as possible.</p> <p>If cubical sub-boxes are not possible (e.g., drawing 8 samples from a unit square as 8 is not a power of 2), the <code>prefer_more_samples</code> flag determines of we either over or undersample the domain: If the product of the number of samples in each dimension does not equal the requested number of samples, the most under/over-sampled dimension will gain/lose one sample.</p> PARAMETER  DESCRIPTION <code>x_min</code> <p>The minimum corner point of the n-dimensional box, specifying the start of each dimension.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> <code>x_max</code> <p>The maximum corner point of the n-dimensional box, specifying the end of each dimension.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> <code>prefer_more_samples</code> <p>Flag indicating whether to prefer a sample count slightly above (True) or below (False) the desired total if an exact match isn't possible due to the properties of the regular grid. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <p><pre><code>min_corner = torch.tensor([0, 0, 0])  # Define the minimum corner of the box\nmax_corner = torch.tensor([1, 1, 1])  # Define the maximum corner of the box\nsampler = RegularGridSampler(min_corner, max_corner, prefer_more_samples=True)\nsamples = sampler(100)\nprint(samples.shape)\n</code></pre> Output: <pre><code>torch.Size([125, 3])\n</code></pre></p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __init__(\nself,\nx_min: Union[torch.Tensor, list],\nx_max: Union[torch.Tensor, list],\nprefer_more_samples: bool = True,\n):\nsuper().__init__(x_min, x_max)\nself.prefer_more_samples = prefer_more_samples\nif torch.allclose(self.x_delta, torch.zeros(self.x_delta.shape)):\n# all samples are drawn from the same point\nself.x_aspect = torch.zeros(self.x_delta.shape)\nself.x_aspect[0] = 1.0\nelse:\nabs_x_delta = torch.abs(self.x_delta)\nself.x_aspect = abs_x_delta / torch.sum(abs_x_delta)\n</code></pre>"},{"location":"api/continuiti/discrete/#continuiti.discrete.RegularGridSampler.__call__","title":"<code>__call__(n_samples)</code>","text":"<p>Generate a uniformly spaced grid of samples within an n-dimensional box.</p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>The number of samples to generate.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor containing the samples of shape (ndim, ~n_samples, ...) as a grid.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __call__(self, n_samples: int) -&gt; torch.Tensor:\n\"\"\"Generate a uniformly spaced grid of samples within an n-dimensional box.\n    Args:\n        n_samples: The number of samples to generate.\n    Returns:\n        Tensor containing the samples of shape (ndim, ~n_samples, ...) as a grid.\n    \"\"\"\nsamples_per_dim = self.__calculate_samples_per_dim(n_samples)\nsamples_per_dim = self.__adjust_samples_to_fit(n_samples, samples_per_dim)\n# Generate grid\ngrids = [\ntorch.linspace(start, end, n_samples_dim)\nfor start, end, n_samples_dim in zip(\nself.x_min, self.x_max, samples_per_dim\n)\n]\nmesh = torch.meshgrid(*grids, indexing=\"ij\")\nreturn torch.stack(mesh, dim=-1).permute(-1, *range(self.ndim))\n</code></pre>"},{"location":"api/continuiti/discrete/#continuiti.discrete.RegularGridSampler.__calculate_samples_per_dim","title":"<code>__calculate_samples_per_dim(n_samples)</code>","text":"<p>Calculate the (floating point) number of samples in each dimension to obtain an evenly spaced grid. This method also ensures that there is at least one sample in each dimension. The implemented method is best understood by the following example.</p> Example <p>For <code>x_min = [0, 0, 1]</code>, <code>xmax = [1, 2, 1]</code> and <code>n_samples = 200</code>, this method computes:</p> <p><pre><code>x_aspect = [1/3, 2/3, 0]\nmask = [1, 1, 0]\nscale_fac = 2/9\nrelevant_ndim = 2\nsamples_per_dim = (200 / (2/9))^(1 / 2) = sqrt(900) = 30\nsamples_per_dim = x_aspect*30 = [10, 20, 0]\nsamples_per_dim = max(samples_per_dim, 1) = [10, 20, 1]\n</code></pre> Output: <pre><code>tensor([10, 20, 1])\n</code></pre></p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>Desired total number of samples.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Approximate number of samples for each dimension as a float vector.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __calculate_samples_per_dim(self, n_samples: int) -&gt; torch.Tensor:\n\"\"\"Calculate the (floating point) number of samples in each dimension to\n    obtain an evenly spaced grid. This method also ensures that there is at\n    least one sample in each dimension. The implemented method is best\n    understood by the following example.\n    Example:\n        For `x_min = [0, 0, 1]`, `xmax = [1, 2, 1]` and `n_samples = 200`, this method computes:\n        ```\n        x_aspect = [1/3, 2/3, 0]\n        mask = [1, 1, 0]\n        scale_fac = 2/9\n        relevant_ndim = 2\n        samples_per_dim = (200 / (2/9))^(1 / 2) = sqrt(900) = 30\n        samples_per_dim = x_aspect*30 = [10, 20, 0]\n        samples_per_dim = max(samples_per_dim, 1) = [10, 20, 1]\n        ```\n        Output:\n        ```\n        tensor([10, 20, 1])\n        ```\n    Args:\n        n_samples: Desired total number of samples.\n    Returns:\n        Approximate number of samples for each dimension as a float vector.\n    \"\"\"\nmask = ~torch.isclose(self.x_aspect, torch.zeros(self.x_aspect.shape))\nscale_fac = torch.prod(self.x_aspect[mask])\nrelevant_ndim = torch.sum(mask)\nsamples_per_dim = torch.pow(n_samples / scale_fac, 1 / relevant_ndim)\nsamples_per_dim = self.x_aspect * samples_per_dim\nsamples_per_dim = torch.max(\nsamples_per_dim, torch.ones(samples_per_dim.shape)\n)  # ensure every dimension is sampled\nreturn samples_per_dim\n</code></pre>"},{"location":"api/continuiti/discrete/#continuiti.discrete.RegularGridSampler.__adjust_samples_to_fit","title":"<code>__adjust_samples_to_fit(n_samples, samples_per_dim)</code>","text":"<p>Round and adjust the <code>samples_per_dim</code> to fit the <code>n_samples</code> requirement.</p> <p>The result of <code>__calculate_samples_per_dim</code> is a floating point representation, which is rounded by this method to the next integer value. If the product of the rounded samples equals the required number of samples, we return this number. Otherwise, the most under-sampled dimension or the most over-sampled dimension will gain or lose one sample, according to the <code>prefer_more_samples</code> flag.</p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>Desired total number of samples.</p> <p> TYPE: <code>int</code> </p> <code>samples_per_dim</code> <p>Initial distribution of samples across dimensions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Adjusted number of samples for each dimension as an integer vector.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __adjust_samples_to_fit(\nself, n_samples: int, samples_per_dim: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Round and adjust the `samples_per_dim` to fit the `n_samples` requirement.\n    The result of `__calculate_samples_per_dim` is a floating point\n    representation, which is rounded by this method to the next integer value.\n    If the product of the rounded samples equals the required number of samples,\n    we return this number. Otherwise, the most under-sampled dimension or\n    the most over-sampled dimension will gain or lose one sample, according\n    to the `prefer_more_samples` flag.\n    Args:\n        n_samples: Desired total number of samples.\n        samples_per_dim: Initial distribution of samples across dimensions.\n    Returns:\n        Adjusted number of samples for each dimension as an integer vector.\n    \"\"\"\nsamples_per_dim_int = torch.round(samples_per_dim).to(dtype=torch.int)\ncurrent_total = torch.prod(samples_per_dim_int)\nif current_total == n_samples:\n# no need to adjust anymore\nreturn samples_per_dim_int\nsample_diff = samples_per_dim - samples_per_dim_int\nif current_total &gt; n_samples and not self.prefer_more_samples:\n# decrease samples in most over-sampled dimension\ndim = torch.argmin(sample_diff)\nsamples_per_dim_int[dim] -= 1\nelif current_total &lt; n_samples and self.prefer_more_samples:\n# increase samples in most under-sampled dimension\ndim = torch.argmax(sample_diff)\nsamples_per_dim_int[dim] += 1\nreturn samples_per_dim_int\n</code></pre>"},{"location":"api/continuiti/discrete/box_sampler/","title":"Box sampler","text":"<p><code>continuiti.discrete.box_sampler</code></p> <p>Abstract base class for sampling from n-dimensional boxes.</p>"},{"location":"api/continuiti/discrete/box_sampler/#continuiti.discrete.box_sampler.BoxSampler","title":"<code>BoxSampler(x_min, x_max)</code>","text":"<p>             Bases: <code>Sampler</code>, <code>ABC</code></p> <p>Abstract base class for sampling from n-dimensional boxes.</p> <p>Given two points in \\(\\mathbb{R}^n\\) with coordinates $$ (x_1^{(min)},x_2^{(min)},\\dots,x_n^{(min)}), \\quad (x_1^{(max)},x_2^{(max)},\\dots,x_n^{(max)}), $$ an n-dimensional box \\(B \\subset \\mathbb{R}^n\\) is given by the Cartesian product $$ B = [x_1^{(min)}, x_1^{(max)}] \\times \\dots \\times [x_n^{(min)}, x_n^{(max)}]. $$</p> PARAMETER  DESCRIPTION <code>x_min</code> <p>Minimum corner point of the n-dimensional box.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> <code>x_max</code> <p>Maximum corner point of the n-dimensional box.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> Source code in <code>src/continuiti/discrete/box_sampler.py</code> <pre><code>def __init__(\nself, x_min: Union[torch.Tensor, list], x_max: Union[torch.Tensor, list]\n):\n# Convert lists to tensors\nif isinstance(x_min, list):\nx_min = torch.tensor(x_min)\nif isinstance(x_max, list):\nx_max = torch.tensor(x_max)\nassert x_min.shape == x_max.shape\nassert x_min.ndim == x_max.ndim == 1\nsuper().__init__(len(x_min))\nself.x_min = x_min\nself.x_max = x_max\nself.x_delta = x_max - x_min\n</code></pre>"},{"location":"api/continuiti/discrete/regular_grid/","title":"Regular grid","text":"<p><code>continuiti.discrete.regular_grid</code></p> <p>Samplers sampling on a regular grid from n-dimensional boxes.</p>"},{"location":"api/continuiti/discrete/regular_grid/#continuiti.discrete.regular_grid.RegularGridSampler","title":"<code>RegularGridSampler(x_min, x_max, prefer_more_samples=True)</code>","text":"<p>             Bases: <code>BoxSampler</code></p> <p>Regular Grid sampler class.</p> <p>A class for generating regularly spaced samples within an n-dimensional box defined by its minimum and maximum corner points. This sampler creates a regular grid evenly spaced in each dimension, trying to create sub-boxes that are as close to cubical as possible.</p> <p>If cubical sub-boxes are not possible (e.g., drawing 8 samples from a unit square as 8 is not a power of 2), the <code>prefer_more_samples</code> flag determines of we either over or undersample the domain: If the product of the number of samples in each dimension does not equal the requested number of samples, the most under/over-sampled dimension will gain/lose one sample.</p> PARAMETER  DESCRIPTION <code>x_min</code> <p>The minimum corner point of the n-dimensional box, specifying the start of each dimension.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> <code>x_max</code> <p>The maximum corner point of the n-dimensional box, specifying the end of each dimension.</p> <p> TYPE: <code>Union[Tensor, list]</code> </p> <code>prefer_more_samples</code> <p>Flag indicating whether to prefer a sample count slightly above (True) or below (False) the desired total if an exact match isn't possible due to the properties of the regular grid. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <p><pre><code>min_corner = torch.tensor([0, 0, 0])  # Define the minimum corner of the box\nmax_corner = torch.tensor([1, 1, 1])  # Define the maximum corner of the box\nsampler = RegularGridSampler(min_corner, max_corner, prefer_more_samples=True)\nsamples = sampler(100)\nprint(samples.shape)\n</code></pre> Output: <pre><code>torch.Size([125, 3])\n</code></pre></p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __init__(\nself,\nx_min: Union[torch.Tensor, list],\nx_max: Union[torch.Tensor, list],\nprefer_more_samples: bool = True,\n):\nsuper().__init__(x_min, x_max)\nself.prefer_more_samples = prefer_more_samples\nif torch.allclose(self.x_delta, torch.zeros(self.x_delta.shape)):\n# all samples are drawn from the same point\nself.x_aspect = torch.zeros(self.x_delta.shape)\nself.x_aspect[0] = 1.0\nelse:\nabs_x_delta = torch.abs(self.x_delta)\nself.x_aspect = abs_x_delta / torch.sum(abs_x_delta)\n</code></pre>"},{"location":"api/continuiti/discrete/regular_grid/#continuiti.discrete.regular_grid.RegularGridSampler.__call__","title":"<code>__call__(n_samples)</code>","text":"<p>Generate a uniformly spaced grid of samples within an n-dimensional box.</p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>The number of samples to generate.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor containing the samples of shape (ndim, ~n_samples, ...) as a grid.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __call__(self, n_samples: int) -&gt; torch.Tensor:\n\"\"\"Generate a uniformly spaced grid of samples within an n-dimensional box.\n    Args:\n        n_samples: The number of samples to generate.\n    Returns:\n        Tensor containing the samples of shape (ndim, ~n_samples, ...) as a grid.\n    \"\"\"\nsamples_per_dim = self.__calculate_samples_per_dim(n_samples)\nsamples_per_dim = self.__adjust_samples_to_fit(n_samples, samples_per_dim)\n# Generate grid\ngrids = [\ntorch.linspace(start, end, n_samples_dim)\nfor start, end, n_samples_dim in zip(\nself.x_min, self.x_max, samples_per_dim\n)\n]\nmesh = torch.meshgrid(*grids, indexing=\"ij\")\nreturn torch.stack(mesh, dim=-1).permute(-1, *range(self.ndim))\n</code></pre>"},{"location":"api/continuiti/discrete/regular_grid/#continuiti.discrete.regular_grid.RegularGridSampler.__calculate_samples_per_dim","title":"<code>__calculate_samples_per_dim(n_samples)</code>","text":"<p>Calculate the (floating point) number of samples in each dimension to obtain an evenly spaced grid. This method also ensures that there is at least one sample in each dimension. The implemented method is best understood by the following example.</p> Example <p>For <code>x_min = [0, 0, 1]</code>, <code>xmax = [1, 2, 1]</code> and <code>n_samples = 200</code>, this method computes:</p> <p><pre><code>x_aspect = [1/3, 2/3, 0]\nmask = [1, 1, 0]\nscale_fac = 2/9\nrelevant_ndim = 2\nsamples_per_dim = (200 / (2/9))^(1 / 2) = sqrt(900) = 30\nsamples_per_dim = x_aspect*30 = [10, 20, 0]\nsamples_per_dim = max(samples_per_dim, 1) = [10, 20, 1]\n</code></pre> Output: <pre><code>tensor([10, 20, 1])\n</code></pre></p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>Desired total number of samples.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Approximate number of samples for each dimension as a float vector.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __calculate_samples_per_dim(self, n_samples: int) -&gt; torch.Tensor:\n\"\"\"Calculate the (floating point) number of samples in each dimension to\n    obtain an evenly spaced grid. This method also ensures that there is at\n    least one sample in each dimension. The implemented method is best\n    understood by the following example.\n    Example:\n        For `x_min = [0, 0, 1]`, `xmax = [1, 2, 1]` and `n_samples = 200`, this method computes:\n        ```\n        x_aspect = [1/3, 2/3, 0]\n        mask = [1, 1, 0]\n        scale_fac = 2/9\n        relevant_ndim = 2\n        samples_per_dim = (200 / (2/9))^(1 / 2) = sqrt(900) = 30\n        samples_per_dim = x_aspect*30 = [10, 20, 0]\n        samples_per_dim = max(samples_per_dim, 1) = [10, 20, 1]\n        ```\n        Output:\n        ```\n        tensor([10, 20, 1])\n        ```\n    Args:\n        n_samples: Desired total number of samples.\n    Returns:\n        Approximate number of samples for each dimension as a float vector.\n    \"\"\"\nmask = ~torch.isclose(self.x_aspect, torch.zeros(self.x_aspect.shape))\nscale_fac = torch.prod(self.x_aspect[mask])\nrelevant_ndim = torch.sum(mask)\nsamples_per_dim = torch.pow(n_samples / scale_fac, 1 / relevant_ndim)\nsamples_per_dim = self.x_aspect * samples_per_dim\nsamples_per_dim = torch.max(\nsamples_per_dim, torch.ones(samples_per_dim.shape)\n)  # ensure every dimension is sampled\nreturn samples_per_dim\n</code></pre>"},{"location":"api/continuiti/discrete/regular_grid/#continuiti.discrete.regular_grid.RegularGridSampler.__adjust_samples_to_fit","title":"<code>__adjust_samples_to_fit(n_samples, samples_per_dim)</code>","text":"<p>Round and adjust the <code>samples_per_dim</code> to fit the <code>n_samples</code> requirement.</p> <p>The result of <code>__calculate_samples_per_dim</code> is a floating point representation, which is rounded by this method to the next integer value. If the product of the rounded samples equals the required number of samples, we return this number. Otherwise, the most under-sampled dimension or the most over-sampled dimension will gain or lose one sample, according to the <code>prefer_more_samples</code> flag.</p> PARAMETER  DESCRIPTION <code>n_samples</code> <p>Desired total number of samples.</p> <p> TYPE: <code>int</code> </p> <code>samples_per_dim</code> <p>Initial distribution of samples across dimensions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Adjusted number of samples for each dimension as an integer vector.</p> Source code in <code>src/continuiti/discrete/regular_grid.py</code> <pre><code>def __adjust_samples_to_fit(\nself, n_samples: int, samples_per_dim: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Round and adjust the `samples_per_dim` to fit the `n_samples` requirement.\n    The result of `__calculate_samples_per_dim` is a floating point\n    representation, which is rounded by this method to the next integer value.\n    If the product of the rounded samples equals the required number of samples,\n    we return this number. Otherwise, the most under-sampled dimension or\n    the most over-sampled dimension will gain or lose one sample, according\n    to the `prefer_more_samples` flag.\n    Args:\n        n_samples: Desired total number of samples.\n        samples_per_dim: Initial distribution of samples across dimensions.\n    Returns:\n        Adjusted number of samples for each dimension as an integer vector.\n    \"\"\"\nsamples_per_dim_int = torch.round(samples_per_dim).to(dtype=torch.int)\ncurrent_total = torch.prod(samples_per_dim_int)\nif current_total == n_samples:\n# no need to adjust anymore\nreturn samples_per_dim_int\nsample_diff = samples_per_dim - samples_per_dim_int\nif current_total &gt; n_samples and not self.prefer_more_samples:\n# decrease samples in most over-sampled dimension\ndim = torch.argmin(sample_diff)\nsamples_per_dim_int[dim] -= 1\nelif current_total &lt; n_samples and self.prefer_more_samples:\n# increase samples in most under-sampled dimension\ndim = torch.argmax(sample_diff)\nsamples_per_dim_int[dim] += 1\nreturn samples_per_dim_int\n</code></pre>"},{"location":"api/continuiti/discrete/sampler/","title":"Sampler","text":"<p><code>continuiti.discrete.sampler</code></p> <p>Abstract base class for sampling from domains.</p>"},{"location":"api/continuiti/discrete/sampler/#continuiti.discrete.sampler.Sampler","title":"<code>Sampler(ndim)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for sampling discrete points from a domain.</p> <p>A sampler is a mechanism or process that samples discrete points from a domain based on a specific criterion or distribution.</p> PARAMETER  DESCRIPTION <code>ndim</code> <p>Dimension of the domain.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/continuiti/discrete/sampler.py</code> <pre><code>def __init__(self, ndim: int):\nself.ndim = ndim\n</code></pre>"},{"location":"api/continuiti/discrete/sampler/#continuiti.discrete.sampler.Sampler.__call__","title":"<code>__call__(n)</code>  <code>abstractmethod</code>","text":"<p>Draws samples from a domain.</p> PARAMETER  DESCRIPTION <code>n</code> <p>Number of samples drawn by the sampler from the domain.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Samples as tensor of shape (n, ndim).</p> Source code in <code>src/continuiti/discrete/sampler.py</code> <pre><code>@abstractmethod\ndef __call__(self, n: int) -&gt; torch.Tensor:\n\"\"\"Draws samples from a domain.\n    Args:\n        n: Number of samples drawn by the sampler from the domain.\n    Returns:\n        Samples as tensor of shape (n, ndim).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/discrete/uniform/","title":"Uniform","text":"<p><code>continuiti.discrete.uniform</code></p> <p>Uniform samplers.</p>"},{"location":"api/continuiti/discrete/uniform/#continuiti.discrete.uniform.UniformBoxSampler","title":"<code>UniformBoxSampler(x_min, x_max)</code>","text":"<p>             Bases: <code>BoxSampler</code></p> <p>Sample uniformly from an n-dimensional box.</p> Example <p><pre><code>sampler = UniformBoxSampler(torch.tensor([0, 0]), torch.tensor([1, 1]))\nsamples = sampler(100)\nsamples.shape\n</code></pre> Output: <pre><code>torch.Size([2, 100])\n</code></pre></p> Note <p>Using <code>torch.rand</code> the UniformBoxSampler samples from a right-open interval in every dimension.</p> Source code in <code>src/continuiti/discrete/box_sampler.py</code> <pre><code>def __init__(\nself, x_min: Union[torch.Tensor, list], x_max: Union[torch.Tensor, list]\n):\n# Convert lists to tensors\nif isinstance(x_min, list):\nx_min = torch.tensor(x_min)\nif isinstance(x_max, list):\nx_max = torch.tensor(x_max)\nassert x_min.shape == x_max.shape\nassert x_min.ndim == x_max.ndim == 1\nsuper().__init__(len(x_min))\nself.x_min = x_min\nself.x_max = x_max\nself.x_delta = x_max - x_min\n</code></pre>"},{"location":"api/continuiti/discrete/uniform/#continuiti.discrete.uniform.UniformBoxSampler.__call__","title":"<code>__call__(n)</code>","text":"<p>Generates <code>n</code> uniformly distributed samples within the n-dimensional box.</p> PARAMETER  DESCRIPTION <code>n</code> <p>Number of samples to draw.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Samples as tensor of shape (dim, n).</p> Source code in <code>src/continuiti/discrete/uniform.py</code> <pre><code>def __call__(self, n: int) -&gt; torch.Tensor:\n\"\"\"Generates `n` uniformly distributed samples within the n-dimensional box.\n    Args:\n        n: Number of samples to draw.\n    Returns:\n        Samples as tensor of shape (dim, n).\n    \"\"\"\nsample = torch.rand((n, self.ndim))\nx = sample * self.x_delta + self.x_min\nreturn x.permute(1, 0)\n</code></pre>"},{"location":"api/continuiti/networks/","title":"Networks","text":"<p><code>continuiti.networks</code></p> <p>Networks in continuiti.</p>"},{"location":"api/continuiti/networks/#continuiti.networks.FullyConnected","title":"<code>FullyConnected(input_size, output_size, width, act=None, device=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Fully connected network.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input dimension.</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Output dimension.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Width of the hidden layer.</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/networks/fully_connected.py</code> <pre><code>def __init__(\nself,\ninput_size: int,\noutput_size: int,\nwidth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.inner_layer = torch.nn.Linear(input_size, width, device=device)\nself.outer_layer = torch.nn.Linear(width, output_size, device=device)\nself.act = act or torch.nn.GELU()\nself.norm = torch.nn.LayerNorm(width, device=device)\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.FullyConnected.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuiti/networks/fully_connected.py</code> <pre><code>def forward(self, x: torch.Tensor):\n\"\"\"Forward pass.\"\"\"\nx = self.inner_layer(x)\nx = self.act(x)\nx = self.norm(x)\nx = self.outer_layer(x)\nreturn x\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.DeepResidualNetwork","title":"<code>DeepResidualNetwork(input_size, output_size, width, depth, act=None, device=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Deep residual network.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Size of input tensor</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Size of output tensor</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Width of hidden layers</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>Number of hidden layers</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def __init__(\nself,\ninput_size: int,\noutput_size: int,\nwidth: int,\ndepth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nassert depth &gt;= 1, \"DeepResidualNetwork has at least depth 1.\"\nsuper().__init__()\nself.act = act or torch.nn.GELU()\nself.first_layer = torch.nn.Linear(input_size, width, device=device)\nself.hidden_layers = torch.nn.ModuleList(\n[\nResidualLayer(\nwidth,\nact=self.act,\ndevice=device,\n)\nfor _ in range(1, depth)\n]\n)\nself.last_layer = torch.nn.Linear(width, output_size, device=device)\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.DeepResidualNetwork.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass.\"\"\"\nx = self.first_layer(x)\nx = self.act(x)\nfor layer in self.hidden_layers:\nx = layer(x)\nreturn self.last_layer(x)\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.MultiHeadAttention","title":"<code>MultiHeadAttention(hidden_dim, n_heads, attention=None, dropout_p=0, bias=True)</code>","text":"<p>             Bases: <code>Attention</code></p> <p>Multi-Head Attention module.</p> <p>Module as described in the paper Attention is All you Need with optional bias for the projections. This implementation allows to use attention implementations other than the standard scaled dot product attention implemented by the MultiheadAttention PyTorch module.</p> \\[MultiHead(Q,K,V)=Concat(head_1,\\dots,head_n)W^O + b^O\\] <p>where</p> \\[head_i=Attention(QW_i^Q+b_i^Q, KW_i^K+b_i^K, VW_i^V+b_i^V).\\] PARAMETER  DESCRIPTION <code>hidden_dim</code> <p>dimension of the hidden layers (embedding dimension).</p> <p> TYPE: <code>int</code> </p> <code>n_heads</code> <p>number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>attention</code> <p>implementation of attention (defaults to scaled dot product attention). Needs to have the arguments <code>query</code>, <code>key</code>, <code>value</code>, <code>attn_mask</code>, and <code>dropout_p</code>.</p> <p> TYPE: <code>Attention</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>dropout probability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>bias</code> <p>If True, then the projection onto the different heads is performed with bias.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/continuiti/networks/multi_head_attention.py</code> <pre><code>def __init__(\nself,\nhidden_dim: int,\nn_heads: int,\nattention: Attention = None,\ndropout_p: float = 0,\nbias: bool = True,\n):\nsuper().__init__()\nself.hidden_dim = hidden_dim\nself.n_heads = n_heads\nself.dropout_p = dropout_p\nself.bias = bias\nif attention is None:\nattention = ScaledDotProductAttention()\nself.attention = attention\nself.head_dim = hidden_dim // n_heads\nassert (\nself.head_dim * n_heads == hidden_dim\n), \"hidden_dim must be divisible by n_heads\"\n# projection networks\nself.query_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.key_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.value_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.out_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.MultiHeadAttention.forward","title":"<code>forward(query, key, value, attn_mask=None)</code>","text":"<p>Compute the attention scores.</p> PARAMETER  DESCRIPTION <code>query</code> <p>Query tensor of shape (batch_size, target_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>key</code> <p>Key tensor of shape (batch_size, source_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>value</code> <p>Value tensor of shape (batch_size, source_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>attn_mask</code> <p>Attention mask of shape (batch_size, target_sequence_length, source_sequence_length).</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Attention scores of shape (batch_size, target_sequence_length, hidden_dim).</p> Source code in <code>src/continuiti/networks/multi_head_attention.py</code> <pre><code>def forward(\nself,\nquery: torch.Tensor,\nkey: torch.Tensor,\nvalue: torch.Tensor,\nattn_mask: torch.Tensor = None,\n) -&gt; torch.Tensor:\nr\"\"\"Compute the attention scores.\n    Args:\n        query: Query tensor of shape (batch_size, target_sequence_length, hidden_dim).\n        key: Key tensor of shape (batch_size, source_sequence_length, hidden_dim).\n        value: Value tensor of shape (batch_size, source_sequence_length, hidden_dim).\n        attn_mask: Attention mask of shape (batch_size, target_sequence_length, source_sequence_length).\n    Returns:\n        Attention scores of shape (batch_size, target_sequence_length, hidden_dim).\n    \"\"\"\nassert query.ndim == key.ndim == value.ndim == 3, (\n\"Query, key, and value need to have three dimensions (batch_size, ..., hidden_dim). This format ensures that\"\n\"the module can correctly apply the multi-head attention mechanism, which includes splitting embeddings \"\n\"into multiple heads, applying the internal attention implementation for each head, concatenating and \"\n\"projecting results, while ensuring that the attention mask is applied correctly.\"\n)\nassert (\nquery.size(0) == key.size(0) == value.size(0)\n), \"Batch size does not match for input tensors\"\nassert (\nquery.size(-1) == key.size(-1) == value.size(-1)\n), \"Embedding/hidden dimension does not match for input tensors\"\nbatch_size = query.size(0)\nsrc_len = key.size(1)\ntgt_len = query.size(1)\n# project values\nquery = self.query_project(query)\nkey = self.key_project(key)\nvalue = self.value_project(value)\n# form individual heads\nquery = query.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\nkey = key.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\nvalue = value.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n# reshape attention mask to match heads\nif attn_mask is not None:\nassert (\nattn_mask.size(0) == batch_size\n), \"Attention mask batch size does not match input tensors.\"\nassert (\nattn_mask.size(1) == tgt_len\n), \"First dimension of the attention mask needs to match target length.\"\nassert (\nattn_mask.size(2) == src_len\n), \"Second dimension of the attention mask needs to match source length.\"\nattn_mask = attn_mask.unsqueeze(1)  # mask for a single head\nattn_mask = attn_mask.repeat(1, self.n_heads, 1, 1)  # mask for every head\n# perform attention\nattn_out = self.attention(\nquery=query,\nkey=key,\nvalue=value,\nattn_mask=attn_mask,\n)\nattn_out = attn_out.transpose(1, 2).reshape(batch_size, -1, self.hidden_dim)\n# output projection\nreturn self.out_project(attn_out)\n</code></pre>"},{"location":"api/continuiti/networks/#continuiti.networks.ScaledDotProductAttention","title":"<code>ScaledDotProductAttention(dropout_p=0.0)</code>","text":"<p>             Bases: <code>Attention</code></p> <p>Scaled dot product attention module.</p> <p>This module is a wrapper for the torch implementation of the scaled dot product attention mechanism as described in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). This attention mechanism computes the attention weights based on the dot product of the query and key matrices, scaled by the square root of the dimension of the key vectors. The weights are then applied to the value vectors to obtain the final output.</p> Source code in <code>src/continuiti/networks/scaled_dot_product_attention.py</code> <pre><code>def __init__(self, dropout_p: float = 0.0):\nsuper().__init__()\nself.dropout_p = dropout_p\n</code></pre>"},{"location":"api/continuiti/networks/attention/","title":"Attention","text":"<p><code>continuiti.networks.attention</code></p> <p>Attention base class in continuiti.</p>"},{"location":"api/continuiti/networks/attention/#continuiti.networks.attention.Attention","title":"<code>Attention()</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base class for various attention implementations.</p> <p>Attention assigns different parts of an input varying importance without set kernels. The importance of different components is designated using \"soft\" weights. These weights are assigned according to specific algorithms (e.g. scaled-dot-product attention).</p> Source code in <code>src/continuiti/networks/attention.py</code> <pre><code>def __init__(self):\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/networks/attention/#continuiti.networks.attention.Attention.forward","title":"<code>forward(query, key, value, attn_mask=None)</code>  <code>abstractmethod</code>","text":"<p>Calculates the attention scores.</p> PARAMETER  DESCRIPTION <code>query</code> <p>query tensor; shape (batch_size, target_seq_length, hidden_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>key</code> <p>key tensor; shape (batch_size, source_seq_length, hidden_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>value</code> <p>value tensor; shape (batch_size, source_seq_length, hidden_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>attn_mask</code> <p>tensor indicating which values are used to calculate the output; shape (batch_size, target_seq_length, source_seq_length)</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>tensor containing the outputs of the attention implementation; shape (batch_size, target_seq_length, hidden_dim)</p> Source code in <code>src/continuiti/networks/attention.py</code> <pre><code>@abstractmethod\ndef forward(\nself,\nquery: torch.Tensor,\nkey: torch.Tensor,\nvalue: torch.Tensor,\nattn_mask: torch.Tensor = None,\n) -&gt; torch.Tensor:\n\"\"\"Calculates the attention scores.\n    Args:\n        query: query tensor; shape (batch_size, target_seq_length, hidden_dim)\n        key: key tensor; shape (batch_size, source_seq_length, hidden_dim)\n        value: value tensor; shape (batch_size, source_seq_length, hidden_dim)\n        attn_mask: tensor indicating which values are used to calculate the output;\n            shape (batch_size, target_seq_length, source_seq_length)\n    Returns:\n        tensor containing the outputs of the attention implementation;\n            shape (batch_size, target_seq_length, hidden_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/networks/deep_residual_network/","title":"Deep residual network","text":"<p><code>continuiti.networks.deep_residual_network</code></p> <p>Deep residual network in continuiti.</p>"},{"location":"api/continuiti/networks/deep_residual_network/#continuiti.networks.deep_residual_network.ResidualLayer","title":"<code>ResidualLayer(width, act=None, device=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Residual layer.</p> PARAMETER  DESCRIPTION <code>width</code> <p>Width of the layer.</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def __init__(\nself,\nwidth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.layer = torch.nn.Linear(width, width, device=device)\nself.act = act or torch.nn.GELU()\nself.norm = torch.nn.LayerNorm(width, device=device)\n</code></pre>"},{"location":"api/continuiti/networks/deep_residual_network/#continuiti.networks.deep_residual_network.ResidualLayer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def forward(self, x: torch.Tensor):\n\"\"\"Forward pass.\"\"\"\nreturn self.norm(self.act(self.layer(x))) + x\n</code></pre>"},{"location":"api/continuiti/networks/deep_residual_network/#continuiti.networks.deep_residual_network.DeepResidualNetwork","title":"<code>DeepResidualNetwork(input_size, output_size, width, depth, act=None, device=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Deep residual network.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Size of input tensor</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Size of output tensor</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Width of hidden layers</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>Number of hidden layers</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def __init__(\nself,\ninput_size: int,\noutput_size: int,\nwidth: int,\ndepth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nassert depth &gt;= 1, \"DeepResidualNetwork has at least depth 1.\"\nsuper().__init__()\nself.act = act or torch.nn.GELU()\nself.first_layer = torch.nn.Linear(input_size, width, device=device)\nself.hidden_layers = torch.nn.ModuleList(\n[\nResidualLayer(\nwidth,\nact=self.act,\ndevice=device,\n)\nfor _ in range(1, depth)\n]\n)\nself.last_layer = torch.nn.Linear(width, output_size, device=device)\n</code></pre>"},{"location":"api/continuiti/networks/deep_residual_network/#continuiti.networks.deep_residual_network.DeepResidualNetwork.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuiti/networks/deep_residual_network.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass.\"\"\"\nx = self.first_layer(x)\nx = self.act(x)\nfor layer in self.hidden_layers:\nx = layer(x)\nreturn self.last_layer(x)\n</code></pre>"},{"location":"api/continuiti/networks/fully_connected/","title":"Fully connected","text":"<p><code>continuiti.networks.fully_connected</code></p> <p>Fully connected neural network in continuiti.</p>"},{"location":"api/continuiti/networks/fully_connected/#continuiti.networks.fully_connected.FullyConnected","title":"<code>FullyConnected(input_size, output_size, width, act=None, device=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Fully connected network.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input dimension.</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Output dimension.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Width of the hidden layer.</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/networks/fully_connected.py</code> <pre><code>def __init__(\nself,\ninput_size: int,\noutput_size: int,\nwidth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.inner_layer = torch.nn.Linear(input_size, width, device=device)\nself.outer_layer = torch.nn.Linear(width, output_size, device=device)\nself.act = act or torch.nn.GELU()\nself.norm = torch.nn.LayerNorm(width, device=device)\n</code></pre>"},{"location":"api/continuiti/networks/fully_connected/#continuiti.networks.fully_connected.FullyConnected.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuiti/networks/fully_connected.py</code> <pre><code>def forward(self, x: torch.Tensor):\n\"\"\"Forward pass.\"\"\"\nx = self.inner_layer(x)\nx = self.act(x)\nx = self.norm(x)\nx = self.outer_layer(x)\nreturn x\n</code></pre>"},{"location":"api/continuiti/networks/multi_head_attention/","title":"Multi head attention","text":"<p><code>continuiti.networks.multi_head_attention</code></p> <p>Multi-Head-Attention in continuiti.</p>"},{"location":"api/continuiti/networks/multi_head_attention/#continuiti.networks.multi_head_attention.MultiHeadAttention","title":"<code>MultiHeadAttention(hidden_dim, n_heads, attention=None, dropout_p=0, bias=True)</code>","text":"<p>             Bases: <code>Attention</code></p> <p>Multi-Head Attention module.</p> <p>Module as described in the paper Attention is All you Need with optional bias for the projections. This implementation allows to use attention implementations other than the standard scaled dot product attention implemented by the MultiheadAttention PyTorch module.</p> \\[MultiHead(Q,K,V)=Concat(head_1,\\dots,head_n)W^O + b^O\\] <p>where</p> \\[head_i=Attention(QW_i^Q+b_i^Q, KW_i^K+b_i^K, VW_i^V+b_i^V).\\] PARAMETER  DESCRIPTION <code>hidden_dim</code> <p>dimension of the hidden layers (embedding dimension).</p> <p> TYPE: <code>int</code> </p> <code>n_heads</code> <p>number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>attention</code> <p>implementation of attention (defaults to scaled dot product attention). Needs to have the arguments <code>query</code>, <code>key</code>, <code>value</code>, <code>attn_mask</code>, and <code>dropout_p</code>.</p> <p> TYPE: <code>Attention</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>dropout probability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>bias</code> <p>If True, then the projection onto the different heads is performed with bias.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/continuiti/networks/multi_head_attention.py</code> <pre><code>def __init__(\nself,\nhidden_dim: int,\nn_heads: int,\nattention: Attention = None,\ndropout_p: float = 0,\nbias: bool = True,\n):\nsuper().__init__()\nself.hidden_dim = hidden_dim\nself.n_heads = n_heads\nself.dropout_p = dropout_p\nself.bias = bias\nif attention is None:\nattention = ScaledDotProductAttention()\nself.attention = attention\nself.head_dim = hidden_dim // n_heads\nassert (\nself.head_dim * n_heads == hidden_dim\n), \"hidden_dim must be divisible by n_heads\"\n# projection networks\nself.query_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.key_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.value_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\nself.out_project = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n</code></pre>"},{"location":"api/continuiti/networks/multi_head_attention/#continuiti.networks.multi_head_attention.MultiHeadAttention.forward","title":"<code>forward(query, key, value, attn_mask=None)</code>","text":"<p>Compute the attention scores.</p> PARAMETER  DESCRIPTION <code>query</code> <p>Query tensor of shape (batch_size, target_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>key</code> <p>Key tensor of shape (batch_size, source_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>value</code> <p>Value tensor of shape (batch_size, source_sequence_length, hidden_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>attn_mask</code> <p>Attention mask of shape (batch_size, target_sequence_length, source_sequence_length).</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Attention scores of shape (batch_size, target_sequence_length, hidden_dim).</p> Source code in <code>src/continuiti/networks/multi_head_attention.py</code> <pre><code>def forward(\nself,\nquery: torch.Tensor,\nkey: torch.Tensor,\nvalue: torch.Tensor,\nattn_mask: torch.Tensor = None,\n) -&gt; torch.Tensor:\nr\"\"\"Compute the attention scores.\n    Args:\n        query: Query tensor of shape (batch_size, target_sequence_length, hidden_dim).\n        key: Key tensor of shape (batch_size, source_sequence_length, hidden_dim).\n        value: Value tensor of shape (batch_size, source_sequence_length, hidden_dim).\n        attn_mask: Attention mask of shape (batch_size, target_sequence_length, source_sequence_length).\n    Returns:\n        Attention scores of shape (batch_size, target_sequence_length, hidden_dim).\n    \"\"\"\nassert query.ndim == key.ndim == value.ndim == 3, (\n\"Query, key, and value need to have three dimensions (batch_size, ..., hidden_dim). This format ensures that\"\n\"the module can correctly apply the multi-head attention mechanism, which includes splitting embeddings \"\n\"into multiple heads, applying the internal attention implementation for each head, concatenating and \"\n\"projecting results, while ensuring that the attention mask is applied correctly.\"\n)\nassert (\nquery.size(0) == key.size(0) == value.size(0)\n), \"Batch size does not match for input tensors\"\nassert (\nquery.size(-1) == key.size(-1) == value.size(-1)\n), \"Embedding/hidden dimension does not match for input tensors\"\nbatch_size = query.size(0)\nsrc_len = key.size(1)\ntgt_len = query.size(1)\n# project values\nquery = self.query_project(query)\nkey = self.key_project(key)\nvalue = self.value_project(value)\n# form individual heads\nquery = query.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\nkey = key.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\nvalue = value.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n# reshape attention mask to match heads\nif attn_mask is not None:\nassert (\nattn_mask.size(0) == batch_size\n), \"Attention mask batch size does not match input tensors.\"\nassert (\nattn_mask.size(1) == tgt_len\n), \"First dimension of the attention mask needs to match target length.\"\nassert (\nattn_mask.size(2) == src_len\n), \"Second dimension of the attention mask needs to match source length.\"\nattn_mask = attn_mask.unsqueeze(1)  # mask for a single head\nattn_mask = attn_mask.repeat(1, self.n_heads, 1, 1)  # mask for every head\n# perform attention\nattn_out = self.attention(\nquery=query,\nkey=key,\nvalue=value,\nattn_mask=attn_mask,\n)\nattn_out = attn_out.transpose(1, 2).reshape(batch_size, -1, self.hidden_dim)\n# output projection\nreturn self.out_project(attn_out)\n</code></pre>"},{"location":"api/continuiti/networks/scaled_dot_product_attention/","title":"Scaled dot product attention","text":"<p><code>continuiti.networks.scaled_dot_product_attention</code></p> <p>Scaled dot product attention module.</p>"},{"location":"api/continuiti/networks/scaled_dot_product_attention/#continuiti.networks.scaled_dot_product_attention.ScaledDotProductAttention","title":"<code>ScaledDotProductAttention(dropout_p=0.0)</code>","text":"<p>             Bases: <code>Attention</code></p> <p>Scaled dot product attention module.</p> <p>This module is a wrapper for the torch implementation of the scaled dot product attention mechanism as described in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). This attention mechanism computes the attention weights based on the dot product of the query and key matrices, scaled by the square root of the dimension of the key vectors. The weights are then applied to the value vectors to obtain the final output.</p> Source code in <code>src/continuiti/networks/scaled_dot_product_attention.py</code> <pre><code>def __init__(self, dropout_p: float = 0.0):\nsuper().__init__()\nself.dropout_p = dropout_p\n</code></pre>"},{"location":"api/continuiti/operators/","title":"Operators","text":"<p><code>continuiti.operators</code></p> <p>Operators in continuiti.</p> <p>Every operator maps collocation points <code>x</code>, function values <code>u</code>, and evaluation points <code>y</code> to evaluations of <code>v</code>:</p> <pre><code>v = operator(x, u, y)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator","title":"<code>Operator(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Operator base class.</p> <p>An operator is a neural network model that maps functions by mapping an observation to the evaluations of the mapped function at given coordinates.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>Optional[OperatorShapes]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>OperatorShapes</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator.forward","title":"<code>forward(x, u, y)</code>  <code>abstractmethod</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>@abstractmethod\ndef forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator.save","title":"<code>save(path)</code>","text":"<p>Save the operator to a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def save(self, path: str):\n\"\"\"Save the operator to a file.\n    Args:\n        path: Path to the file.\n    \"\"\"\ntorch.save(self.state_dict(), path)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator.load","title":"<code>load(path)</code>","text":"<p>Load the operator from a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def load(self, path: str):\n\"\"\"Load the operator from a file.\n    Args:\n        path: Path to the file.\n    \"\"\"\nself.load_state_dict(torch.load(path))\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator.num_params","title":"<code>num_params()</code>","text":"<p>Return the number of trainable parameters.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def num_params(self) -&gt; int:\n\"\"\"Return the number of trainable parameters.\"\"\"\nreturn sum(p.numel() for p in self.parameters())\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.Operator.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the operator.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __str__(self):\n\"\"\"Return string representation of the operator.\"\"\"\nreturn self.__class__.__name__\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.NeuralOperator","title":"<code>NeuralOperator(shapes, layers, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Neural operator architecture</p> <p>Maps continuous functions given as observation to another continuous function and returns point-wise evaluations. The architecture is a stack of continuous kernel integrations with a lifting layer and a projection layer.</p> <p>Reference: N. Kovachki et al. Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs. JMLR 24 1-97 (2023)</p> <p>For now, sensor positions are equal across all layers.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the input and output data.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>layers</code> <p>List of operator layers.</p> <p> TYPE: <code>List[Operator]</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nlayers: List[Operator],\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.layers = torch.nn.ModuleList(layers)\nself.act = act or torch.nn.GELU()\nself.first_dim = layers[0].shapes.u.dim\nself.last_dim = layers[-1].shapes.v.dim\nassert self.shapes.x == layers[0].shapes.x\nassert self.shapes.u.size == layers[0].shapes.u.size\nassert self.shapes.y == layers[-1].shapes.y\nassert self.shapes.v.size == layers[-1].shapes.v.size\nself.lifting = torch.nn.Linear(self.shapes.u.dim, self.first_dim, device=device)\nself.projection = torch.nn.Linear(\nself.last_dim, self.shapes.v.dim, device=device\n)\nself.W = torch.nn.ModuleList(\n[\ntorch.nn.Linear(layer.shapes.u.dim, layer.shapes.v.dim, device=device)\nfor layer in layers[:-1]\n]\n)\nself.norms = torch.nn.ModuleList(\n[\ntorch.nn.LayerNorm(layer.shapes.v.dim, device=device)\nfor layer in layers[:-1]\n]\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.NeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Coordinates where the mapped function is evaluated of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/neuraloperator.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Coordinates where the mapped function is evaluated of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nassert u.shape[1:] == torch.Size([self.shapes.u.dim, *self.shapes.u.size])\n# Lifting\nu = u.permute(0, *range(2, u.dim()), 1)\nv = self.lifting(u)\n# Hidden layers\nfor layer, W, norm in zip(self.layers[:-1], self.W, self.norms):\nv1 = v.permute(0, -1, *range(1, v.dim() - 1))\nv1 = layer(x, v1, x)\nv1 = v1.permute(0, *range(2, v1.dim()), 1)\nv = v1 + W(v)\nv = self.act(v)\nv = norm(v)\n# Last layer (evaluates y)\nv = v.permute(0, -1, *range(1, v.dim() - 1))\nv = self.layers[-1](x, v, y)\nv = v.permute(0, *range(2, v.dim()), 1)\n# Projection\nw = self.projection(v)\nw = w.permute(0, -1, *range(1, w.dim() - 1))\nassert w.shape[1:] == torch.Size([self.shapes.v.dim, *y.size()[2:]])\nreturn w\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepONet","title":"<code>DeepONet(shapes, branch_width=32, branch_depth=3, trunk_width=32, trunk_depth=3, basis_functions=8, act=None, device=None, branch_network=None, trunk_network=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Maps continuous functions given as observation to another continuous function and returns point-wise evaluations. The architecture is inspired by the universal approximation theorem for operators.</p> <p>Reference: Lu Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell 3 218-229 (2021)</p> <p>Note: This operator is not discretization invariant, i.e., it assumes that all observations were evaluated at the same positions.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>branch_width</code> <p>Width of branch network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of branch network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>trunk_width</code> <p>Width of trunk network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of trunk network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>basis_functions</code> <p>Number of basis functions.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>act</code> <p>Activation function used in default trunk and branch networks.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>branch_network</code> <p>Custom branch network that maps input function evaluations to <code>basis_functions</code> many coefficients (if set, branch_width and branch_depth will be ignored).</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>trunk_network</code> <p>Custom trunk network that maps <code>shapes.y.dim</code>-dimensional evaluation coordinates to <code>basis_functions</code> many basis function evaluations (if set, trunk_width and trunk_depth will be ignored).</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/deeponet.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nbranch_width: int = 32,\nbranch_depth: int = 3,\ntrunk_width: int = 32,\ntrunk_depth: int = 3,\nbasis_functions: int = 8,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\nbranch_network: Optional[torch.nn.Module] = None,\ntrunk_network: Optional[torch.nn.Module] = None,\n):\nsuper().__init__(shapes, device)\n# trunk network\nif trunk_network is not None:\nself.trunk = trunk_network\nself.trunk.to(device)\nelse:\nself.trunk = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=shapes.v.dim * basis_functions,\nwidth=trunk_width,\ndepth=trunk_depth,\nact=act,\ndevice=device,\n)\n# branch network\nif branch_network is not None:\nself.branch = branch_network\nself.branch.to(device)\nelse:\nbranch_input_dim = math.prod(shapes.u.size) * shapes.u.dim\nself.branch = torch.nn.Sequential(\ntorch.nn.Flatten(),\nDeepResidualNetwork(\ninput_size=branch_input_dim,\noutput_size=shapes.v.dim * basis_functions,\nwidth=branch_width,\ndepth=branch_depth,\nact=act,\ndevice=device,\n),\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepONet.forward","title":"<code>forward(_, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>_</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Operator output (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/deeponet.py</code> <pre><code>def forward(\nself, _: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        _: Ignored.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Operator output (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nassert u.size(0) == y.size(0)\ny_num = y.shape[2:]\n# flatten inputs for trunk network\ny = y.swapaxes(1, -1).flatten(0, -2)\nassert y.shape[-1:] == torch.Size([self.shapes.y.dim])\n# Pass through branch network\nb = self.branch(u)\n# Pass through trunk network\nt = self.trunk(y)\nassert b.shape[1:] == t.shape[1:], (\nf\"Branch network output of shape {b.shape[1:]} does not match \"\nf\"trunk network output of shape {t.shape[1:]}\"\n)\n# determine basis functions dynamically\nbasis_functions = b.shape[1] // self.shapes.v.dim\n# dot product\nb = b.reshape(-1, self.shapes.v.dim, basis_functions)\nt = t.reshape(\nb.size(0),\n-1,\nself.shapes.v.dim,\nbasis_functions,\n)\ndot_prod = torch.einsum(\"abcd,acd-&gt;acb\", t, b)\ndot_prod = dot_prod.reshape(-1, self.shapes.v.dim, *y_num)\nreturn dot_prod\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.BelNet","title":"<code>BelNet(shapes, K=8, N_1=32, D_1=3, N_2=32, D_2=3, a_x=None, a_u=None, a_y=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The BelNet architecture is an extension of the DeepONet architecture that adds a learnable projection basis network to interpolate the sensor inputs. Therefore, it supports changing sensor positions, or in other terms, is discretization invariant.</p> <p>Reference: Z. Zhang et al. BelNet: basis enhanced learning, a mesh-free neural operator. Proceedings of the royal society A (2023).</p> <p>Note: In the paper, you can use Figure 6 for reference, but we swapped the notation of <code>x</code> and <code>y</code> to comply with the convention in continuiti, where <code>x</code> is the collocation points and <code>y</code> is the evaluation points. We also replace the single layer projection and construction networks by more expressive deep residual networks.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>K</code> <p>Number of basis functions</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>N_1</code> <p>Width of the projection basis network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>D_1</code> <p>Depth of the projection basis network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>N_2</code> <p>Width of the construction network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>D_2</code> <p>Depth of the construction network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>a_x</code> <p>Activation function of projection networks. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>a_u</code> <p>Activation function applied after the projection. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>a_y</code> <p>Activation function of the construction network. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/belnet.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nK: int = 8,\nN_1: int = 32,\nD_1: int = 3,\nN_2: int = 32,\nD_2: int = 3,\na_x: Optional[torch.nn.Module] = None,\na_u: Optional[torch.nn.Module] = None,\na_y: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.K = K\nself.a_x = a_x or torch.nn.Tanh()\nself.a_u = a_u or torch.nn.Tanh()\nself.a_y = a_y or torch.nn.Tanh()\nself.Nx = math.prod(self.shapes.x.size) * self.shapes.x.dim\nself.Nu = math.prod(self.shapes.u.size) * self.shapes.u.dim\nself.Kv = K * self.shapes.v.dim\n# K projection nets\nself.p = DeepResidualNetwork(\ninput_size=self.Nx,\noutput_size=self.Nu * K,\nwidth=N_1,\ndepth=D_1,\nact=self.a_x,\ndevice=device,\n)\n# construction net\nself.q = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=self.Kv,\nwidth=N_2,\ndepth=D_2,\nact=self.a_y,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.BelNet.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Operator output (batch_size, v_dim, num_evaluations...)</p> Source code in <code>src/continuiti/operators/belnet.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...)\n        u: Input function values of shape (batch_size, u_dim, num_sensors...)\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...)\n    Returns:\n        Operator output (batch_size, v_dim, num_evaluations...)\n    \"\"\"\nassert x.size(0) == u.size(0) == y.size(0)\ny_size = y.size()[2:]\nnum_evaluations = math.prod(y.size()[2:])\n# flatten inputs\nx = x.reshape(-1, self.Nx)\nu = u.reshape(-1, self.Nu)\ny = y.reshape(-1, self.shapes.y.dim)\n# build projection matrix\nP = self.p(x)\nP = P.reshape(-1, self.K, self.Nu)\n# perform the projection\naPu = self.a_u(torch.einsum(\"bkn,bn-&gt;bk\", P, u))\nassert aPu.shape[1:] == torch.Size([self.K])\n# construction net\nQ = self.q(y)\nassert Q.shape[1:] == torch.Size([self.Kv])\n# dot product\nQ = Q.reshape(-1, num_evaluations, self.K, self.shapes.v.dim)\noutput = torch.einsum(\"bk,bckv-&gt;bvc\", aPu, Q)\noutput = output.reshape(-1, self.shapes.v.dim, *y_size)\nreturn output\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepNeuralOperator","title":"<code>DeepNeuralOperator(shapes, width=32, depth=3, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The <code>DeepNeuralOperator</code> class integrates a deep residual network within a neural operator framework. It uses all input locations, input values, and the evaluation point as input for a deep residual network.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>An instance of <code>OperatorShapes</code>.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>width</code> <p>The width of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>depth</code> <p>The depth of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/dno.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nwidth: int = 32,\ndepth: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.width = width\nself.depth = depth\nself.x_num = math.prod(shapes.x.size)\nself.u_num = math.prod(shapes.u.size)\nself.net_input_size = (\nshapes.x.dim * self.x_num + shapes.u.dim * self.u_num + shapes.y.dim\n)\nself.net = DeepResidualNetwork(\ninput_size=self.net_input_size,\noutput_size=shapes.v.dim,\nwidth=width,\ndepth=depth,\nact=act,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepNeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> <p>Performs the forward pass through the operator, processing the input function values <code>u</code> and input function probe locations <code>x</code> by flattening them. They are then expanded to match the dimensions of the evaluation coordinates y. The preprocessed x, preprocessed u, and y are stacked and passed through a deep residual network.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input coordinates of shape (batch_size, x_dim, num_sensors...), representing the points in space at which the input function values are probed.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...), representing the values of the input functions at different sensor locations.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...), representing the points in space at which the output function values are to be computed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The output of the operator, of shape (batch_size, v_dim, num_evaluations...), representing the computed function values at the specified evaluation coordinates.</p> Source code in <code>src/continuiti/operators/dno.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Performs the forward pass through the operator, processing the input function values `u` and input function\n    probe locations `x` by flattening them. They are then expanded to match the dimensions of the evaluation\n    coordinates y. The preprocessed x, preprocessed u, and y are stacked and passed through a deep residual network.\n    Args:\n        x: Input coordinates of shape (batch_size, x_dim, num_sensors...), representing the points in space at\n            which the input function values are probed.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...), representing the values of the input\n            functions at different sensor locations.\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...), representing the points in space at\n            which the output function values are to be computed.\n    Returns:\n        The output of the operator, of shape (batch_size, v_dim, num_evaluations...), representing the computed function\n            values at the specified evaluation coordinates.\n    \"\"\"\nbatch_size = u.size(0)\ny_num = math.prod(y.size()[2:])\nu_repeated = u.flatten(1, -1).unsqueeze(1).expand(-1, y_num, -1)\nassert u_repeated.shape == (batch_size, y_num, self.shapes.u.dim * self.u_num)\nx_repeated = x.flatten(1, -1).unsqueeze(1).expand(-1, y_num, -1)\nassert x_repeated.shape == (batch_size, y_num, self.shapes.x.dim * self.x_num)\ny_flatten = y.flatten(2, -1).transpose(1, 2)\nassert y_flatten.shape == (batch_size, y_num, self.shapes.y.dim)\nnet_input = torch.cat([x_repeated, u_repeated, y_flatten], dim=-1)\nassert net_input.shape == (batch_size, y_num, self.net_input_size)\nnet_output = self.net(net_input)\nassert net_output.shape == (batch_size, y_num, self.shapes.v.dim)\nnet_output = net_output.transpose(1, 2)\nassert net_output.shape == (batch_size, self.shapes.v.dim, y_num)\nnet_output = net_output.reshape(batch_size, self.shapes.v.dim, *y.size()[2:])\nreturn net_output\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.FourierNeuralOperator","title":"<code>FourierNeuralOperator(shapes, depth=3, width=3, act=None, device=None, **kwargs)</code>","text":"<p>             Bases: <code>NeuralOperator</code></p> <p>Fourier Neural Operator (FNO) architecture</p> <p>Reference: Z. Li et al. Fourier Neural Operator for Parametric Partial   Differential Equations arXiv:2010.08895 (2020)</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the input and output data.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>depth</code> <p>Number of Fourier layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>width</code> <p>Latent dimension of the Fourier layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for the Fourier layers.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/continuiti/operators/fno.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\ndepth: int = 3,\nwidth: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n**kwargs,\n):\nlatent_shapes = OperatorShapes(\nx=shapes.x,\nu=TensorShape(width, shapes.u.size),\ny=shapes.x,\nv=TensorShape(width, shapes.u.size),\n)\noutput_shapes = OperatorShapes(\nx=shapes.x,\nu=TensorShape(width, shapes.u.size),\ny=shapes.y,\nv=TensorShape(width, shapes.v.size),\n)\nlayers = []\nfor _ in range(depth - 1):\nlayers += [FourierLayer(latent_shapes, device=device, **kwargs)]\nlayers += [FourierLayer(output_shapes, device=device, **kwargs)]\nlayers = torch.nn.ModuleList(layers)\nsuper().__init__(shapes, layers, act, device)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.OperatorShapes","title":"<code>OperatorShapes(x, u, y, v)</code>  <code>dataclass</code>","text":"<p>Shape of input and output functions of an Operator.</p> ATTRIBUTE DESCRIPTION <code>x</code> <p>Sensor locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>u</code> <p>Input function evaluated at sensor locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>y</code> <p>Evaluation locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>v</code> <p>Output function evaluated at evaluation locations.</p> <p> TYPE: <code>TensorShape</code> </p>"},{"location":"api/continuiti/operators/#continuiti.operators.ConvolutionalNeuralNetwork","title":"<code>ConvolutionalNeuralNetwork(shapes, width=16, depth=3, kernel_size=3, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The <code>ConvolutionalNeuralNetwork</code> class is a convolutional neural network that can be viewed at as an operator on a fixed grid.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>An instance of <code>OperatorShapes</code>.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>width</code> <p>The number hidden channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>depth</code> <p>The number of hidden layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>kernel_size</code> <p>The size of the convolutional kernel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/cnn.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nwidth: int = 16,\ndepth: int = 3,\nkernel_size: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nassert depth &gt;= 1, \"Depth is at least one.\"\nsuper().__init__(shapes, device)\nself.act = torch.nn.Tanh() if act is None else act\npadding = kernel_size // 2\nassert shapes.x.dim in [1, 2, 3], \"Only 1D, 2D, and 3D grids supported.\"\nConv = [torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d][shapes.x.dim - 1]\nself.first_layer = Conv(\nshapes.u.dim, width, kernel_size=kernel_size, padding=padding, device=device\n)\nself.hidden_layers = torch.nn.ModuleList(\nConv(width, width, kernel_size=kernel_size, padding=padding, device=device)\nfor _ in range(depth - 1)\n)\nself.last_layer = Conv(\nwidth, shapes.v.dim, kernel_size=kernel_size, padding=padding, device=device\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.ConvolutionalNeuralNetwork.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> <p>Performs the forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The output of the operator, of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/cnn.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Performs the forward pass through the operator.\n    Args:\n        x: Ignored.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Ignored.\n    Returns:\n        The output of the operator, of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# Convolutional layers\nresidual = u\nu = self.act(self.first_layer(u))\nfor layer in self.hidden_layers:\nu = self.act(layer(u))\nu = self.last_layer(u) + residual\nreturn u\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepCatOperator","title":"<code>DeepCatOperator(shapes, branch_width=32, branch_depth=4, trunk_width=32, trunk_depth=4, branch_cat_ratio=0.5, cat_net_width=32, cat_net_depth=4, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Deep Cat Operator.</p> <p>This class implements the DeepCatOperator, a neural operator inspired by the DeepONet.</p> <p>It consists of three main parts:</p> <ol> <li>Branch Network: Processes the sensor inputs (<code>u</code>).</li> <li>Trunk Network: Processes the evaluation locations (<code>y</code>).</li> <li>Cat Network: Combines the outputs from the Branch- and Trunk-Network to produce the final output.</li> </ol> <p>The architecture has the following structure:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 *Branch Network*    \u2502    \u2502 *Trunk Network*    \u2502\n\u2502  Input (u)          \u2502    \u2502 Input (y)          \u2502\n\u2502  Output (b)         \u2502    \u2502 Output (t)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2510\n\u2502 *Concatenation*                               \u2502\n\u2502 Input (b, t)                                  \u2502\n\u2502 Output (c)                                    \u2502\n\u2502 branch_cat_ratio = b.numel() / cat_net_width  \u2502\n\u2514 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2518\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 *Cat Network*    \u2502\n              \u2502  Input (c)       \u2502\n              \u2502  Output (v)      \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This allows the operator to integrate evaluation locations earlier, while ensuring that both the sensor inputs and the evaluation location contribute in a predictable form to the flow of information. Directly stacking both the sensors and evaluation location can lead to an imbalance in the number of features in the neural operator. The arg <code>branch_cat_ratio</code> dictates how this fraction is set (defaults to 50/50). The cat-network does not require the neural operator to learn good basis functions with the trunk network only. The information from the input space and the evaluation locations can be taken into account early, allowing for better abstraction.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>branch_width</code> <p>Width of the branch net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of the branch net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>trunk_width</code> <p>Width of the trunk net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of the trunk net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>branch_cat_ratio</code> <p>Ratio indicating which fraction of the concatenated tensor originates from the branch net. Controls flow of information into branch- and trunk-net. Defaults to 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>cat_net_width</code> <p>Width of the cat net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>cat_net_depth</code> <p>Depth of the cat net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>act</code> <p>Activation function. Defaults to Tanh.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/dco.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nbranch_width: int = 32,\nbranch_depth: int = 4,\ntrunk_width: int = 32,\ntrunk_depth: int = 4,\nbranch_cat_ratio: float = 0.5,\ncat_net_width: int = 32,\ncat_net_depth: int = 4,\nact: Optional[nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes=shapes, device=device)\nif act is None:\nact = nn.Tanh()\nassert (\n0.0 &lt; branch_cat_ratio &lt; 1.0\n), f\"Ratio has to be in (0, 1), but found {branch_cat_ratio}\"\nbranch_out_width = ceil(cat_net_width * branch_cat_ratio)\nassert (\nbranch_out_width != cat_net_width\n), f\"Input cat ratio {branch_cat_ratio} results in eval net width equal zero.\"\ninput_in_width = prod(shapes.u.size) * shapes.u.dim\nself.branch_net = DeepResidualNetwork(\ninput_size=input_in_width,\noutput_size=branch_out_width,\nwidth=branch_width,\ndepth=branch_depth,\nact=act,\ndevice=device,\n)\neval_out_width = cat_net_width - branch_out_width\nself.trunk_net = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=eval_out_width,\nwidth=trunk_width,\ndepth=trunk_depth,\nact=act,\ndevice=device,\n)\nself.cat_act = act  # no activation before first and after last layer\nself.cat_net = DeepResidualNetwork(\ninput_size=cat_net_width,\noutput_size=shapes.v.dim,\nwidth=cat_net_width,\ndepth=cat_net_depth,\nact=act,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/#continuiti.operators.DeepCatOperator.forward","title":"<code>forward(_, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>_</code> <p>Tensor containing sensor locations. Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor containing values of sensors of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor containing evaluation locations of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of predicted evaluation values of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/dco.py</code> <pre><code>def forward(\nself, _: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        _: Tensor containing sensor locations. Ignored.\n        u: Tensor containing values of sensors of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor containing evaluation locations of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of predicted evaluation values of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nipt = torch.flatten(u, start_dim=1)\nipt = self.branch_net(ipt)\ny_num = y.shape[2:]\neval = y.flatten(start_dim=2).transpose(1, -1)\neval = self.trunk_net(eval)\nipt = ipt.unsqueeze(1).expand(-1, eval.size(1), -1)\ncat = torch.cat([ipt, eval], dim=-1)\nout = self.cat_act(cat)\nout = self.cat_net(out)\nreturn out.reshape(-1, self.shapes.v.dim, *y_num)\n</code></pre>"},{"location":"api/continuiti/operators/belnet/","title":"Belnet","text":"<p><code>continuiti.operators.belnet</code></p> <p>The BelNet architecture.</p>"},{"location":"api/continuiti/operators/belnet/#continuiti.operators.belnet.BelNet","title":"<code>BelNet(shapes, K=8, N_1=32, D_1=3, N_2=32, D_2=3, a_x=None, a_u=None, a_y=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The BelNet architecture is an extension of the DeepONet architecture that adds a learnable projection basis network to interpolate the sensor inputs. Therefore, it supports changing sensor positions, or in other terms, is discretization invariant.</p> <p>Reference: Z. Zhang et al. BelNet: basis enhanced learning, a mesh-free neural operator. Proceedings of the royal society A (2023).</p> <p>Note: In the paper, you can use Figure 6 for reference, but we swapped the notation of <code>x</code> and <code>y</code> to comply with the convention in continuiti, where <code>x</code> is the collocation points and <code>y</code> is the evaluation points. We also replace the single layer projection and construction networks by more expressive deep residual networks.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>K</code> <p>Number of basis functions</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>N_1</code> <p>Width of the projection basis network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>D_1</code> <p>Depth of the projection basis network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>N_2</code> <p>Width of the construction network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>D_2</code> <p>Depth of the construction network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>a_x</code> <p>Activation function of projection networks. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>a_u</code> <p>Activation function applied after the projection. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>a_y</code> <p>Activation function of the construction network. Default: Tanh</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/belnet.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nK: int = 8,\nN_1: int = 32,\nD_1: int = 3,\nN_2: int = 32,\nD_2: int = 3,\na_x: Optional[torch.nn.Module] = None,\na_u: Optional[torch.nn.Module] = None,\na_y: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.K = K\nself.a_x = a_x or torch.nn.Tanh()\nself.a_u = a_u or torch.nn.Tanh()\nself.a_y = a_y or torch.nn.Tanh()\nself.Nx = math.prod(self.shapes.x.size) * self.shapes.x.dim\nself.Nu = math.prod(self.shapes.u.size) * self.shapes.u.dim\nself.Kv = K * self.shapes.v.dim\n# K projection nets\nself.p = DeepResidualNetwork(\ninput_size=self.Nx,\noutput_size=self.Nu * K,\nwidth=N_1,\ndepth=D_1,\nact=self.a_x,\ndevice=device,\n)\n# construction net\nself.q = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=self.Kv,\nwidth=N_2,\ndepth=D_2,\nact=self.a_y,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/belnet/#continuiti.operators.belnet.BelNet.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Operator output (batch_size, v_dim, num_evaluations...)</p> Source code in <code>src/continuiti/operators/belnet.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...)\n        u: Input function values of shape (batch_size, u_dim, num_sensors...)\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...)\n    Returns:\n        Operator output (batch_size, v_dim, num_evaluations...)\n    \"\"\"\nassert x.size(0) == u.size(0) == y.size(0)\ny_size = y.size()[2:]\nnum_evaluations = math.prod(y.size()[2:])\n# flatten inputs\nx = x.reshape(-1, self.Nx)\nu = u.reshape(-1, self.Nu)\ny = y.reshape(-1, self.shapes.y.dim)\n# build projection matrix\nP = self.p(x)\nP = P.reshape(-1, self.K, self.Nu)\n# perform the projection\naPu = self.a_u(torch.einsum(\"bkn,bn-&gt;bk\", P, u))\nassert aPu.shape[1:] == torch.Size([self.K])\n# construction net\nQ = self.q(y)\nassert Q.shape[1:] == torch.Size([self.Kv])\n# dot product\nQ = Q.reshape(-1, num_evaluations, self.K, self.shapes.v.dim)\noutput = torch.einsum(\"bk,bckv-&gt;bvc\", aPu, Q)\noutput = output.reshape(-1, self.shapes.v.dim, *y_size)\nreturn output\n</code></pre>"},{"location":"api/continuiti/operators/cnn/","title":"Cnn","text":"<p><code>continuiti.operators.cnn</code></p> <p>The ConvolutionalNeuralNetwork (CNN) architecture.</p>"},{"location":"api/continuiti/operators/cnn/#continuiti.operators.cnn.ConvolutionalNeuralNetwork","title":"<code>ConvolutionalNeuralNetwork(shapes, width=16, depth=3, kernel_size=3, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The <code>ConvolutionalNeuralNetwork</code> class is a convolutional neural network that can be viewed at as an operator on a fixed grid.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>An instance of <code>OperatorShapes</code>.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>width</code> <p>The number hidden channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>depth</code> <p>The number of hidden layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>kernel_size</code> <p>The size of the convolutional kernel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/cnn.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nwidth: int = 16,\ndepth: int = 3,\nkernel_size: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nassert depth &gt;= 1, \"Depth is at least one.\"\nsuper().__init__(shapes, device)\nself.act = torch.nn.Tanh() if act is None else act\npadding = kernel_size // 2\nassert shapes.x.dim in [1, 2, 3], \"Only 1D, 2D, and 3D grids supported.\"\nConv = [torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d][shapes.x.dim - 1]\nself.first_layer = Conv(\nshapes.u.dim, width, kernel_size=kernel_size, padding=padding, device=device\n)\nself.hidden_layers = torch.nn.ModuleList(\nConv(width, width, kernel_size=kernel_size, padding=padding, device=device)\nfor _ in range(depth - 1)\n)\nself.last_layer = Conv(\nwidth, shapes.v.dim, kernel_size=kernel_size, padding=padding, device=device\n)\n</code></pre>"},{"location":"api/continuiti/operators/cnn/#continuiti.operators.cnn.ConvolutionalNeuralNetwork.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> <p>Performs the forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The output of the operator, of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/cnn.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Performs the forward pass through the operator.\n    Args:\n        x: Ignored.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Ignored.\n    Returns:\n        The output of the operator, of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# Convolutional layers\nresidual = u\nu = self.act(self.first_layer(u))\nfor layer in self.hidden_layers:\nu = self.act(layer(u))\nu = self.last_layer(u) + residual\nreturn u\n</code></pre>"},{"location":"api/continuiti/operators/dco/","title":"Dco","text":"<p><code>continuiti.operators.dco</code></p> <p>The DeepCatOperator (DCO) architecture.</p>"},{"location":"api/continuiti/operators/dco/#continuiti.operators.dco.DeepCatOperator","title":"<code>DeepCatOperator(shapes, branch_width=32, branch_depth=4, trunk_width=32, trunk_depth=4, branch_cat_ratio=0.5, cat_net_width=32, cat_net_depth=4, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Deep Cat Operator.</p> <p>This class implements the DeepCatOperator, a neural operator inspired by the DeepONet.</p> <p>It consists of three main parts:</p> <ol> <li>Branch Network: Processes the sensor inputs (<code>u</code>).</li> <li>Trunk Network: Processes the evaluation locations (<code>y</code>).</li> <li>Cat Network: Combines the outputs from the Branch- and Trunk-Network to produce the final output.</li> </ol> <p>The architecture has the following structure:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 *Branch Network*    \u2502    \u2502 *Trunk Network*    \u2502\n\u2502  Input (u)          \u2502    \u2502 Input (y)          \u2502\n\u2502  Output (b)         \u2502    \u2502 Output (t)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2510\n\u2502 *Concatenation*                               \u2502\n\u2502 Input (b, t)                                  \u2502\n\u2502 Output (c)                                    \u2502\n\u2502 branch_cat_ratio = b.numel() / cat_net_width  \u2502\n\u2514 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2534 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2518\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 *Cat Network*    \u2502\n              \u2502  Input (c)       \u2502\n              \u2502  Output (v)      \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This allows the operator to integrate evaluation locations earlier, while ensuring that both the sensor inputs and the evaluation location contribute in a predictable form to the flow of information. Directly stacking both the sensors and evaluation location can lead to an imbalance in the number of features in the neural operator. The arg <code>branch_cat_ratio</code> dictates how this fraction is set (defaults to 50/50). The cat-network does not require the neural operator to learn good basis functions with the trunk network only. The information from the input space and the evaluation locations can be taken into account early, allowing for better abstraction.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>branch_width</code> <p>Width of the branch net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of the branch net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>trunk_width</code> <p>Width of the trunk net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of the trunk net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>branch_cat_ratio</code> <p>Ratio indicating which fraction of the concatenated tensor originates from the branch net. Controls flow of information into branch- and trunk-net. Defaults to 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>cat_net_width</code> <p>Width of the cat net (deep residual network). Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>cat_net_depth</code> <p>Depth of the cat net (deep residual network). Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>act</code> <p>Activation function. Defaults to Tanh.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/dco.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nbranch_width: int = 32,\nbranch_depth: int = 4,\ntrunk_width: int = 32,\ntrunk_depth: int = 4,\nbranch_cat_ratio: float = 0.5,\ncat_net_width: int = 32,\ncat_net_depth: int = 4,\nact: Optional[nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes=shapes, device=device)\nif act is None:\nact = nn.Tanh()\nassert (\n0.0 &lt; branch_cat_ratio &lt; 1.0\n), f\"Ratio has to be in (0, 1), but found {branch_cat_ratio}\"\nbranch_out_width = ceil(cat_net_width * branch_cat_ratio)\nassert (\nbranch_out_width != cat_net_width\n), f\"Input cat ratio {branch_cat_ratio} results in eval net width equal zero.\"\ninput_in_width = prod(shapes.u.size) * shapes.u.dim\nself.branch_net = DeepResidualNetwork(\ninput_size=input_in_width,\noutput_size=branch_out_width,\nwidth=branch_width,\ndepth=branch_depth,\nact=act,\ndevice=device,\n)\neval_out_width = cat_net_width - branch_out_width\nself.trunk_net = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=eval_out_width,\nwidth=trunk_width,\ndepth=trunk_depth,\nact=act,\ndevice=device,\n)\nself.cat_act = act  # no activation before first and after last layer\nself.cat_net = DeepResidualNetwork(\ninput_size=cat_net_width,\noutput_size=shapes.v.dim,\nwidth=cat_net_width,\ndepth=cat_net_depth,\nact=act,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/dco/#continuiti.operators.dco.DeepCatOperator.forward","title":"<code>forward(_, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>_</code> <p>Tensor containing sensor locations. Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor containing values of sensors of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor containing evaluation locations of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of predicted evaluation values of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/dco.py</code> <pre><code>def forward(\nself, _: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        _: Tensor containing sensor locations. Ignored.\n        u: Tensor containing values of sensors of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor containing evaluation locations of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of predicted evaluation values of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nipt = torch.flatten(u, start_dim=1)\nipt = self.branch_net(ipt)\ny_num = y.shape[2:]\neval = y.flatten(start_dim=2).transpose(1, -1)\neval = self.trunk_net(eval)\nipt = ipt.unsqueeze(1).expand(-1, eval.size(1), -1)\ncat = torch.cat([ipt, eval], dim=-1)\nout = self.cat_act(cat)\nout = self.cat_net(out)\nreturn out.reshape(-1, self.shapes.v.dim, *y_num)\n</code></pre>"},{"location":"api/continuiti/operators/deeponet/","title":"Deeponet","text":"<p><code>continuiti.operators.deeponet</code></p> <p>The DeepONet architecture.</p>"},{"location":"api/continuiti/operators/deeponet/#continuiti.operators.deeponet.DeepONet","title":"<code>DeepONet(shapes, branch_width=32, branch_depth=3, trunk_width=32, trunk_depth=3, basis_functions=8, act=None, device=None, branch_network=None, trunk_network=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Maps continuous functions given as observation to another continuous function and returns point-wise evaluations. The architecture is inspired by the universal approximation theorem for operators.</p> <p>Reference: Lu Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell 3 218-229 (2021)</p> <p>Note: This operator is not discretization invariant, i.e., it assumes that all observations were evaluated at the same positions.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>branch_width</code> <p>Width of branch network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of branch network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>trunk_width</code> <p>Width of trunk network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of trunk network.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>basis_functions</code> <p>Number of basis functions.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>act</code> <p>Activation function used in default trunk and branch networks.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>branch_network</code> <p>Custom branch network that maps input function evaluations to <code>basis_functions</code> many coefficients (if set, branch_width and branch_depth will be ignored).</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>trunk_network</code> <p>Custom trunk network that maps <code>shapes.y.dim</code>-dimensional evaluation coordinates to <code>basis_functions</code> many basis function evaluations (if set, trunk_width and trunk_depth will be ignored).</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/deeponet.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nbranch_width: int = 32,\nbranch_depth: int = 3,\ntrunk_width: int = 32,\ntrunk_depth: int = 3,\nbasis_functions: int = 8,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\nbranch_network: Optional[torch.nn.Module] = None,\ntrunk_network: Optional[torch.nn.Module] = None,\n):\nsuper().__init__(shapes, device)\n# trunk network\nif trunk_network is not None:\nself.trunk = trunk_network\nself.trunk.to(device)\nelse:\nself.trunk = DeepResidualNetwork(\ninput_size=shapes.y.dim,\noutput_size=shapes.v.dim * basis_functions,\nwidth=trunk_width,\ndepth=trunk_depth,\nact=act,\ndevice=device,\n)\n# branch network\nif branch_network is not None:\nself.branch = branch_network\nself.branch.to(device)\nelse:\nbranch_input_dim = math.prod(shapes.u.size) * shapes.u.dim\nself.branch = torch.nn.Sequential(\ntorch.nn.Flatten(),\nDeepResidualNetwork(\ninput_size=branch_input_dim,\noutput_size=shapes.v.dim * basis_functions,\nwidth=branch_width,\ndepth=branch_depth,\nact=act,\ndevice=device,\n),\n)\n</code></pre>"},{"location":"api/continuiti/operators/deeponet/#continuiti.operators.deeponet.DeepONet.forward","title":"<code>forward(_, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>_</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Operator output (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/deeponet.py</code> <pre><code>def forward(\nself, _: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        _: Ignored.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Operator output (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nassert u.size(0) == y.size(0)\ny_num = y.shape[2:]\n# flatten inputs for trunk network\ny = y.swapaxes(1, -1).flatten(0, -2)\nassert y.shape[-1:] == torch.Size([self.shapes.y.dim])\n# Pass through branch network\nb = self.branch(u)\n# Pass through trunk network\nt = self.trunk(y)\nassert b.shape[1:] == t.shape[1:], (\nf\"Branch network output of shape {b.shape[1:]} does not match \"\nf\"trunk network output of shape {t.shape[1:]}\"\n)\n# determine basis functions dynamically\nbasis_functions = b.shape[1] // self.shapes.v.dim\n# dot product\nb = b.reshape(-1, self.shapes.v.dim, basis_functions)\nt = t.reshape(\nb.size(0),\n-1,\nself.shapes.v.dim,\nbasis_functions,\n)\ndot_prod = torch.einsum(\"abcd,acd-&gt;acb\", t, b)\ndot_prod = dot_prod.reshape(-1, self.shapes.v.dim, *y_num)\nreturn dot_prod\n</code></pre>"},{"location":"api/continuiti/operators/dno/","title":"Dno","text":"<p><code>continuiti.operators.dno</code></p> <p>The Deep Neural Operator (DNO) architecture.</p>"},{"location":"api/continuiti/operators/dno/#continuiti.operators.dno.DeepNeuralOperator","title":"<code>DeepNeuralOperator(shapes, width=32, depth=3, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>The <code>DeepNeuralOperator</code> class integrates a deep residual network within a neural operator framework. It uses all input locations, input values, and the evaluation point as input for a deep residual network.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>An instance of <code>OperatorShapes</code>.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>width</code> <p>The width of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>depth</code> <p>The depth of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function of the <code>DeepResidualNetwork</code>.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/dno.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nwidth: int = 32,\ndepth: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.width = width\nself.depth = depth\nself.x_num = math.prod(shapes.x.size)\nself.u_num = math.prod(shapes.u.size)\nself.net_input_size = (\nshapes.x.dim * self.x_num + shapes.u.dim * self.u_num + shapes.y.dim\n)\nself.net = DeepResidualNetwork(\ninput_size=self.net_input_size,\noutput_size=shapes.v.dim,\nwidth=width,\ndepth=depth,\nact=act,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/dno/#continuiti.operators.dno.DeepNeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> <p>Performs the forward pass through the operator, processing the input function values <code>u</code> and input function probe locations <code>x</code> by flattening them. They are then expanded to match the dimensions of the evaluation coordinates y. The preprocessed x, preprocessed u, and y are stacked and passed through a deep residual network.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input coordinates of shape (batch_size, x_dim, num_sensors...), representing the points in space at which the input function values are probed.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...), representing the values of the input functions at different sensor locations.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...), representing the points in space at which the output function values are to be computed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The output of the operator, of shape (batch_size, v_dim, num_evaluations...), representing the computed function values at the specified evaluation coordinates.</p> Source code in <code>src/continuiti/operators/dno.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Performs the forward pass through the operator, processing the input function values `u` and input function\n    probe locations `x` by flattening them. They are then expanded to match the dimensions of the evaluation\n    coordinates y. The preprocessed x, preprocessed u, and y are stacked and passed through a deep residual network.\n    Args:\n        x: Input coordinates of shape (batch_size, x_dim, num_sensors...), representing the points in space at\n            which the input function values are probed.\n        u: Input function values of shape (batch_size, u_dim, num_sensors...), representing the values of the input\n            functions at different sensor locations.\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...), representing the points in space at\n            which the output function values are to be computed.\n    Returns:\n        The output of the operator, of shape (batch_size, v_dim, num_evaluations...), representing the computed function\n            values at the specified evaluation coordinates.\n    \"\"\"\nbatch_size = u.size(0)\ny_num = math.prod(y.size()[2:])\nu_repeated = u.flatten(1, -1).unsqueeze(1).expand(-1, y_num, -1)\nassert u_repeated.shape == (batch_size, y_num, self.shapes.u.dim * self.u_num)\nx_repeated = x.flatten(1, -1).unsqueeze(1).expand(-1, y_num, -1)\nassert x_repeated.shape == (batch_size, y_num, self.shapes.x.dim * self.x_num)\ny_flatten = y.flatten(2, -1).transpose(1, 2)\nassert y_flatten.shape == (batch_size, y_num, self.shapes.y.dim)\nnet_input = torch.cat([x_repeated, u_repeated, y_flatten], dim=-1)\nassert net_input.shape == (batch_size, y_num, self.net_input_size)\nnet_output = self.net(net_input)\nassert net_output.shape == (batch_size, y_num, self.shapes.v.dim)\nnet_output = net_output.transpose(1, 2)\nassert net_output.shape == (batch_size, self.shapes.v.dim, y_num)\nnet_output = net_output.reshape(batch_size, self.shapes.v.dim, *y.size()[2:])\nreturn net_output\n</code></pre>"},{"location":"api/continuiti/operators/fno/","title":"Fno","text":"<p><code>continuiti.operators.fno</code></p> <p>The Fourier Neural Operator</p>"},{"location":"api/continuiti/operators/fno/#continuiti.operators.fno.FourierNeuralOperator","title":"<code>FourierNeuralOperator(shapes, depth=3, width=3, act=None, device=None, **kwargs)</code>","text":"<p>             Bases: <code>NeuralOperator</code></p> <p>Fourier Neural Operator (FNO) architecture</p> <p>Reference: Z. Li et al. Fourier Neural Operator for Parametric Partial   Differential Equations arXiv:2010.08895 (2020)</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the input and output data.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>depth</code> <p>Number of Fourier layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>width</code> <p>Latent dimension of the Fourier layers.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for the Fourier layers.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/continuiti/operators/fno.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\ndepth: int = 3,\nwidth: int = 3,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n**kwargs,\n):\nlatent_shapes = OperatorShapes(\nx=shapes.x,\nu=TensorShape(width, shapes.u.size),\ny=shapes.x,\nv=TensorShape(width, shapes.u.size),\n)\noutput_shapes = OperatorShapes(\nx=shapes.x,\nu=TensorShape(width, shapes.u.size),\ny=shapes.y,\nv=TensorShape(width, shapes.v.size),\n)\nlayers = []\nfor _ in range(depth - 1):\nlayers += [FourierLayer(latent_shapes, device=device, **kwargs)]\nlayers += [FourierLayer(output_shapes, device=device, **kwargs)]\nlayers = torch.nn.ModuleList(layers)\nsuper().__init__(shapes, layers, act, device)\n</code></pre>"},{"location":"api/continuiti/operators/fourierlayer/","title":"Fourierlayer","text":"<p><code>continuiti.operators.fourierlayer</code></p> <p>The Fourier layer of the Fourier Neural Operator (FNO).</p>"},{"location":"api/continuiti/operators/fourierlayer/#continuiti.operators.fourierlayer.FourierLayer1d","title":"<code>FourierLayer1d(shapes, num_frequencies=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Fourier layer for <code>x.dim = 1</code>.</p> Note <p>This implementation is here for reference only. See <code>FourierLayer</code> for general implementation.</p> Source code in <code>src/continuiti/operators/fourierlayer.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nnum_frequencies: Optional[int] = None,\ndevice: Optional[torch.device] = None,\n) -&gt; None:\nsuper().__init__(shapes, device)\nassert (\nshapes.x.dim == 1\n), f\"This is the implementation of a 1d Fourier operator. However given dimensionality is x.dim = {shapes.x.dim}\"\nx_num = math.prod(shapes.x.size)\nself.num_frequencies = (\nx_num // 2 + 1 if num_frequencies is None else num_frequencies\n)\nassert (\nself.num_frequencies &lt;= x_num // 2 + 1\n), \"num_frequencies is too large. The fft of a real valued function has only (shapes.x.num // 2 + 1) unique frequencies.\"\nassert shapes.u.dim == shapes.v.dim\nshape = (self.num_frequencies, shapes.u.dim, shapes.u.dim)\nweights_real = torch.empty(shape, device=device)\nweights_img = torch.empty(shape, device=device)\n# initialize\nnn.init.kaiming_uniform_(weights_real, a=math.sqrt(5))\nnn.init.kaiming_uniform_(weights_img, a=math.sqrt(5))\nself.weights_complex = torch.complex(weights_real, weights_img)\nself.kernel = torch.nn.Parameter(\ntorch.view_as_real(self.weights_complex)\n)  # NCCL does not support complex numbers\n</code></pre>"},{"location":"api/continuiti/operators/fourierlayer/#continuiti.operators.fourierlayer.FourierLayer1d.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_sensors...).</p> Source code in <code>src/continuiti/operators/fourierlayer.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_sensors...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_sensors...).\n    \"\"\"\nu_fourier = rfft(u, axis=2, norm=\"forward\")\nkernel = torch.view_as_complex(self.kernel)\nout_fourier = torch.einsum(\n\"nds,bsn-&gt;bdn\", kernel, u_fourier[:, : self.num_frequencies, :]\n)\nout = irfft(out_fourier, axis=2, n=y.shape[2], norm=\"forward\")\nreturn out\n</code></pre>"},{"location":"api/continuiti/operators/fourierlayer/#continuiti.operators.fourierlayer.FourierLayer","title":"<code>FourierLayer(shapes, num_modes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Fourier layer. This layer performs an integral kernel operation in Fourier space.</p> <p>The convolution with a kernel becomes an element-wise product in Fourier space, reducing the complexity of the computation from quadratic to linear. For the Fourier transformation pytorch's implementation of the FFT is used.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shape of dataset</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>num_modes</code> <p>List with number of modes per fft dimension. The number of fft-dimensions is equal to shapes.x.dim. If num_modes is None, the maximum number of modes is assumed which is given by the number of points per dimension.</p> <p> TYPE: <code>Optional[Tuple[int]]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Example <pre><code>dataset = OperatorDataset(x, u, y, v)\nfourier = FourierLayer(dataset.shapes)\nxi, ui, yi, vi = dataset[0:1]\nvi_pred = fourier(xi, ui, yi)\n</code></pre> Source code in <code>src/continuiti/operators/fourierlayer.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nnum_modes: Optional[Tuple[int]] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nassert (\nself.shapes.y.dim == self.shapes.x.dim\n), f\"x.dim == y.dim is a necessary requirement for the FourierLayer. Given y.dim={self.shapes.y.dim} x.dim={self.shapes.x.dim}\"\n# make sure self.grid_shape is list\nself.grid_shape = list(self.shapes.u.size)\nself.num_modes = (\nself.grid_shape.copy() if num_modes is None else list(num_modes)\n)\nself.num_modes = [\nmin(grid_dim, mode)\nfor grid_dim, mode in zip(self.grid_shape, self.num_modes)\n]\nassert (\nlen(self.num_modes) == self.shapes.x.dim\n), \"The number of dimensions specified by 'num_modes' is inconsistent with x.dim.\"\n# The last dimension is half+1 because we use the `torch.fft.rfftn` method.\n# This is due to the negative frequency modes being redundant.\nself.num_modes[-1] = self.num_modes[-1] // 2 + 1\nweights_shape = (*self.num_modes, shapes.v.dim, shapes.u.dim)\nweights_real = torch.empty(weights_shape, device=device)\nweights_img = torch.empty(weights_shape, device=device)\nnn.init.kaiming_uniform_(weights_real, a=math.sqrt(5))\nnn.init.kaiming_uniform_(weights_img, a=math.sqrt(5))\nself.weights_complex = torch.complex(weights_real, weights_img)\nself.kernel = torch.nn.Parameter(\ntorch.view_as_real(self.weights_complex)\n)  # NCCL does not support complex numbers\n</code></pre>"},{"location":"api/continuiti/operators/fourierlayer/#continuiti.operators.fourierlayer.FourierLayer.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass. Performs a kernel integral operation in Fourier space.</p> Note <ul> <li><code>x.dim == y.dim</code> is a necessary condition for the FourierLayer.</li> <li><code>x</code> and <code>y</code> have to be sampled on a regular grid.</li> </ul> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/fourierlayer.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass. Performs a kernel integral operation in Fourier space.\n    Note:\n        * `x.dim == y.dim` is a necessary condition for the FourierLayer.\n        * `x` and `y` have to be sampled on a regular grid.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# shapes which can change for different forward passes\nbatch_size = y.shape[0]\n# fft related parameters\nnum_fft_dimensions = self.shapes.x.dim\nfft_dimensions = list(range(1, num_fft_dimensions + 1))\n# prepare for FFT\nu = u.permute(0, *range(2, u.dim()), 1)\nassert u.dim() == num_fft_dimensions + 2\n# compute n-dimensional real-valued fourier transform\nu_fft = rfftn(u, dim=fft_dimensions, norm=\"forward\")\n# transform Fourier modes from 'standard order' to 'ascending order'\nu_fft = self._get_ascending_order(u_fft, dim=fft_dimensions)\n# add or remove frequencies such that the the fft dimensions of u_fft match self.num_modes\nu_fft = self._add_or_remove_frequencies(\nu_fft, target_shape=self.num_modes, dim=fft_dimensions\n)\n# perform kernel integral operation in Fourier space\nout_fft = self._contract_with_kernel(u_fft)\n# the output shape is determined by y.shape[2:] (num_evaluations...) and not x.shape[2:] (num_sensors...)\ntarget_shape = list(y.shape[2:])\n# fft_shape is the same except last dimension, we only need half the frequencies for the last dimension\nfft_shape = target_shape.copy()\nfft_shape[-1] = fft_shape[-1] // 2 + 1\n# add or remove frequencies such that the fft dimensions of out_fft match fft_shape\nout_fft = self._add_or_remove_frequencies(\nout_fft, target_shape=fft_shape, dim=fft_dimensions\n)\n# transform Fourier modes from 'ascending order' to 'normal order'\nout_fft = self._get_standard_order(out_fft, dim=fft_dimensions)\n# transform back into real-space\nout = irfftn(\nout_fft,\ndim=fft_dimensions,\ns=target_shape,\nnorm=\"forward\",\n)\n# match (batch_size, v_dim, num_evaluations...)\nout = out.permute(0, -1, *range(1, out.dim() - 1))\nassert out.shape == (batch_size, self.shapes.v.dim, *y.size()[2:])\nreturn out\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/","title":"Integralkernel","text":"<p><code>continuiti.operators.integralkernel</code></p> <p>Integral kernel operations.</p>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.Kernel","title":"<code>Kernel(shapes, device=None)</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Kernel abstract base class.</p> <p>In general, a kernel is a function</p> \\[\\begin{align*} \\kappa:\\ \\mathbb{R}^{d_x} \\times \\mathbb{R}^{d_y} &amp;\\to \\mathbb{R}^{{d_u}\\times{d_v}}, \\\\         (x, y) &amp;\\mapsto \\kappa(x, y). \\end{align*}\\] <p>In continuiti, we add a batch dimension and the number of sensor points for \\(x\\) and \\(y\\) to enable efficient implementation of the kernel, such that the shapes of the input tensors are</p> <p><pre><code>    x: (batch_size, x_dim, x_num...)\ny: (batch_size, y_dim, y_num...)\n</code></pre> and the kernel output is of shape</p> <pre><code>    (batch_size, u_dim, v_dim, x_num..., y_num...)\n</code></pre> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>def __init__(self, shapes: OperatorShapes, device: Optional[torch.device] = None):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.Kernel.forward","title":"<code>forward(x, y)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of coordinates of shape <code>(batch_size, shapes.x.dim, x_num...)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates of shape <code>(batch_size, shapes.y.dim, y_num...)</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape <code>(batch_size, shapes.u.dim, shapes.v.dim, x_num..., y_num...)</code>.</p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward pass.\n    Args:\n        x: Tensor of coordinates of shape `(batch_size, shapes.x.dim, x_num...)`.\n        y: Tensor of coordinates of shape `(batch_size, shapes.y.dim, y_num...)`.\n    Returns:\n        Tensor of shape `(batch_size, shapes.u.dim, shapes.v.dim, x_num..., y_num...)`.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.NeuralNetworkKernel","title":"<code>NeuralNetworkKernel(shapes, kernel_width, kernel_depth, act=None, device=None)</code>","text":"<p>             Bases: <code>Kernel</code></p> <p>Neural network kernel.</p> <p>The neural network kernel is a kernel where \\(\\kappa(x, y)\\) is parameterized by a simple feed forward neural network with skip connections.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the operator</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>kernel_width</code> <p>Width of kernel network</p> <p> TYPE: <code>int</code> </p> <code>kernel_depth</code> <p>Depth of kernel network</p> <p> TYPE: <code>int</code> </p> <code>act</code> <p>Activation function</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nkernel_width: int,\nkernel_depth: int,\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.shapes = shapes\nself.net = DeepResidualNetwork(\ninput_size=shapes.y.dim + shapes.x.dim,\noutput_size=shapes.u.dim * shapes.v.dim,\nwidth=kernel_width,\ndepth=kernel_depth,\nact=act,\ndevice=device,\n)\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.NeuralNetworkKernel.forward","title":"<code>forward(x, y)</code>","text":"<p>Forward pass.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of coordinates of shape <code>(batch_size, shapes.x.dim, x_num...)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates of shape <code>(batch_size, shapes.y.dim, y_num...)</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape <code>(batch_size, shapes.u.dim, shapes.v.dim, x_num..., y_num...)</code>.</p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward pass.\n    Args:\n        x: Tensor of coordinates of shape `(batch_size, shapes.x.dim, x_num...)`.\n        y: Tensor of coordinates of shape `(batch_size, shapes.y.dim, y_num...)`.\n    Returns:\n        Tensor of shape `(batch_size, shapes.u.dim, shapes.v.dim, x_num..., y_num...)`.\n    \"\"\"\n# shapes that can change for different forward passes\nbatch_size = x.shape[0]\nassert batch_size == y.shape[0]\nx_num, y_num = math.prod(x.shape[2:]), math.prod(y.shape[2:])\n# shapes that are fixed\nx_dim, y_dim = self.shapes.x.dim, self.shapes.y.dim\nu_dim, v_dim = self.shapes.u.dim, self.shapes.v.dim\n# flatten the spatial dimensions and move coordinate to the last dimension\nx_flatten = x.flatten(2, -1).permute(0, 2, 1)\ny_flatten = y.flatten(2, -1).permute(0, 2, 1)\n# In order to evaluate all kernel values k(x_i, y_j), we need every x_i and y_j combination.\nnetwork_input = torch.concat(\n[\nx_flatten.unsqueeze(2).repeat(1, 1, y_num, 1),  # repeat tensor in dim=2\ny_flatten.unsqueeze(1).repeat(1, x_num, 1, 1),  # repeat tensor in dim=1\n],\ndim=-1,\n)\nassert network_input.shape == torch.Size(\n[batch_size, x_num, y_num, x_dim + y_dim]\n)\noutput = self.net(network_input)\nassert output.shape == torch.Size([batch_size, x_num, y_num, u_dim * v_dim])\noutput = output.permute(0, 3, 1, 2)\noutput = output.reshape(batch_size, u_dim, v_dim, *x.shape[2:], *y.shape[2:])\nreturn output\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.NaiveIntegralKernel","title":"<code>NaiveIntegralKernel(kernel)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Naive integral kernel operator.</p> <p>Maps continuous functions via integral kernel application to another continuous function and returns point-wise evaluations.</p> <p>In mathematical terms, for some given \\(y\\), we obtain $$ v(y) = \\int u(x)~\\kappa(x, y)~dx     \\approx \\frac{1}{N} \\sum_{i=1}^{N} u_i~\\kappa(x_i, y) $$ where \\((x_i, u_i)\\) are the \\(N\\) sensors where \\(u\\) is evaluated.</p> Note <p>This implementation is not efficient for a large number of sensors and only serves as a proof of concept. Please refer to other integral kernel operators for more efficient implementations (e.g. Fourier layers).</p> PARAMETER  DESCRIPTION <code>kernel</code> <p>Kernel function.</p> <p> TYPE: <code>Kernel</code> </p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>def __init__(\nself,\nkernel: Kernel,\n):\nsuper().__init__(kernel.shapes, kernel.device)\nself.kernel = kernel\n</code></pre>"},{"location":"api/continuiti/operators/integralkernel/#continuiti.operators.integralkernel.NaiveIntegralKernel.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/integralkernel.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# shapes that can change for different forward passes\nbatch_size = x.shape[0]\nassert batch_size == y.shape[0]\nx_num, y_num = math.prod(x.shape[2:]), math.prod(y.shape[2:])\n# shapes that are fixed\nu_dim, v_dim = self.shapes.u.dim, self.shapes.v.dim\n# Apply the kernel function\nk = self.kernel(x, y)\nassert k.shape == torch.Size(\n[batch_size, u_dim, v_dim, *x.shape[2:], *y.shape[2:]]\n)\nk = k.reshape(batch_size, u_dim, v_dim, x_num, y_num)\n# Compute integral\nu = u.reshape(batch_size, u_dim, x_num)\nintegral = torch.einsum(\"buvxy,bux-&gt;bvy\", k, u) / x_num\nassert integral.shape == torch.Size([batch_size, v_dim, y_num])\nintegral = integral.reshape(batch_size, v_dim, *y.shape[2:])\nreturn integral\n</code></pre>"},{"location":"api/continuiti/operators/losses/","title":"Losses","text":"<p><code>continuiti.operators.losses</code></p> <p>Loss functions for operator learning.</p> <p>Every loss function takes an operator <code>op</code>, sensor positions <code>x</code>, sensor values <code>u</code>, evaluation coordinates <code>y</code>, and labels <code>v</code> as input and returns a scalar loss:</p> <pre><code>loss = loss_fn(op, x, u, y, v)\n</code></pre>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.Loss","title":"<code>Loss</code>","text":"<p>Loss function for training operators in continuiti.</p>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.Loss.__call__","title":"<code>__call__(op, x, u, y, v)</code>  <code>abstractmethod</code>","text":"<p>Evaluate loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object.</p> <p> TYPE: <code>Operator</code> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, v_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/operators/losses.py</code> <pre><code>@abstractmethod\ndef __call__(\nself,\nop: \"Operator\",\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Evaluate loss.\n    Args:\n        op: Operator object.\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Tensor of labels of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.MSELoss","title":"<code>MSELoss()</code>","text":"<p>             Bases: <code>Loss</code></p> <p>Computes the mean-squared error between the predicted and true labels.</p> <pre><code>loss = mse(op(x, u, y), v)\n</code></pre> Source code in <code>src/continuiti/operators/losses.py</code> <pre><code>def __init__(self):\nself.mse = torch.nn.MSELoss()\n</code></pre>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.MSELoss.__call__","title":"<code>__call__(op, x, u, y, v)</code>","text":"<p>Evaluate MSE loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object.</p> <p> TYPE: <code>Operator</code> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, v_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/operators/losses.py</code> <pre><code>def __call__(\nself,\nop: \"Operator\",\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Evaluate MSE loss.\n    Args:\n        op: Operator object.\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Tensor of labels of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Align shapes\nv_pred = v_pred.reshape(v.shape)\n# Return MSE\nreturn self.mse(v_pred, v)\n</code></pre>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.RelativeL1Error","title":"<code>RelativeL1Error()</code>","text":"<p>             Bases: <code>Loss</code></p> <p>Computes the relative L1 error between the predicted and true labels.</p> <pre><code>loss = l1(v, op(x, u, y)) / l1(v, 0)\n</code></pre> Source code in <code>src/continuiti/operators/losses.py</code> <pre><code>def __init__(self):\nself.l1 = torch.nn.L1Loss()\n</code></pre>"},{"location":"api/continuiti/operators/losses/#continuiti.operators.losses.RelativeL1Error.__call__","title":"<code>__call__(op, x, u, y, v)</code>","text":"<p>Evaluate relative L1 error.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object.</p> <p> TYPE: <code>Operator</code> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, v_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/operators/losses.py</code> <pre><code>def __call__(\nself,\nop: \"Operator\",\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Evaluate relative L1 error.\n    Args:\n        op: Operator object.\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Tensor of labels of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Align shapes\nv_pred = v_pred.reshape(v.shape)\n# Return relative L1 error\nreturn self.l1(v, v_pred) / self.l1(v, torch.zeros_like(v))\n</code></pre>"},{"location":"api/continuiti/operators/neuraloperator/","title":"Neuraloperator","text":"<p><code>continuiti.operators.neuraloperator</code></p> <p>Operators can be stacked into a <code>NeuralOperator</code> architecture, which is a stack of continuous convolutions with a lifting layer and a projection layer.</p>"},{"location":"api/continuiti/operators/neuraloperator/#continuiti.operators.neuraloperator.NeuralOperator","title":"<code>NeuralOperator(shapes, layers, act=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Neural operator architecture</p> <p>Maps continuous functions given as observation to another continuous function and returns point-wise evaluations. The architecture is a stack of continuous kernel integrations with a lifting layer and a projection layer.</p> <p>Reference: N. Kovachki et al. Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs. JMLR 24 1-97 (2023)</p> <p>For now, sensor positions are equal across all layers.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Shapes of the input and output data.</p> <p> TYPE: <code>OperatorShapes</code> </p> <code>layers</code> <p>List of operator layers.</p> <p> TYPE: <code>List[Operator]</code> </p> <code>act</code> <p>Activation function.</p> <p> TYPE: <code>Optional[Module]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\nshapes: OperatorShapes,\nlayers: List[Operator],\nact: Optional[torch.nn.Module] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__(shapes, device)\nself.layers = torch.nn.ModuleList(layers)\nself.act = act or torch.nn.GELU()\nself.first_dim = layers[0].shapes.u.dim\nself.last_dim = layers[-1].shapes.v.dim\nassert self.shapes.x == layers[0].shapes.x\nassert self.shapes.u.size == layers[0].shapes.u.size\nassert self.shapes.y == layers[-1].shapes.y\nassert self.shapes.v.size == layers[-1].shapes.v.size\nself.lifting = torch.nn.Linear(self.shapes.u.dim, self.first_dim, device=device)\nself.projection = torch.nn.Linear(\nself.last_dim, self.shapes.v.dim, device=device\n)\nself.W = torch.nn.ModuleList(\n[\ntorch.nn.Linear(layer.shapes.u.dim, layer.shapes.v.dim, device=device)\nfor layer in layers[:-1]\n]\n)\nself.norms = torch.nn.ModuleList(\n[\ntorch.nn.LayerNorm(layer.shapes.v.dim, device=device)\nfor layer in layers[:-1]\n]\n)\n</code></pre>"},{"location":"api/continuiti/operators/neuraloperator/#continuiti.operators.neuraloperator.NeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Coordinates where the mapped function is evaluated of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/neuraloperator.py</code> <pre><code>def forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Coordinates where the mapped function is evaluated of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nassert u.shape[1:] == torch.Size([self.shapes.u.dim, *self.shapes.u.size])\n# Lifting\nu = u.permute(0, *range(2, u.dim()), 1)\nv = self.lifting(u)\n# Hidden layers\nfor layer, W, norm in zip(self.layers[:-1], self.W, self.norms):\nv1 = v.permute(0, -1, *range(1, v.dim() - 1))\nv1 = layer(x, v1, x)\nv1 = v1.permute(0, *range(2, v1.dim()), 1)\nv = v1 + W(v)\nv = self.act(v)\nv = norm(v)\n# Last layer (evaluates y)\nv = v.permute(0, -1, *range(1, v.dim() - 1))\nv = self.layers[-1](x, v, y)\nv = v.permute(0, *range(2, v.dim()), 1)\n# Projection\nw = self.projection(v)\nw = w.permute(0, -1, *range(1, w.dim() - 1))\nassert w.shape[1:] == torch.Size([self.shapes.v.dim, *y.size()[2:]])\nreturn w\n</code></pre>"},{"location":"api/continuiti/operators/operator/","title":"Operator","text":"<p><code>continuiti.operators.operator</code></p> <p>In continuiti, all models for operator learning are based on the <code>Operator</code> base class.</p>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator","title":"<code>Operator(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Operator base class.</p> <p>An operator is a neural network model that maps functions by mapping an observation to the evaluations of the mapped function at given coordinates.</p> PARAMETER  DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>Optional[OperatorShapes]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device.</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>shapes</code> <p>Operator shapes.</p> <p> TYPE: <code>OperatorShapes</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator.forward","title":"<code>forward(x, u, y)</code>  <code>abstractmethod</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>@abstractmethod\ndef forward(\nself, x: torch.Tensor, u: torch.Tensor, y: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator.save","title":"<code>save(path)</code>","text":"<p>Save the operator to a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def save(self, path: str):\n\"\"\"Save the operator to a file.\n    Args:\n        path: Path to the file.\n    \"\"\"\ntorch.save(self.state_dict(), path)\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator.load","title":"<code>load(path)</code>","text":"<p>Load the operator from a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def load(self, path: str):\n\"\"\"Load the operator from a file.\n    Args:\n        path: Path to the file.\n    \"\"\"\nself.load_state_dict(torch.load(path))\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator.num_params","title":"<code>num_params()</code>","text":"<p>Return the number of trainable parameters.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def num_params(self) -&gt; int:\n\"\"\"Return the number of trainable parameters.\"\"\"\nreturn sum(p.numel() for p in self.parameters())\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.Operator.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the operator.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __str__(self):\n\"\"\"Return string representation of the operator.\"\"\"\nreturn self.__class__.__name__\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.MaskedOperator","title":"<code>MaskedOperator(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code>, <code>ABC</code></p> <p>Masked operator base class.</p> <p>A masked operator can apply masks during the forward pass to selectively use or ignore parts of the input. Masked operators allow for different numbers of sensors in addition to the common property of being able to handle varying numbers of evaluations.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/operators/operator/#continuiti.operators.operator.MaskedOperator.forward","title":"<code>forward(x, u, y, sensor_mask=None, eval_mask=None)</code>  <code>abstractmethod</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Input function values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>sensor_mask</code> <p>Boolean mask for x and u of shape (batch_size, 1, num_sensors...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>eval_mask</code> <p>Boolean mask for y of shape (batch_size, 1, num_evaluations...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>@abstractmethod\ndef forward(\nself,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nsensor_mask: Optional[torch.Tensor] = None,\neval_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Input function values of shape (batch_size, u_dim, num_sensors...).\n        y: Evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        sensor_mask: Boolean mask for x and u of shape (batch_size, 1, num_sensors...).\n        eval_mask: Boolean mask for y of shape (batch_size, 1, num_evaluations...).\n    Returns:\n        Evaluations of the mapped function with shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/operators/shape/","title":"Shape","text":"<p><code>continuiti.operators.shape</code></p>"},{"location":"api/continuiti/operators/shape/#continuiti.operators.shape.OperatorShapes","title":"<code>OperatorShapes(x, u, y, v)</code>  <code>dataclass</code>","text":"<p>Shape of input and output functions of an Operator.</p> ATTRIBUTE DESCRIPTION <code>x</code> <p>Sensor locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>u</code> <p>Input function evaluated at sensor locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>y</code> <p>Evaluation locations.</p> <p> TYPE: <code>TensorShape</code> </p> <code>v</code> <p>Output function evaluated at evaluation locations.</p> <p> TYPE: <code>TensorShape</code> </p>"},{"location":"api/continuiti/pde/","title":"Pde","text":"<p><code>continuiti.pde</code></p> <p>This module contains utilities for solving PDEs in continuiti, e.g., physics-informed loss functions.</p>"},{"location":"api/continuiti/pde/#continuiti.pde.Grad","title":"<code>Grad(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Gradient operator.</p> <p>The gradient is a function operator that maps a function to its gradient.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.Grad.forward","title":"<code>forward(x, u, y=None)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nif y is not None:\nassert torch.equal(x, y), \"x and y must be equal for gradient operator\"\nassert x.requires_grad, \"x must require gradients for gradient operator\"\n# Compute gradients\ngradients = torch.autograd.grad(\nu,\nx,\ngrad_outputs=torch.ones_like(u),\ncreate_graph=True,\nretain_graph=True,\n)[0]\nreturn gradients\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.Div","title":"<code>Div(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Divergence operator.</p> <p>The divergence is a function operator that maps a function to its divergence.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.Div.forward","title":"<code>forward(x, u, y=None)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nif y is not None:\nassert torch.equal(x, y), \"x and y must be equal for divergence operator\"\nassert x.requires_grad, \"x must require gradients for divergence operator\"\n# Compute divergence\ngradients = Grad()(x, u)\nreturn torch.sum(gradients, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.PDE","title":"<code>PDE</code>","text":"<p>PDE base class.</p> Example <p>In general, we can implement a PDE like \\(\\nabla v = u\\) as follows:</p> <pre><code>def pde(x, u, y, v):  # v = op(x, u, y)\nv_y = grad(y, v)\nreturn mse(v_y, u)\nloss_fn = PhysicsInformedLoss(pde)\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.PDE.__call__","title":"<code>__call__(x, u, y, v)</code>  <code>abstractmethod</code>","text":"<p>Computes PDE loss.</p> <p>Usually, we have <code>v = op(x, u, y)</code>, e.g., in the physics-informed loss.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of predicted values of shape (batch_size, v_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>@abstractmethod\ndef __call__(\nself,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Computes PDE loss.\n    Usually, we have `v = op(x, u, y)`, e.g., in the physics-informed loss.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Tensor of _predicted_ values of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.PhysicsInformedLoss","title":"<code>PhysicsInformedLoss(pde)</code>","text":"<p>Physics-informed loss function for training operators in continuiti.</p> <pre><code>loss = pde(x, u, y, op(x, u, y))\n</code></pre> PARAMETER  DESCRIPTION <code>pde</code> <p>Maps evaluation coordinates \\(y\\) and callable \\(v\\) to PDE loss.</p> <p> TYPE: <code>PDE</code> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>def __init__(self, pde: PDE):\nself.pde = pde\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.PhysicsInformedLoss.__call__","title":"<code>__call__(op, x, u, y, _)</code>","text":"<p>Evaluate loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object.</p> <p> TYPE: <code>Operator</code> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Ignored.</p> <p> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>def __call__(\nself,\nop: Operator,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\n_: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Evaluate loss.\n    Args:\n        op: Operator object.\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Ignored.\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Get pde loss\nreturn self.pde(x, u, y, v_pred)\n</code></pre>"},{"location":"api/continuiti/pde/#continuiti.pde.div","title":"<code>div(u)</code>","text":"<p>Compute the divergence of a function.</p> Example <p>Computing the divergence of the output function of an operator: <pre><code>v = lambda y: operator(x, u, y)\nd = div(v)(y)\n</code></pre></p> PARAMETER  DESCRIPTION <code>u</code> <p>Function to compute the divergence of.</p> <p> TYPE: <code>Callable[[Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Tensor], Tensor]</code> <p>Function that computes the divergence of the input function.</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def div(u: Callable[[Tensor], Tensor]) -&gt; Callable[[Tensor], Tensor]:\n\"\"\"Compute the divergence of a function.\n    Example:\n        Computing the divergence of the output function of an operator:\n        ```python\n        v = lambda y: operator(x, u, y)\n        d = div(v)(y)\n        ```\n    Args:\n        u: Function to compute the divergence of.\n    Returns:\n        Function that computes the divergence of the input function.\n    \"\"\"\nreturn lambda x: Div()(x, u(x))\n</code></pre>"},{"location":"api/continuiti/pde/grad/","title":"Grad","text":"<p><code>continuiti.pde.grad</code></p> <p>Functional gradients in continuiti.</p> <p>Derivatives are function operators, so it is natural to define them as operators within continuiti.</p> <p>The following gradients define several derivation operators (e.g., grad, div) that simplify the definition of PDEs in physics-informed losses.</p>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.Grad","title":"<code>Grad(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Gradient operator.</p> <p>The gradient is a function operator that maps a function to its gradient.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.Grad.forward","title":"<code>forward(x, u, y=None)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nif y is not None:\nassert torch.equal(x, y), \"x and y must be equal for gradient operator\"\nassert x.requires_grad, \"x must require gradients for gradient operator\"\n# Compute gradients\ngradients = torch.autograd.grad(\nu,\nx,\ngrad_outputs=torch.ones_like(u),\ncreate_graph=True,\nretain_graph=True,\n)[0]\nreturn gradients\n</code></pre>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.Div","title":"<code>Div(shapes=None, device=None)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Divergence operator.</p> <p>The divergence is a function operator that maps a function to its divergence.</p> Source code in <code>src/continuiti/operators/operator.py</code> <pre><code>def __init__(\nself,\nshapes: Optional[OperatorShapes] = None,\ndevice: Optional[torch.device] = None,\n):\nsuper().__init__()\nself.shapes = shapes\nself.device = device\n</code></pre>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.Div.forward","title":"<code>forward(x, u, y=None)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation positions of shape (batch_size, y_dim, num_evaluations...).\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\nif y is not None:\nassert torch.equal(x, y), \"x and y must be equal for divergence operator\"\nassert x.requires_grad, \"x must require gradients for divergence operator\"\n# Compute divergence\ngradients = Grad()(x, u)\nreturn torch.sum(gradients, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.grad","title":"<code>grad(u)</code>","text":"<p>Compute the gradient of a function.</p> Example <p>Computing the gradient of the output function of an operator: <pre><code>v = lambda y: operator(x, u, y)\ng = grad(v)(y)\n</code></pre></p> PARAMETER  DESCRIPTION <code>u</code> <p>Function to compute the gradient of.</p> <p> TYPE: <code>Callable[[Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Tensor], Tensor]</code> <p>Function that computes the gradient of the input function.</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def grad(u: Callable[[Tensor], Tensor]) -&gt; Callable[[Tensor], Tensor]:\n\"\"\"Compute the gradient of a function.\n    Example:\n        Computing the gradient of the output function of an operator:\n        ```python\n        v = lambda y: operator(x, u, y)\n        g = grad(v)(y)\n        ```\n    Args:\n        u: Function to compute the gradient of.\n    Returns:\n        Function that computes the gradient of the input function.\n    \"\"\"\nreturn lambda x: Grad()(x, u(x))\n</code></pre>"},{"location":"api/continuiti/pde/grad/#continuiti.pde.grad.div","title":"<code>div(u)</code>","text":"<p>Compute the divergence of a function.</p> Example <p>Computing the divergence of the output function of an operator: <pre><code>v = lambda y: operator(x, u, y)\nd = div(v)(y)\n</code></pre></p> PARAMETER  DESCRIPTION <code>u</code> <p>Function to compute the divergence of.</p> <p> TYPE: <code>Callable[[Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Tensor], Tensor]</code> <p>Function that computes the divergence of the input function.</p> Source code in <code>src/continuiti/pde/grad.py</code> <pre><code>def div(u: Callable[[Tensor], Tensor]) -&gt; Callable[[Tensor], Tensor]:\n\"\"\"Compute the divergence of a function.\n    Example:\n        Computing the divergence of the output function of an operator:\n        ```python\n        v = lambda y: operator(x, u, y)\n        d = div(v)(y)\n        ```\n    Args:\n        u: Function to compute the divergence of.\n    Returns:\n        Function that computes the divergence of the input function.\n    \"\"\"\nreturn lambda x: Div()(x, u(x))\n</code></pre>"},{"location":"api/continuiti/pde/physicsinformed/","title":"Physicsinformed","text":"<p><code>continuiti.pde.physicsinformed</code></p> <p>PDEs and physics-informed loss functions.</p>"},{"location":"api/continuiti/pde/physicsinformed/#continuiti.pde.physicsinformed.PDE","title":"<code>PDE</code>","text":"<p>PDE base class.</p> Example <p>In general, we can implement a PDE like \\(\\nabla v = u\\) as follows:</p> <pre><code>def pde(x, u, y, v):  # v = op(x, u, y)\nv_y = grad(y, v)\nreturn mse(v_y, u)\nloss_fn = PhysicsInformedLoss(pde)\n</code></pre>"},{"location":"api/continuiti/pde/physicsinformed/#continuiti.pde.physicsinformed.PDE.__call__","title":"<code>__call__(x, u, y, v)</code>  <code>abstractmethod</code>","text":"<p>Computes PDE loss.</p> <p>Usually, we have <code>v = op(x, u, y)</code>, e.g., in the physics-informed loss.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of predicted values of shape (batch_size, v_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>@abstractmethod\ndef __call__(\nself,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\nv: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Computes PDE loss.\n    Usually, we have `v = op(x, u, y)`, e.g., in the physics-informed loss.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Tensor of _predicted_ values of shape (batch_size, v_dim, num_evaluations...).\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/pde/physicsinformed/#continuiti.pde.physicsinformed.PhysicsInformedLoss","title":"<code>PhysicsInformedLoss(pde)</code>","text":"<p>Physics-informed loss function for training operators in continuiti.</p> <pre><code>loss = pde(x, u, y, op(x, u, y))\n</code></pre> PARAMETER  DESCRIPTION <code>pde</code> <p>Maps evaluation coordinates \\(y\\) and callable \\(v\\) to PDE loss.</p> <p> TYPE: <code>PDE</code> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>def __init__(self, pde: PDE):\nself.pde = pde\n</code></pre>"},{"location":"api/continuiti/pde/physicsinformed/#continuiti.pde.physicsinformed.PhysicsInformedLoss.__call__","title":"<code>__call__(op, x, u, y, _)</code>","text":"<p>Evaluate loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object.</p> <p> TYPE: <code>Operator</code> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Ignored.</p> <p> </p> Source code in <code>src/continuiti/pde/physicsinformed.py</code> <pre><code>def __call__(\nself,\nop: Operator,\nx: torch.Tensor,\nu: torch.Tensor,\ny: torch.Tensor,\n_: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Evaluate loss.\n    Args:\n        op: Operator object.\n        x: Tensor of sensor positions of shape (batch_size, x_dim, num_sensors...).\n        u: Tensor of sensor values of shape (batch_size, u_dim, num_sensors...).\n        y: Tensor of evaluation coordinates of shape (batch_size, y_dim, num_evaluations...).\n        v: Ignored.\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Get pde loss\nreturn self.pde(x, u, y, v_pred)\n</code></pre>"},{"location":"api/continuiti/trainer/","title":"Trainer","text":"<p><code>continuiti.trainer</code></p> <p>Trainer for operator learning.</p>"},{"location":"api/continuiti/trainer/#continuiti.trainer.Trainer","title":"<code>Trainer(operator, optimizer=None, lr=0.001, loss_fn=None, device=device, verbose=None)</code>","text":"<p>Trainer implements a default training loop for operator learning.</p> Example <pre><code>from continuiti.trainer import Trainer\nfrom continuiti.operators.losses import MSELoss\n...\noptimizer = torch.optim.Adam(operator.parameters(), lr=1e-3)\nloss_fn = MSELoss()\ntrainer = Trainer(operator, optimizer, loss_fn, device=\"cuda:0\")\ntrainer.fit(dataset, tol=1e-3, epochs=1000)\n</code></pre> PARAMETER  DESCRIPTION <code>operator</code> <p>Operator to be trained.</p> <p> TYPE: <code>Operator</code> </p> <code>optimizer</code> <p>Torch-like optimizer. Default is Adam with learning rate <code>lr</code>.</p> <p> TYPE: <code>Optional[Optimizer]</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>Learning rate. Ignored if optimizer is not None. Default is 1e-3.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>loss_fn</code> <p>Loss function taking (op, x, u, y, v). Default is MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to train on. Default is CPU.</p> <p> TYPE: <code>device</code> DEFAULT: <code>device</code> </p> <code>verbose</code> <p>Print model parameters and use PrintTrainingLoss callback by default. Default is True.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/trainer/trainer.py</code> <pre><code>def __init__(\nself,\noperator: Operator,\noptimizer: Optional[torch.optim.Optimizer] = None,\nlr: float = 1e-3,\nloss_fn: Optional[Loss] = None,\ndevice: torch.device = device,\nverbose: Optional[bool] = None,\n):\nself.operator = operator\nself.optimizer = (\noptimizer\nif optimizer is not None\nelse torch.optim.Adam(operator.parameters(), lr=lr)\n)\nself.loss_fn = loss_fn if loss_fn is not None else MSELoss()\nif isinstance(device, torch.device):\nself.device = device\nelse:\nself.device = torch.device(device)\n# Verbosity\nif self.device.index is not None:\nif verbose is False:\nself.verbose = False\nelse:\nself.verbose = self.device.index == 0\nelse:\nself.verbose = verbose or True\n</code></pre>"},{"location":"api/continuiti/trainer/#continuiti.trainer.Trainer.fit","title":"<code>fit(dataset, tol=1e-05, epochs=1000, callbacks=None, criterion=None, batch_size=32, shuffle=True, test_dataset=None, lr_scheduler=True)</code>","text":"<p>Fit operator to data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> TYPE: <code>OperatorDataset</code> </p> <code>tol</code> <p>Tolerance for stopping criterion. Ignored if criterion is not None.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-05</code> </p> <code>epochs</code> <p>Maximum number of epochs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>callbacks</code> <p>List of additional callbacks.</p> <p> TYPE: <code>Optional[List[Callback]]</code> DEFAULT: <code>None</code> </p> <code>criterion</code> <p>Stopping criterion. Defaults to TrainingLossCriteria(tol).</p> <p> TYPE: <code>Optional[Criterion]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>shuffle</code> <p>Shuffle data set.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>test_dataset</code> <p>Test data set.</p> <p> TYPE: <code>Optional[OperatorDataset]</code> DEFAULT: <code>None</code> </p> <code>lr_scheduler</code> <p>Learning rate scheduler. If True, <code>LinearLRScheduler</code> is used.</p> <p> TYPE: <code>Union[bool, Callback]</code> DEFAULT: <code>True</code> </p> Source code in <code>src/continuiti/trainer/trainer.py</code> <pre><code>def fit(\nself,\ndataset: OperatorDataset,\ntol: float = 1e-5,\nepochs: int = 1000,\ncallbacks: Optional[List[Callback]] = None,\ncriterion: Optional[Criterion] = None,\nbatch_size: int = 32,\nshuffle: bool = True,\ntest_dataset: Optional[OperatorDataset] = None,\nlr_scheduler: Union[bool, Callback] = True,\n):\n\"\"\"Fit operator to data set.\n    Args:\n        dataset: Data set.\n        tol: Tolerance for stopping criterion. Ignored if criterion is not None.\n        epochs: Maximum number of epochs.\n        callbacks: List of additional callbacks.\n        criterion: Stopping criterion. Defaults to TrainingLossCriteria(tol).\n        batch_size: Batch size.\n        shuffle: Shuffle data set.\n        test_dataset: Test data set.\n        lr_scheduler: Learning rate scheduler. If True, `LinearLRScheduler` is used.\n    \"\"\"\n# Callbacks\ncallbacks = callbacks or []\nif self.verbose:\nsteps = math.ceil(len(dataset) / batch_size)\ncallbacks.append(PrintTrainingLoss(epochs, steps))\nif lr_scheduler is not False:\nif lr_scheduler is True:\nlr_scheduler = LinearLRScheduler(self.optimizer, epochs)\ncallbacks.append(lr_scheduler)\n# Default criterion\nif criterion is None:\nif test_dataset is None:\ncriterion = TrainingLossCriterion(tol)\nelse:\ncriterion = TestLossCriterion(tol)\n# Print number of model parameters\nif self.verbose:\nif hasattr(self.operator, \"num_params\"):\nnum_params = self.operator.num_params()\nelse:\nnum_params = sum(p.numel() for p in self.operator.parameters())\nprint(f\"Parameters: {num_params}\", end=\"  \")\n# Move operator to device\noperator = self.operator.to(self.device)\n# Use DistributedDataParallel if available\nis_distributed = dist.is_available() and dist.is_initialized()\nsampler, test_sampler = None, None\nif is_distributed:\ntorch.cuda.set_device(self.device)\noperator = DDP(\noperator,\ndevice_ids=[self.device],\n)\nsampler = DistributedSampler(dataset, shuffle=shuffle)\nif test_dataset is not None:\ntest_sampler = DistributedSampler(test_dataset, shuffle=shuffle)\nshuffle = False\nassert (\nbatch_size % dist.get_world_size() == 0\n), \"Batch size must be divisible by world size\"\nbatch_size = batch_size // dist.get_world_size()  # Per-GPU batch size\nnum_workers = dist.get_world_size()\nif self.verbose:\nngpu = dist.get_world_size()\nprint(f\"Device: CUDA ({ngpu} GPU{'' if ngpu == 1 else 's'})\")\nelse:\nnum_workers = 0\nif self.verbose:\nprint(f\"Device: {self.device}\")\n# Create data loader\ndata_loader = DataLoader(\ndataset,\nbatch_size=batch_size,\nshuffle=shuffle,\nsampler=sampler,\nnum_workers=num_workers,\n)\nif test_dataset is not None:\ntest_data_loader = DataLoader(\ntest_dataset,\nbatch_size=batch_size,\nshuffle=shuffle,\nsampler=test_sampler,\nnum_workers=num_workers,\n)\n# Call on_train_begin\nfor callback in callbacks:\ncallback.on_train_begin()\n# Train\nloss_train, loss_test, epoch = None, None, 0\nfor epoch in range(epochs):\nloss_train = 0\nif is_distributed:\nsampler.set_epoch(epoch)\n# Callbacks\nlogs = Logs(\nepoch=epoch + 1,\nstep=0,\nloss_train=loss_train,\nloss_test=loss_test,\n)\noperator.train()\nfor xuyv in data_loader:\nxuyv = [t.to(self.device) for t in xuyv]\ndef closure(xuyv=xuyv):\nself.optimizer.zero_grad()\nloss = self.loss_fn(operator, *xuyv)\nloss.backward(retain_graph=True)\nreturn loss\nloss = self.optimizer.step(closure)\n# Compute mean loss\nloss_train += loss.detach().item()\n# Callbacks\nlogs.step += 1\nlogs.loss_train = loss_train / logs.step\nfor callback in callbacks:\ncallback.step(logs)\n# Compute test loss\nif test_dataset is not None:\noperator.eval()\nloss_test = 0\nfor xuyv in test_data_loader:\nxuyv = [t.to(self.device) for t in xuyv]\nloss = self.loss_fn(operator, *xuyv)\nif is_distributed:\ndist.all_reduce(loss)\nloss /= dist.get_world_size()\nloss_test += loss.detach().item()\nloss_test /= len(test_data_loader)\nlogs.loss_test = loss_test\n# Callbacks\nfor callback in callbacks:\ncallback(logs)\n# Stopping criterion\nif criterion is not None:\nif criterion(logs):\nif self.verbose:\nprint(\"- stopping criterion met\")\nbreak\n# Call on_train_end\nfor callback in callbacks:\ncallback.on_train_end()\n# Move operator back to CPU\nself.operator.to(\"cpu\")\nreturn logs\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/","title":"Callbacks","text":"<p><code>continuiti.trainer.callbacks</code></p> <p>Callbacks for Trainer in continuiti.</p>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.Callback","title":"<code>Callback</code>","text":"<p>Callback base class for <code>fit</code> method of <code>Trainer</code>.</p>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.Callback.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __call__(self, logs: Logs):\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.Callback.step","title":"<code>step(logs)</code>","text":"<p>Called after every gradient step.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def step(self, logs: Logs):\n\"\"\"Called after every gradient step.\n    Args:\n        logs: Training logs.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.Callback.on_train_begin","title":"<code>on_train_begin()</code>","text":"<p>Called at the beginning of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_begin(self):\n\"\"\"Called at the beginning of training.\"\"\"\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.Callback.on_train_end","title":"<code>on_train_end()</code>","text":"<p>Called at the end of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_end(self):\n\"\"\"Called at the end of training.\"\"\"\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.PrintTrainingLoss","title":"<code>PrintTrainingLoss(epochs=None, steps=None)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to print training/test loss.</p> PARAMETER  DESCRIPTION <code>epochs</code> <p>Number of epochs. Default is None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>steps</code> <p>Number of steps per epoch. Default is None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __init__(self, epochs: Optional[int] = None, steps: Optional[int] = None):\nself.epochs = epochs\nself.steps = steps\nself.steps_performed = 0\nself.start_time = time()\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.PrintTrainingLoss.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __call__(self, logs: Logs):\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    \"\"\"\nelapsed = time() - self.start_time\nsec_per_step = elapsed / self.steps_performed\ns = \"\"\ns += f\"Epoch {logs.epoch:2.0f}\"\nif self.epochs is not None:\ns += f\"/{self.epochs}\"\ns += \"  \"\ns += f\"Step {logs.step:2.0f}\"\nif self.steps is not None:\ns += f\"/{self.steps}\"\n# Progress bar\nn = 20\ndone = math.ceil(logs.step / self.steps * n)\ns += \"  [\" + \"=\" * done + \" \" * (n - done) + \"]\"\ns += \"  \"\nif sec_per_step &lt; 1:\ns += f\"{sec_per_step * 1000:.0f}ms/step\"\nelse:\ns += f\"{sec_per_step:.2f}s/step\"\ndef to_min(t):\nmin = t // 60\nsec = math.floor(t) % 60\nreturn f\"{min:.0f}:{sec:02.0f}min\"\ns += f\"  [{to_min(elapsed)}\"\nif self.epochs is not None and self.steps is not None:\nremaining_steps = (self.epochs - logs.epoch) * self.steps\nremaining_steps += self.steps - logs.step\neta = remaining_steps * sec_per_step\ns += f\"&lt;{to_min(eta)}\"\ns += \"] - \"\ns += f\"loss/train = {logs.loss_train:.4e}  \"\nif logs.loss_test is not None:\ns += f\"loss/test = {logs.loss_test:.4e} \"\nprint(\"\\r\" + s, end=\"\")\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.PrintTrainingLoss.step","title":"<code>step(logs)</code>","text":"<p>Called after every gradient step.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def step(self, logs: Logs):\n\"\"\"Called after every gradient step.\n    Args:\n        logs: Training logs.\n    \"\"\"\nself.steps_performed += 1\nself.__call__(logs)\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.PrintTrainingLoss.on_train_begin","title":"<code>on_train_begin()</code>","text":"<p>Called at the beginning of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_begin(self):\n\"\"\"Called at the beginning of training.\"\"\"\nself.start_time = time()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.PrintTrainingLoss.on_train_end","title":"<code>on_train_end()</code>","text":"<p>Called at the end of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_end(self):\n\"\"\"Called at the end of training.\"\"\"\nprint(\"\")\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.LearningCurve","title":"<code>LearningCurve(keys=None)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to plot learning curve.</p> PARAMETER  DESCRIPTION <code>keys</code> <p>List of keys to plot. Default is [\"loss_train\"].</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __init__(self, keys: Optional[List[str]] = None):\nif keys is None:\nkeys = [\"loss_train\", \"loss_test\"]\nself.keys = keys\nself.on_train_begin()\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.LearningCurve.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __call__(self, logs: Logs):\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    \"\"\"\nfor key in self.keys:\ntry:\ntest = logs.__getattribute__(key)\nself.losses[key].append(test)\nexcept AttributeError:\npass\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.LearningCurve.on_train_begin","title":"<code>on_train_begin()</code>","text":"<p>Called at the beginning of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_begin(self):\n\"\"\"Called at the beginning of training.\"\"\"\nself.losses = {key: [] for key in self.keys}\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.LearningCurve.on_train_end","title":"<code>on_train_end()</code>","text":"<p>Called at the end of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_end(self):\n\"\"\"Called at the end of training.\"\"\"\nfor key in self.keys:\nvals = self.losses[key]\nepochs = list(range(1, len(vals) + 1))\nplt.plot(epochs, vals)\nplt.yscale(\"log\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend(self.keys)\nplt.show()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.OptunaCallback","title":"<code>OptunaCallback(trial)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to report intermediate values to Optuna.</p> PARAMETER  DESCRIPTION <code>trial</code> <p>Optuna trial.</p> <p> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __init__(self, trial):\nself.trial = trial\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.OptunaCallback.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __call__(self, logs: Logs):\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    \"\"\"\nself.trial.report(logs.loss_train, step=logs.epoch)\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.MLFlowLogger","title":"<code>MLFlowLogger(operator=None)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to log metrics to MLFlow.</p> PARAMETER  DESCRIPTION <code>operator</code> <p>Operator to save snapshots. Default is None.</p> <p> TYPE: <code>Optional[Operator]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __init__(self, operator: Optional[Operator] = None):\nself.operator = operator\nself.best_loss = math.inf\nsuper().__init__()\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.MLFlowLogger.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def __call__(self, logs: Logs):\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    \"\"\"\nmlflow.log_metric(\"loss/train\", logs.loss_train, step=logs.epoch)\nloss = logs.loss_train\nif logs.loss_test is not None:\nmlflow.log_metric(\"loss/test\", logs.loss_test, step=logs.epoch)\nloss = logs.loss_test\n# Save best model\nself.best_loss = min(self.best_loss, loss)\nif self.best_loss == loss:\nself._save_model(\"best\")\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.MLFlowLogger.on_train_begin","title":"<code>on_train_begin()</code>","text":"<p>Called at the beginning of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_begin(self):\n\"\"\"Called at the beginning of training.\"\"\"\nif not mlflow.active_run():\nmlflow.start_run()\nself._save_model(\"initial\")\n</code></pre>"},{"location":"api/continuiti/trainer/callbacks/#continuiti.trainer.callbacks.MLFlowLogger.on_train_end","title":"<code>on_train_end()</code>","text":"<p>Called at the end of training.</p> Source code in <code>src/continuiti/trainer/callbacks.py</code> <pre><code>def on_train_end(self):\n\"\"\"Called at the end of training.\"\"\"\nself._save_model(\"final\")\nmlflow.end_run()\n</code></pre>"},{"location":"api/continuiti/trainer/criterion/","title":"Criterion","text":"<p><code>continuiti.trainer.criterion</code></p> <p>Stopping criterion for Trainer in continuiti.</p>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.Criterion","title":"<code>Criterion</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Stopping criterion base class for <code>fit</code> method of <code>Trainer</code>.</p>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.Criterion.__call__","title":"<code>__call__(logs)</code>  <code>abstractmethod</code>","text":"<p>Evaluate stopping criterion. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether to stop training.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/continuiti/trainer/criterion.py</code> <pre><code>@abstractmethod\ndef __call__(self, logs: Logs) -&gt; bool:\n\"\"\"Evaluate stopping criterion.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    Returns:\n        bool: Whether to stop training.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.TrainingLossCriterion","title":"<code>TrainingLossCriterion(threshold)</code>","text":"<p>             Bases: <code>Criterion</code></p> <p>Stopping criterion based on training loss.</p> Source code in <code>src/continuiti/trainer/criterion.py</code> <pre><code>def __init__(self, threshold: float):\nself.threshold = threshold\n</code></pre>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.TrainingLossCriterion.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if training loss is below threshold.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/continuiti/trainer/criterion.py</code> <pre><code>def __call__(self, logs: Logs) -&gt; bool:\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    Returns:\n        bool: True if training loss is below threshold.\n    \"\"\"\nreturn logs.loss_train &lt; self.threshold\n</code></pre>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.TestLossCriterion","title":"<code>TestLossCriterion(threshold)</code>","text":"<p>             Bases: <code>Criterion</code></p> <p>Stopping criterion based on training and test loss.</p> Source code in <code>src/continuiti/trainer/criterion.py</code> <pre><code>def __init__(self, threshold: float):\nself.threshold = threshold\n</code></pre>"},{"location":"api/continuiti/trainer/criterion/#continuiti.trainer.criterion.TestLossCriterion.__call__","title":"<code>__call__(logs)</code>","text":"<p>Callback function. Called at the end of each epoch.</p> PARAMETER  DESCRIPTION <code>logs</code> <p>Training logs.</p> <p> TYPE: <code>Logs</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if training and test loss are below threshold.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/continuiti/trainer/criterion.py</code> <pre><code>def __call__(self, logs: Logs) -&gt; bool:\n\"\"\"Callback function.\n    Called at the end of each epoch.\n    Args:\n        logs: Training logs.\n    Returns:\n        bool: True if training and test loss are below threshold.\n    \"\"\"\nreturn logs.loss_train &lt; self.threshold and logs.loss_test &lt; self.threshold\n</code></pre>"},{"location":"api/continuiti/trainer/device/","title":"Device","text":"<p><code>continuiti.trainer.device</code></p> <p>Default torch device.</p>"},{"location":"api/continuiti/trainer/device/#continuiti.trainer.device.get_device","title":"<code>get_device()</code>","text":"<p>Get torch device.</p> <p>Defaults to <code>cuda</code> or <code>mps</code> if available, otherwise to <code>cpu</code>.</p> <p>Use the environment variable <code>USE_MPS_BACKEND</code> to disable the <code>mps</code> backend.</p> RETURNS DESCRIPTION <code>device</code> <p>Device.</p> Source code in <code>src/continuiti/trainer/device.py</code> <pre><code>def get_device() -&gt; torch.device:\n\"\"\"Get torch device.\n    Defaults to `cuda` or `mps` if available, otherwise to `cpu`.\n    Use the environment variable `USE_MPS_BACKEND` to disable the `mps` backend.\n    Returns:\n        Device.\n    \"\"\"\ndevice = torch.device(\"cpu\")\nuse_mps_backend = os.environ.get(\"USE_MPS_BACKEND\", \"True\").lower() in (\"true\", \"1\")\nif use_mps_backend and torch.backends.mps.is_available():\ndevice = torch.device(\"mps\")\nif torch.cuda.is_available():\nif \"RANK\" in os.environ:\nif not dist.is_initialized():\ndist.init_process_group(\"nccl\")\nrank = dist.get_rank()\ndevice = torch.device(f\"cuda:{rank}\")\nelse:\ndevice = torch.device(\"cuda\")\nreturn device\n</code></pre>"},{"location":"api/continuiti/trainer/logs/","title":"Logs","text":"<p><code>continuiti.trainer.logs</code></p>"},{"location":"api/continuiti/trainer/logs/#continuiti.trainer.logs.Logs","title":"<code>Logs(epoch, step, loss_train, loss_test)</code>  <code>dataclass</code>","text":"<p>Logs for callbacks and criteria within Trainer in continuiti.</p> ATTRIBUTE DESCRIPTION <code>epoch</code> <p>Current epoch.</p> <p> TYPE: <code>int</code> </p> <code>step</code> <p>Current step.</p> <p> TYPE: <code>int</code> </p> <code>loss_train</code> <p>Training loss.</p> <p> TYPE: <code>float</code> </p> <code>loss_test</code> <p>Test loss.</p> <p> TYPE: <code>float</code> </p>"},{"location":"api/continuiti/trainer/scheduler/","title":"Scheduler","text":"<p><code>continuiti.trainer.scheduler</code></p> <p>Learning rate scheduler for Trainer in continuiti.</p>"},{"location":"api/continuiti/trainer/scheduler/#continuiti.trainer.scheduler.LinearLRScheduler","title":"<code>LinearLRScheduler(optimizer, max_epochs)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback for a linear learning rate scheduler.</p> <pre><code>lr(epoch) = lr0 * (1 - epoch / max_epochs)\n</code></pre> <p>where <code>lr0</code> is the initial learning rate of the optimizer.</p> PARAMETER  DESCRIPTION <code>optimizer</code> <p>Optimizer. The learning rate of the first parameter group will be updated.</p> <p> TYPE: <code>Optimizer</code> </p> <code>max_epochs</code> <p>Maximum number of epochs.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/continuiti/trainer/scheduler.py</code> <pre><code>def __init__(self, optimizer: torch.optim.Optimizer, max_epochs: int):\nself.optimizer = optimizer\nself.max_epochs = max_epochs\nlr0 = self.optimizer.param_groups[0][\"lr\"]\nself.schedule = lambda epoch: lr0 * (1 - epoch / max_epochs)\n</code></pre>"},{"location":"api/continuiti/trainer/trainer/","title":"Trainer","text":"<p><code>continuiti.trainer.trainer</code></p>"},{"location":"api/continuiti/trainer/trainer/#continuiti.trainer.trainer.Trainer","title":"<code>Trainer(operator, optimizer=None, lr=0.001, loss_fn=None, device=device, verbose=None)</code>","text":"<p>Trainer implements a default training loop for operator learning.</p> Example <pre><code>from continuiti.trainer import Trainer\nfrom continuiti.operators.losses import MSELoss\n...\noptimizer = torch.optim.Adam(operator.parameters(), lr=1e-3)\nloss_fn = MSELoss()\ntrainer = Trainer(operator, optimizer, loss_fn, device=\"cuda:0\")\ntrainer.fit(dataset, tol=1e-3, epochs=1000)\n</code></pre> PARAMETER  DESCRIPTION <code>operator</code> <p>Operator to be trained.</p> <p> TYPE: <code>Operator</code> </p> <code>optimizer</code> <p>Torch-like optimizer. Default is Adam with learning rate <code>lr</code>.</p> <p> TYPE: <code>Optional[Optimizer]</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>Learning rate. Ignored if optimizer is not None. Default is 1e-3.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>loss_fn</code> <p>Loss function taking (op, x, u, y, v). Default is MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to train on. Default is CPU.</p> <p> TYPE: <code>device</code> DEFAULT: <code>device</code> </p> <code>verbose</code> <p>Print model parameters and use PrintTrainingLoss callback by default. Default is True.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuiti/trainer/trainer.py</code> <pre><code>def __init__(\nself,\noperator: Operator,\noptimizer: Optional[torch.optim.Optimizer] = None,\nlr: float = 1e-3,\nloss_fn: Optional[Loss] = None,\ndevice: torch.device = device,\nverbose: Optional[bool] = None,\n):\nself.operator = operator\nself.optimizer = (\noptimizer\nif optimizer is not None\nelse torch.optim.Adam(operator.parameters(), lr=lr)\n)\nself.loss_fn = loss_fn if loss_fn is not None else MSELoss()\nif isinstance(device, torch.device):\nself.device = device\nelse:\nself.device = torch.device(device)\n# Verbosity\nif self.device.index is not None:\nif verbose is False:\nself.verbose = False\nelse:\nself.verbose = self.device.index == 0\nelse:\nself.verbose = verbose or True\n</code></pre>"},{"location":"api/continuiti/trainer/trainer/#continuiti.trainer.trainer.Trainer.fit","title":"<code>fit(dataset, tol=1e-05, epochs=1000, callbacks=None, criterion=None, batch_size=32, shuffle=True, test_dataset=None, lr_scheduler=True)</code>","text":"<p>Fit operator to data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> TYPE: <code>OperatorDataset</code> </p> <code>tol</code> <p>Tolerance for stopping criterion. Ignored if criterion is not None.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-05</code> </p> <code>epochs</code> <p>Maximum number of epochs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>callbacks</code> <p>List of additional callbacks.</p> <p> TYPE: <code>Optional[List[Callback]]</code> DEFAULT: <code>None</code> </p> <code>criterion</code> <p>Stopping criterion. Defaults to TrainingLossCriteria(tol).</p> <p> TYPE: <code>Optional[Criterion]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>shuffle</code> <p>Shuffle data set.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>test_dataset</code> <p>Test data set.</p> <p> TYPE: <code>Optional[OperatorDataset]</code> DEFAULT: <code>None</code> </p> <code>lr_scheduler</code> <p>Learning rate scheduler. If True, <code>LinearLRScheduler</code> is used.</p> <p> TYPE: <code>Union[bool, Callback]</code> DEFAULT: <code>True</code> </p> Source code in <code>src/continuiti/trainer/trainer.py</code> <pre><code>def fit(\nself,\ndataset: OperatorDataset,\ntol: float = 1e-5,\nepochs: int = 1000,\ncallbacks: Optional[List[Callback]] = None,\ncriterion: Optional[Criterion] = None,\nbatch_size: int = 32,\nshuffle: bool = True,\ntest_dataset: Optional[OperatorDataset] = None,\nlr_scheduler: Union[bool, Callback] = True,\n):\n\"\"\"Fit operator to data set.\n    Args:\n        dataset: Data set.\n        tol: Tolerance for stopping criterion. Ignored if criterion is not None.\n        epochs: Maximum number of epochs.\n        callbacks: List of additional callbacks.\n        criterion: Stopping criterion. Defaults to TrainingLossCriteria(tol).\n        batch_size: Batch size.\n        shuffle: Shuffle data set.\n        test_dataset: Test data set.\n        lr_scheduler: Learning rate scheduler. If True, `LinearLRScheduler` is used.\n    \"\"\"\n# Callbacks\ncallbacks = callbacks or []\nif self.verbose:\nsteps = math.ceil(len(dataset) / batch_size)\ncallbacks.append(PrintTrainingLoss(epochs, steps))\nif lr_scheduler is not False:\nif lr_scheduler is True:\nlr_scheduler = LinearLRScheduler(self.optimizer, epochs)\ncallbacks.append(lr_scheduler)\n# Default criterion\nif criterion is None:\nif test_dataset is None:\ncriterion = TrainingLossCriterion(tol)\nelse:\ncriterion = TestLossCriterion(tol)\n# Print number of model parameters\nif self.verbose:\nif hasattr(self.operator, \"num_params\"):\nnum_params = self.operator.num_params()\nelse:\nnum_params = sum(p.numel() for p in self.operator.parameters())\nprint(f\"Parameters: {num_params}\", end=\"  \")\n# Move operator to device\noperator = self.operator.to(self.device)\n# Use DistributedDataParallel if available\nis_distributed = dist.is_available() and dist.is_initialized()\nsampler, test_sampler = None, None\nif is_distributed:\ntorch.cuda.set_device(self.device)\noperator = DDP(\noperator,\ndevice_ids=[self.device],\n)\nsampler = DistributedSampler(dataset, shuffle=shuffle)\nif test_dataset is not None:\ntest_sampler = DistributedSampler(test_dataset, shuffle=shuffle)\nshuffle = False\nassert (\nbatch_size % dist.get_world_size() == 0\n), \"Batch size must be divisible by world size\"\nbatch_size = batch_size // dist.get_world_size()  # Per-GPU batch size\nnum_workers = dist.get_world_size()\nif self.verbose:\nngpu = dist.get_world_size()\nprint(f\"Device: CUDA ({ngpu} GPU{'' if ngpu == 1 else 's'})\")\nelse:\nnum_workers = 0\nif self.verbose:\nprint(f\"Device: {self.device}\")\n# Create data loader\ndata_loader = DataLoader(\ndataset,\nbatch_size=batch_size,\nshuffle=shuffle,\nsampler=sampler,\nnum_workers=num_workers,\n)\nif test_dataset is not None:\ntest_data_loader = DataLoader(\ntest_dataset,\nbatch_size=batch_size,\nshuffle=shuffle,\nsampler=test_sampler,\nnum_workers=num_workers,\n)\n# Call on_train_begin\nfor callback in callbacks:\ncallback.on_train_begin()\n# Train\nloss_train, loss_test, epoch = None, None, 0\nfor epoch in range(epochs):\nloss_train = 0\nif is_distributed:\nsampler.set_epoch(epoch)\n# Callbacks\nlogs = Logs(\nepoch=epoch + 1,\nstep=0,\nloss_train=loss_train,\nloss_test=loss_test,\n)\noperator.train()\nfor xuyv in data_loader:\nxuyv = [t.to(self.device) for t in xuyv]\ndef closure(xuyv=xuyv):\nself.optimizer.zero_grad()\nloss = self.loss_fn(operator, *xuyv)\nloss.backward(retain_graph=True)\nreturn loss\nloss = self.optimizer.step(closure)\n# Compute mean loss\nloss_train += loss.detach().item()\n# Callbacks\nlogs.step += 1\nlogs.loss_train = loss_train / logs.step\nfor callback in callbacks:\ncallback.step(logs)\n# Compute test loss\nif test_dataset is not None:\noperator.eval()\nloss_test = 0\nfor xuyv in test_data_loader:\nxuyv = [t.to(self.device) for t in xuyv]\nloss = self.loss_fn(operator, *xuyv)\nif is_distributed:\ndist.all_reduce(loss)\nloss /= dist.get_world_size()\nloss_test += loss.detach().item()\nloss_test /= len(test_data_loader)\nlogs.loss_test = loss_test\n# Callbacks\nfor callback in callbacks:\ncallback(logs)\n# Stopping criterion\nif criterion is not None:\nif criterion(logs):\nif self.verbose:\nprint(\"- stopping criterion met\")\nbreak\n# Call on_train_end\nfor callback in callbacks:\ncallback.on_train_end()\n# Move operator back to CPU\nself.operator.to(\"cpu\")\nreturn logs\n</code></pre>"},{"location":"api/continuiti/transforms/","title":"Transforms","text":"<p><code>continuiti.transforms</code></p> <p>Data transformations in continuiti.</p>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Transform","title":"<code>Transform(*args, **kwargs)</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for transformations of tensors.</p> <p>Transformations are applied to tensors to improve model performance, enhance generalization, handle varied input sizes, facilitate specific features, reduce overfitting, improve computational efficiency or many other reasons. This class takes some tensor and transforms it into some other tensor.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>Arguments passed to nn.Module parent class.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Arbitrary keyword arguments passed to nn.Module parent class.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>def __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Transform.forward","title":"<code>forward(tensor)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Tensor that should be transformed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Transformed tensor.</p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>@abstractmethod\ndef forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies the transformation.\n    Args:\n        tensor: Tensor that should be transformed.\n    Returns:\n        Transformed tensor.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Transform.undo","title":"<code>undo(tensor)</code>","text":"<p>Applies the inverse of the transformation (if it exists).</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Transformed tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with the transformation undone.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If the inverse of the transformation is not implemented.</p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies the inverse of the transformation (if it exists).\n    Args:\n        tensor: Transformed tensor.\n    Returns:\n        Tensor with the transformation undone.\n    Raises:\n        NotImplementedError: If the inverse of the transformation is not implemented.\n    \"\"\"\nraise NotImplementedError(\n\"The undo method is not implemented for this transform.\"\n)\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Compose","title":"<code>Compose(transforms, *args, **kwargs)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Handles the chained sequential application of multiple transformations.</p> PARAMETER  DESCRIPTION <code>transforms</code> <p>transformations that should be applied in the order they are in the list.</p> <p> TYPE: <code>List[Transform]</code> </p> <code>*args</code> <p>Arguments of parent class.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Arbitrary keyword arguments of parent class.</p> <p> DEFAULT: <code>{}</code> </p> ATTRIBUTE DESCRIPTION <code>transforms</code> <p>Encapsulates multiple transformations into one.</p> <p> TYPE: <code>List</code> </p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def __init__(self, transforms: List[Transform], *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.transforms = transforms\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Compose.forward","title":"<code>forward(tensor)</code>","text":"<p>Applies multiple transformations to a tensor in sequential order.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Tensor to be transformed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with all transformations applied.</p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies multiple transformations to a tensor in sequential order.\n    Args:\n        tensor: Tensor to be transformed.\n    Returns:\n        Tensor with all transformations applied.\n    \"\"\"\nfor transform in self.transforms:\ntensor = transform(tensor)\nreturn tensor\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Compose.undo","title":"<code>undo(tensor)</code>","text":"<p>Undoes multiple transformations.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Transformed tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with undone transformations (if possible).</p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Undoes multiple transformations.\n    Args:\n        tensor: Transformed tensor.\n    Returns:\n        Tensor with undone transformations (if possible).\n    \"\"\"\nfor transform in reversed(self.transforms):\ntensor = transform.undo(tensor)\nreturn tensor\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Normalize","title":"<code>Normalize(mean, std)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Normalization transformation (Z-normalization).</p> <p>This transformation takes a mean \\(\\mu\\) and standard deviation \\(\\sigma\\) to scale tensors \\(x\\) according to</p> \\[\\operatorname{Normalize}(x) = \\frac{x - \\mu}{\\sigma + \\varepsilon} := z,\\] <p>where \\(\\varepsilon\\) is a small value to prevent division by zero.</p> ATTRIBUTE DESCRIPTION <code>epsilon</code> <p>small value to prevent division by zero (<code>torch.finfo.tiny</code>)</p> <p> </p> PARAMETER  DESCRIPTION <code>mean</code> <p>mean used to scale tensors</p> <p> TYPE: <code>Tensor</code> </p> <code>std</code> <p>standard deviation used to scale tensors</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def __init__(self, mean: torch.Tensor, std: torch.Tensor):\nsuper().__init__()\nself.mean = nn.Parameter(mean)\nself.std = nn.Parameter(std)\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Normalize.forward","title":"<code>forward(x)</code>","text":"<p>Apply normalization to the input tensor.</p> \\[z = \\frac{x - \\mu}{\\sigma + \\varepsilon}\\] PARAMETER  DESCRIPTION <code>x</code> <p>input tensor \\(x\\)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>normalized tensor \\(z\\)</p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Apply normalization to the input tensor.\n    $$z = \\frac{x - \\mu}{\\sigma + \\varepsilon}$$\n    Args:\n        x: input tensor $x$\n    Returns:\n        normalized tensor $z$\n    \"\"\"\nreturn (x - self.mean) / (self.std + self.epsilon)\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.Normalize.undo","title":"<code>undo(z)</code>","text":"<p>Undo the normalization.</p> \\[x = z~(\\sigma + \\varepsilon) + \\mu\\] PARAMETER  DESCRIPTION <code>z</code> <p>(normalized) tensor \\(z\\)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>un-normalized tensor \\(x\\)</p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def undo(self, z: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Undo the normalization.\n    $$x = z~(\\sigma + \\varepsilon) + \\mu$$\n    Args:\n        z: (normalized) tensor $z$\n    Returns:\n        un-normalized tensor $x$\n    \"\"\"\nreturn z * (self.std + self.epsilon) + self.mean\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.QuantileScaler","title":"<code>QuantileScaler(src, n_quantile_intervals=1000, target_mean=0.0, target_std=1.0, eps=0.001)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Quantile Scaler Class.</p> <p>A transform for scaling input data to a specified target distribution using quantiles. This is particularly useful for normalizing data in a way that is more robust to outliers than standard z-score normalization.</p> <p>The transformation maps the quantiles of the input data to the quantiles of the target distribution, effectively performing a non-linear scaling that preserves the relative distribution of the data.</p> PARAMETER  DESCRIPTION <code>src</code> <p>tensor from which the source distribution is drawn.</p> <p> TYPE: <code>Tensor</code> </p> <code>n_quantile_intervals</code> <p>Number of individual bins into which the data is categorized.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>target_mean</code> <p>Mean of the target Gaussian distribution. Can be float (all dimensions use the same mean), or tensor (allows for different means along different dimensions).</p> <p> TYPE: <code>Union[float, Tensor]</code> DEFAULT: <code>0.0</code> </p> <code>target_std</code> <p>Std of the target Gaussian distribution. Can be float (all dimensions use the same std), or tensor (allows for different stds along different dimensions).</p> <p> TYPE: <code>Union[float, Tensor]</code> DEFAULT: <code>1.0</code> </p> <code>eps</code> <p>Small value to bound the target distribution to a finite interval.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def __init__(\nself,\nsrc: torch.Tensor,\nn_quantile_intervals: int = 1000,\ntarget_mean: Union[float, torch.Tensor] = 0.0,\ntarget_std: Union[float, torch.Tensor] = 1.0,\neps: float = 1e-3,\n):\nassert eps &lt;= 0.5\nassert eps &gt;= 0\nsuper().__init__()\nif isinstance(target_mean, float):\ntarget_mean = target_mean * torch.ones(1)\nif isinstance(target_std, float):\ntarget_std = target_std * torch.ones(1)\nself.target_mean = target_mean\nself.target_std = target_std\nassert n_quantile_intervals &gt; 0\nself.n_quantile_intervals = n_quantile_intervals\nself.n_q_points = n_quantile_intervals + 2  # n intervals have n + 2 edges\nself.n_dim = src.size(-1)\n# source \"distribution\"\nself.quantile_fractions = torch.linspace(0, 1, self.n_q_points)\nquantile_points = torch.quantile(\nsrc.view(-1, self.n_dim),\nself.quantile_fractions,\ndim=0,\ninterpolation=\"linear\",\n)\nself.quantile_points = nn.Parameter(quantile_points)\nself.deltas = nn.Parameter(quantile_points[1:] - quantile_points[:-1])\n# target distribution\nself.target_distribution = torch.distributions.normal.Normal(\ntarget_mean, target_std\n)\nself.target_quantile_fractions = torch.linspace(\n0 + eps, 1 - eps, self.n_q_points\n)  # bounded domain\ntarget_quantile_points = self.target_distribution.icdf(\nself.target_quantile_fractions\n)\ntarget_quantile_points = target_quantile_points.unsqueeze(1).repeat(\n1, self.n_dim\n)\nself.target_quantile_points = nn.Parameter(target_quantile_points)\nself.target_deltas = nn.Parameter(\ntarget_quantile_points[1:] - target_quantile_points[:-1]\n)\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.QuantileScaler.forward","title":"<code>forward(tensor)</code>","text":"<p>Transforms the input tensor to match the target distribution using quantile scaling.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>The input tensor to transform.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The transformed tensor, scaled to the target distribution.</p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Transforms the input tensor to match the target distribution using quantile scaling.\n    Args:\n        tensor: The input tensor to transform.\n    Returns:\n        The transformed tensor, scaled to the target distribution.\n    \"\"\"\nindices = self._get_scaling_indices(tensor, self.quantile_points)\n# Scale input tensor to the unit interval based on source quantiles\np_min = self.quantile_points[indices].view(tensor.shape)\ndelta = self.deltas[indices].view(tensor.shape)\nout = tensor - p_min\nout = out / delta\n# Scale and shift to match the target distribution\np_t_min = self.target_quantile_points[indices].view(tensor.shape)\ndelta_t = self.target_deltas[indices].view(tensor.shape)\nout = out * delta_t\nout = out + p_t_min\nreturn out\n</code></pre>"},{"location":"api/continuiti/transforms/#continuiti.transforms.QuantileScaler.undo","title":"<code>undo(tensor)</code>","text":"<p>Reverses the transformation applied by the forward method, mapping the tensor back to its original distribution.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>The tensor to reverse the transformation on.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The tensor with the quantile scaling transformation reversed according to the src distribution.</p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Reverses the transformation applied by the forward method, mapping the tensor back to its original\n    distribution.\n    Args:\n        tensor: The tensor to reverse the transformation on.\n    Returns:\n        The tensor with the quantile scaling transformation reversed according to the src distribution.\n    \"\"\"\nindices = self._get_scaling_indices(tensor, self.target_quantile_points)\n# Scale input tensor to the unit interval based on the target distribution\np_t_min = self.target_quantile_points[indices].view(tensor.shape)\ndelta_t = self.target_deltas[indices].view(tensor.shape)\nout = tensor - p_t_min\nout = out / delta_t\n# Scale and shift to match the src distribution\np_min = self.quantile_points[indices].view(tensor.shape)\ndelta = self.deltas[indices].view(tensor.shape)\nout = out * delta\nout = out + p_min\nreturn out\n</code></pre>"},{"location":"api/continuiti/transforms/compose/","title":"Compose","text":"<p><code>continuiti.transforms.compose</code></p> <p>Composes multiple transformations.</p>"},{"location":"api/continuiti/transforms/compose/#continuiti.transforms.compose.Compose","title":"<code>Compose(transforms, *args, **kwargs)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Handles the chained sequential application of multiple transformations.</p> PARAMETER  DESCRIPTION <code>transforms</code> <p>transformations that should be applied in the order they are in the list.</p> <p> TYPE: <code>List[Transform]</code> </p> <code>*args</code> <p>Arguments of parent class.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Arbitrary keyword arguments of parent class.</p> <p> DEFAULT: <code>{}</code> </p> ATTRIBUTE DESCRIPTION <code>transforms</code> <p>Encapsulates multiple transformations into one.</p> <p> TYPE: <code>List</code> </p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def __init__(self, transforms: List[Transform], *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.transforms = transforms\n</code></pre>"},{"location":"api/continuiti/transforms/compose/#continuiti.transforms.compose.Compose.forward","title":"<code>forward(tensor)</code>","text":"<p>Applies multiple transformations to a tensor in sequential order.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Tensor to be transformed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with all transformations applied.</p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies multiple transformations to a tensor in sequential order.\n    Args:\n        tensor: Tensor to be transformed.\n    Returns:\n        Tensor with all transformations applied.\n    \"\"\"\nfor transform in self.transforms:\ntensor = transform(tensor)\nreturn tensor\n</code></pre>"},{"location":"api/continuiti/transforms/compose/#continuiti.transforms.compose.Compose.undo","title":"<code>undo(tensor)</code>","text":"<p>Undoes multiple transformations.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Transformed tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with undone transformations (if possible).</p> Source code in <code>src/continuiti/transforms/compose.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Undoes multiple transformations.\n    Args:\n        tensor: Transformed tensor.\n    Returns:\n        Tensor with undone transformations (if possible).\n    \"\"\"\nfor transform in reversed(self.transforms):\ntensor = transform.undo(tensor)\nreturn tensor\n</code></pre>"},{"location":"api/continuiti/transforms/quantile_scaler/","title":"Quantile scaler","text":"<p><code>continuiti.transforms.quantile_scaler</code></p> <p>Quantile Scaler class.</p>"},{"location":"api/continuiti/transforms/quantile_scaler/#continuiti.transforms.quantile_scaler.QuantileScaler","title":"<code>QuantileScaler(src, n_quantile_intervals=1000, target_mean=0.0, target_std=1.0, eps=0.001)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Quantile Scaler Class.</p> <p>A transform for scaling input data to a specified target distribution using quantiles. This is particularly useful for normalizing data in a way that is more robust to outliers than standard z-score normalization.</p> <p>The transformation maps the quantiles of the input data to the quantiles of the target distribution, effectively performing a non-linear scaling that preserves the relative distribution of the data.</p> PARAMETER  DESCRIPTION <code>src</code> <p>tensor from which the source distribution is drawn.</p> <p> TYPE: <code>Tensor</code> </p> <code>n_quantile_intervals</code> <p>Number of individual bins into which the data is categorized.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>target_mean</code> <p>Mean of the target Gaussian distribution. Can be float (all dimensions use the same mean), or tensor (allows for different means along different dimensions).</p> <p> TYPE: <code>Union[float, Tensor]</code> DEFAULT: <code>0.0</code> </p> <code>target_std</code> <p>Std of the target Gaussian distribution. Can be float (all dimensions use the same std), or tensor (allows for different stds along different dimensions).</p> <p> TYPE: <code>Union[float, Tensor]</code> DEFAULT: <code>1.0</code> </p> <code>eps</code> <p>Small value to bound the target distribution to a finite interval.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def __init__(\nself,\nsrc: torch.Tensor,\nn_quantile_intervals: int = 1000,\ntarget_mean: Union[float, torch.Tensor] = 0.0,\ntarget_std: Union[float, torch.Tensor] = 1.0,\neps: float = 1e-3,\n):\nassert eps &lt;= 0.5\nassert eps &gt;= 0\nsuper().__init__()\nif isinstance(target_mean, float):\ntarget_mean = target_mean * torch.ones(1)\nif isinstance(target_std, float):\ntarget_std = target_std * torch.ones(1)\nself.target_mean = target_mean\nself.target_std = target_std\nassert n_quantile_intervals &gt; 0\nself.n_quantile_intervals = n_quantile_intervals\nself.n_q_points = n_quantile_intervals + 2  # n intervals have n + 2 edges\nself.n_dim = src.size(-1)\n# source \"distribution\"\nself.quantile_fractions = torch.linspace(0, 1, self.n_q_points)\nquantile_points = torch.quantile(\nsrc.view(-1, self.n_dim),\nself.quantile_fractions,\ndim=0,\ninterpolation=\"linear\",\n)\nself.quantile_points = nn.Parameter(quantile_points)\nself.deltas = nn.Parameter(quantile_points[1:] - quantile_points[:-1])\n# target distribution\nself.target_distribution = torch.distributions.normal.Normal(\ntarget_mean, target_std\n)\nself.target_quantile_fractions = torch.linspace(\n0 + eps, 1 - eps, self.n_q_points\n)  # bounded domain\ntarget_quantile_points = self.target_distribution.icdf(\nself.target_quantile_fractions\n)\ntarget_quantile_points = target_quantile_points.unsqueeze(1).repeat(\n1, self.n_dim\n)\nself.target_quantile_points = nn.Parameter(target_quantile_points)\nself.target_deltas = nn.Parameter(\ntarget_quantile_points[1:] - target_quantile_points[:-1]\n)\n</code></pre>"},{"location":"api/continuiti/transforms/quantile_scaler/#continuiti.transforms.quantile_scaler.QuantileScaler.forward","title":"<code>forward(tensor)</code>","text":"<p>Transforms the input tensor to match the target distribution using quantile scaling.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>The input tensor to transform.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The transformed tensor, scaled to the target distribution.</p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Transforms the input tensor to match the target distribution using quantile scaling.\n    Args:\n        tensor: The input tensor to transform.\n    Returns:\n        The transformed tensor, scaled to the target distribution.\n    \"\"\"\nindices = self._get_scaling_indices(tensor, self.quantile_points)\n# Scale input tensor to the unit interval based on source quantiles\np_min = self.quantile_points[indices].view(tensor.shape)\ndelta = self.deltas[indices].view(tensor.shape)\nout = tensor - p_min\nout = out / delta\n# Scale and shift to match the target distribution\np_t_min = self.target_quantile_points[indices].view(tensor.shape)\ndelta_t = self.target_deltas[indices].view(tensor.shape)\nout = out * delta_t\nout = out + p_t_min\nreturn out\n</code></pre>"},{"location":"api/continuiti/transforms/quantile_scaler/#continuiti.transforms.quantile_scaler.QuantileScaler.undo","title":"<code>undo(tensor)</code>","text":"<p>Reverses the transformation applied by the forward method, mapping the tensor back to its original distribution.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>The tensor to reverse the transformation on.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The tensor with the quantile scaling transformation reversed according to the src distribution.</p> Source code in <code>src/continuiti/transforms/quantile_scaler.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Reverses the transformation applied by the forward method, mapping the tensor back to its original\n    distribution.\n    Args:\n        tensor: The tensor to reverse the transformation on.\n    Returns:\n        The tensor with the quantile scaling transformation reversed according to the src distribution.\n    \"\"\"\nindices = self._get_scaling_indices(tensor, self.target_quantile_points)\n# Scale input tensor to the unit interval based on the target distribution\np_t_min = self.target_quantile_points[indices].view(tensor.shape)\ndelta_t = self.target_deltas[indices].view(tensor.shape)\nout = tensor - p_t_min\nout = out / delta_t\n# Scale and shift to match the src distribution\np_min = self.quantile_points[indices].view(tensor.shape)\ndelta = self.deltas[indices].view(tensor.shape)\nout = out * delta\nout = out + p_min\nreturn out\n</code></pre>"},{"location":"api/continuiti/transforms/scaling/","title":"Scaling","text":"<p><code>continuiti.transforms.scaling</code></p>"},{"location":"api/continuiti/transforms/scaling/#continuiti.transforms.scaling.Normalize","title":"<code>Normalize(mean, std)</code>","text":"<p>             Bases: <code>Transform</code></p> <p>Normalization transformation (Z-normalization).</p> <p>This transformation takes a mean \\(\\mu\\) and standard deviation \\(\\sigma\\) to scale tensors \\(x\\) according to</p> \\[\\operatorname{Normalize}(x) = \\frac{x - \\mu}{\\sigma + \\varepsilon} := z,\\] <p>where \\(\\varepsilon\\) is a small value to prevent division by zero.</p> ATTRIBUTE DESCRIPTION <code>epsilon</code> <p>small value to prevent division by zero (<code>torch.finfo.tiny</code>)</p> <p> </p> PARAMETER  DESCRIPTION <code>mean</code> <p>mean used to scale tensors</p> <p> TYPE: <code>Tensor</code> </p> <code>std</code> <p>standard deviation used to scale tensors</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def __init__(self, mean: torch.Tensor, std: torch.Tensor):\nsuper().__init__()\nself.mean = nn.Parameter(mean)\nself.std = nn.Parameter(std)\n</code></pre>"},{"location":"api/continuiti/transforms/scaling/#continuiti.transforms.scaling.Normalize.forward","title":"<code>forward(x)</code>","text":"<p>Apply normalization to the input tensor.</p> \\[z = \\frac{x - \\mu}{\\sigma + \\varepsilon}\\] PARAMETER  DESCRIPTION <code>x</code> <p>input tensor \\(x\\)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>normalized tensor \\(z\\)</p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Apply normalization to the input tensor.\n    $$z = \\frac{x - \\mu}{\\sigma + \\varepsilon}$$\n    Args:\n        x: input tensor $x$\n    Returns:\n        normalized tensor $z$\n    \"\"\"\nreturn (x - self.mean) / (self.std + self.epsilon)\n</code></pre>"},{"location":"api/continuiti/transforms/scaling/#continuiti.transforms.scaling.Normalize.undo","title":"<code>undo(z)</code>","text":"<p>Undo the normalization.</p> \\[x = z~(\\sigma + \\varepsilon) + \\mu\\] PARAMETER  DESCRIPTION <code>z</code> <p>(normalized) tensor \\(z\\)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>un-normalized tensor \\(x\\)</p> Source code in <code>src/continuiti/transforms/scaling.py</code> <pre><code>def undo(self, z: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Undo the normalization.\n    $$x = z~(\\sigma + \\varepsilon) + \\mu$$\n    Args:\n        z: (normalized) tensor $z$\n    Returns:\n        un-normalized tensor $x$\n    \"\"\"\nreturn z * (self.std + self.epsilon) + self.mean\n</code></pre>"},{"location":"api/continuiti/transforms/transform/","title":"Transform","text":"<p><code>continuiti.transforms.transform</code></p> <p>Transform base class.</p>"},{"location":"api/continuiti/transforms/transform/#continuiti.transforms.transform.Transform","title":"<code>Transform(*args, **kwargs)</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for transformations of tensors.</p> <p>Transformations are applied to tensors to improve model performance, enhance generalization, handle varied input sizes, facilitate specific features, reduce overfitting, improve computational efficiency or many other reasons. This class takes some tensor and transforms it into some other tensor.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>Arguments passed to nn.Module parent class.</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Arbitrary keyword arguments passed to nn.Module parent class.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>def __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/continuiti/transforms/transform/#continuiti.transforms.transform.Transform.forward","title":"<code>forward(tensor)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation.</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Tensor that should be transformed.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Transformed tensor.</p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>@abstractmethod\ndef forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies the transformation.\n    Args:\n        tensor: Tensor that should be transformed.\n    Returns:\n        Transformed tensor.\n    \"\"\"\n</code></pre>"},{"location":"api/continuiti/transforms/transform/#continuiti.transforms.transform.Transform.undo","title":"<code>undo(tensor)</code>","text":"<p>Applies the inverse of the transformation (if it exists).</p> PARAMETER  DESCRIPTION <code>tensor</code> <p>Transformed tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with the transformation undone.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If the inverse of the transformation is not implemented.</p> Source code in <code>src/continuiti/transforms/transform.py</code> <pre><code>def undo(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Applies the inverse of the transformation (if it exists).\n    Args:\n        tensor: Transformed tensor.\n    Returns:\n        Tensor with the transformation undone.\n    Raises:\n        NotImplementedError: If the inverse of the transformation is not implemented.\n    \"\"\"\nraise NotImplementedError(\n\"The undo method is not implemented for this transform.\"\n)\n</code></pre>"},{"location":"background/","title":"Background","text":"<p>In this section, we provide more background on operator learning and its implementation in continuiti.</p> <p>Architectures</p><p>Neural operator architectures in continuiti </p>"},{"location":"background/architectures/","title":"Architectures","text":"<p>continuiti implements the following neural operator architectures:</p> <ul> <li>DeepONet</li> <li>Fourier Neural Operator (FNO)</li> <li> <p>BelNet</p> </li> <li> <p>Deep Neural Operator (DNO)</p> </li> <li>Deep Cat Operator (DCO)</li> <li>Convolutional Neural Network (CNN)</li> <li>a generic NeuralOperator class</li> </ul> <p>and more to come...</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>This is an overview of some benchmark results to compare the performance of different operator architectures on various problems.</p> <p>The benchmarks are implemented in the <code>benchmarks</code> directory and we refer to this directory for detailed information on how the benchmarks are run.</p>"},{"location":"benchmarks/#navierstokes","title":"NavierStokes","text":"<p>Reference: Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" arXiv preprint arXiv:2010.08895 (2020) Table 1 (\\(\\nu\\) = 1e\u22125  T=20  N=1000)</p> <p>reported for FNO-3D: 0.1893 (rel. test error)</p> <p>FourierNeuralOperator</p> Depth Parameters Training Time (V100) rel. train error rel. test error 4 201M 91min 4.94e-03 0.1862 16 805M 181min 1.84e-04 0.1486  Visualization of best and worst training and test samples for FNO with depth 4 (as in paper).   Best training sample  rel. error = 2.3351e-03   Worst training sample  rel. error = 1.0090e-02   Best test sample  rel. error = 1.0219e-01   Worst test sample  rel. error = 4.4294e-01"},{"location":"how-to-guides/","title":"How-to Guides","text":"<p>Neural operators extend the concept of neural networks to function mappings, which enables discretization-invariant and mesh-free mappings of data with applications to physics-informed training, super-resolution, and more.</p> <p>This is a collection of notebooks that showcase some applications of continuiti and serve as a guide to solve specific problems.</p> <p>Time Series</p><p>Operator learning for non-uniform time series </p> <p>Super-resolution</p><p>Neural operators for super-resolution </p> <p>Physics-informed</p><p>Training physics-informed neural operators </p> <p>Meshes</p><p>Reading meshes for operator learning </p> <p>Self-supervised</p><p>Training self-supervised neural operators </p>"},{"location":"how-to-guides/meshes/","title":"Meshes","text":"<pre><code>import torch\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.tri import Triangulation\nfrom continuiti.data import OperatorDataset\nfrom continuiti.data.mesh import Gmsh\nfrom continuiti.operators import DeepONet\nfrom continuiti.trainer import Trainer\nfrom continuiti.pde import div, grad, PhysicsInformedLoss\n</code></pre> <p>We use the <code>Gmsh</code> class to read a mesh from a file generated by Gmsh.</p> <pre><code>meshes_dir = pathlib.Path.cwd().joinpath(\"..\", \"data\", \"meshes\")\ngmsh_file = meshes_dir.joinpath(\"mediterranean.msh\").as_posix()\nmesh = Gmsh(gmsh_file)\nvertices = mesh.get_vertices()\n# We use longitude and latitude only and scale them to be in the range [-1, 1]\nvertices = vertices[1:, :]\nvertices -= vertices.mean(dim=1, keepdim=True)\nvertices /= vertices[0, :].abs().max()\n</code></pre> <pre><code>num_observation = 1\nnum_sensors = 5\nx = torch.zeros(num_observation, 2, num_sensors)\nu = torch.zeros(num_observation, 1, num_sensors)\ny = torch.zeros(num_observation, 2, num_sensors)\nv = torch.zeros(num_observation, 1, num_sensors)\nfor i in range(num_observation):\n# Select random sensor positions\nidx = torch.randperm(vertices.shape[1])[:num_sensors]\nx[i] = vertices[:, idx]\n# Generate random sensor measurements\nu[i] = torch.rand(1, num_sensors)\n# The mapped function equals u the sensors\n# (but we will add a physics-informed loss later w.r.t to all vertices)\ny[i] = x[i]\nv[i] = u[i]\ndataset = OperatorDataset(x, u, y, v)\n</code></pre> <pre><code>operator = DeepONet(dataset.shapes)\n</code></pre> <pre><code>y_physics = vertices.clone().requires_grad_(True)[:, ::100] # Use a subset of vertices for the physics-informed loss\nmse = torch.nn.MSELoss()\ndef pde(x, u, y, v):\n# Sensor measurements\nloss = mse(v, u)\n# Physics-informed loss w.r.t. all vertices\nw = lambda y: operator(x, u, y)\nDelta = div(grad(w))\ny_all = y_physics.repeat(u.shape[0], 1, 1).to(y.device)\nloss += (Delta(y_all)**2).mean()\nreturn loss\nloss_fn = PhysicsInformedLoss(pde)\n</code></pre> <pre><code>Trainer(operator, loss_fn=loss_fn).fit(dataset, tol=1e-2, epochs=10000)\n</code></pre> <pre><code>x_test = x0.unsqueeze(0)\nu_test = u0.unsqueeze(0)\ntest_loss = mse(operator(x_test, u_test, x_test), u_test)\nprint(f\"loss/test = {test_loss.item():.4e}\")\n</code></pre> <pre>\n<code>loss/test = 8.9705e-03\n</code>\n</pre> <pre><code>tri = Triangulation(vertices[0], vertices[1], mesh.get_cells())\n</code></pre>"},{"location":"how-to-guides/meshes/#meshes","title":"Meshes","text":"<p>This example demonstrates the use of meshes in continuiti.</p> <p>In this example, we will load a <code>Gmsh</code> file and train a physics-informed neural operator.</p>"},{"location":"how-to-guides/meshes/#problem-statement","title":"Problem Statement","text":"<p>Let's assume we have a set of temperature measurements and want to  reconstruct a physically meaningful temperature distribution in the whole mediterranean sea. We define a data set by generating random sensor inputs and plot the first entry.</p>"},{"location":"how-to-guides/meshes/#neural-operator","title":"Neural Operator","text":"<p>In this example, we use a DeepONet architecture.</p>"},{"location":"how-to-guides/meshes/#physics-informed-loss","title":"Physics-informed loss","text":"<p>As we want to learn a physically meaningful completion, we define a physics-informed loss function satisfying:</p> \\[- \\Delta v = 0\\]"},{"location":"how-to-guides/meshes/#training","title":"Training","text":"<p>We train the neural operator using the physics-informed loss function. </p>"},{"location":"how-to-guides/meshes/#evaluation","title":"Evaluation","text":"<p>Let's evaluate the trained operator on our initial sensor measurements and compute a test error.</p>"},{"location":"how-to-guides/meshes/#plotting","title":"Plotting","text":"<p>We can plot the mapped function on the whole mesh by using the cell  information from the mesh file and pass it as <code>Triangulation</code> to matplotlib.</p>"},{"location":"how-to-guides/physicsinformed/","title":"Physics-informed","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuiti.operators import DeepONet\nfrom continuiti.pde import Grad, PhysicsInformedLoss\nfrom continuiti.data import OperatorDataset\nfrom continuiti.trainer import Trainer\n</code></pre> <pre><code># Input functions are polynomials to degree p\np = 2\npoly = lambda a, x: sum(a[i] * x ** i for i in range(p+1))\n# Generate random coefficients for N polynomials\nnum_functions = 4\na = torch.randn(num_functions, p+1)\n</code></pre> <pre><code>num_sensors = 32\nx = (2 * torch.rand(num_sensors, 1) - 1).reshape(1, num_sensors)\nu = torch.stack([poly(a[i], x) for i in range(num_functions)])\n# Evaluation positions (that require grad for physics-informed loss)\ny = x.clone()\ny.requires_grad = True\n# Reshape inputs for OperatorDataset\nx = x.repeat(num_functions, 1, 1)\nu = u.reshape(num_functions, 1, num_sensors)\ny = y.repeat(num_functions, 1, 1)\nv = torch.zeros(num_functions, 1, num_sensors) # labels won't be used\ndataset = OperatorDataset(x, u, y, v)\n</code></pre> <pre><code>operator = DeepONet(shapes=dataset.shapes)\n</code></pre> <pre><code>mse = torch.nn.MSELoss()\ndef pde(_, u, y, v):\nv_y = Grad()(y, v)\nreturn mse(v_y, u)\n</code></pre> <pre><code>loss_fn = PhysicsInformedLoss(pde)\nTrainer(operator, loss_fn=loss_fn).fit(dataset, tol=1e-4)\n</code></pre> <p>As you can see, the physics-informed neural operator is able to approximate the integration operator quite well (for our training samples).</p>"},{"location":"how-to-guides/physicsinformed/#physics-informed","title":"Physics-informed","text":"<p>Outputs of operators are functions, so we are able to compute derivatives with respect to evaluation coordinates (as in PINNs). Therefore, it is possible to impose physical laws within the loss function that is used to train an operator!</p> <p>This example demonstrates the training of a physics-informed neural operator.</p>"},{"location":"how-to-guides/physicsinformed/#setup","title":"Setup","text":""},{"location":"how-to-guides/physicsinformed/#problem-statement","title":"Problem Statement","text":"<p>Let's assume we want to learn the solution operator \\(G: u \\mapsto v\\) of the differential equation</p> \\[ \\frac{\\partial v}{\\partial x} = u. \\] <p>In this example, the solution operator \\(G\\) is the integration operator</p> \\[ G(u) = \\int u~dx \\ + c, \\] <p>where we choose \\(c\\) such that \\(\\int G(u) = 0\\). We can learn this operator by only prescribing the differential equation in the loss function (aka physics-informed)!</p> <p>Let's choose a set of polynomials with random coefficients as input functions and visualize them.</p>"},{"location":"how-to-guides/physicsinformed/#dataset","title":"Dataset","text":"<p>An operator dataset contains the input functions (evaluated at a random set of sensor positions) and a set of evaluation coordinates, but we do not need to specify any labels as we will use a physics-informed loss function instead.</p>"},{"location":"how-to-guides/physicsinformed/#neural-operator","title":"Neural Operator","text":"<p>In this example, we use a DeepONet architecture with the shapes inferred from the dataset.</p>"},{"location":"how-to-guides/physicsinformed/#physics-informed-loss","title":"Physics-informed loss","text":"<p>Now, we define a physics-informed loss function to train the operator. We specify the (partial) differential equation as a function <code>pde</code> that maps the operator input (<code>x</code>, <code>u</code>, <code>y</code>) and its prediction (<code>v</code>) to the PDE loss. The <code>Grad()</code> function uses autograd under the hood and computes the gradient of the prediction <code>v</code> with respect to the evaluation coordinates <code>y</code>.</p>"},{"location":"how-to-guides/physicsinformed/#training","title":"Training","text":"<p>Putting the PDE loss into a <code>PhysicsInformedLoss</code> object, we can pass it to the trainer and train the neural operator.</p>"},{"location":"how-to-guides/physicsinformed/#evaluation","title":"Evaluation","text":"<p>The trained operator can be evaluated at arbitrary positions, so let's plot the predictions (at a finer resolution) along with the (exact) target function!</p>"},{"location":"how-to-guides/selfsupervised/","title":"Self-supervised","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuiti.benchmarks.sine import SineBenchmark\nfrom continuiti.data.selfsupervised import SelfSupervisedOperatorDataset\nfrom continuiti.operators.integralkernel import NaiveIntegralKernel, NeuralNetworkKernel\nfrom continuiti.trainer import Trainer\n</code></pre> <pre><code>benchmark = SineBenchmark(n_sensors=32, n_train=4, n_evaluations=3)\nsine = benchmark.train_dataset\ndataset = SelfSupervisedOperatorDataset(sine.x, sine.u)\n</code></pre> <p>This dataset contains 128 samples. Let's plot a random one!</p> <pre><code>kernel = NeuralNetworkKernel(dataset.shapes, kernel_width=128, kernel_depth=8)\noperator = NaiveIntegralKernel(kernel)\n</code></pre> <pre><code>Trainer(operator).fit(dataset, tol=1e-3, batch_size=128)\n</code></pre>"},{"location":"how-to-guides/selfsupervised/#self-supervised","title":"Self-supervised","text":"<p>This example shows how to train a neural operator on sine functions in a self-supervised manner.</p>"},{"location":"how-to-guides/selfsupervised/#setup","title":"Setup","text":""},{"location":"how-to-guides/selfsupervised/#dataset","title":"Dataset","text":"<p>Create a data set of sine waves: The <code>SineBenchmark</code> generates \\(N\\) sine waves $$ f(x) = \\sin(w_k x), \\quad w_k = 1 + \\frac{k}{N-1}, $$ $$ \\quad k = 0, \\dots, N-1. $$ We wrap the dataset by a <code>SelfSupervisedDataset</code> that exports samples for self-supervised training, namely $$ \\left(\\mathbf{x}, f(\\mathbf{x}), x_j, f(x_j)\\right), \\quad \\text{for } j = 1, \\dots, M, $$ where \\(\\mathbf{x} = (x_i)_{i=1 \\dots M}\\) are the \\(M\\) equidistantly distributed sensor positions.</p>"},{"location":"how-to-guides/selfsupervised/#operator","title":"Operator","text":"<p>In this example, we use a <code>NaiveIntegralKernel</code> as neural operator with a <code>NeuralNetworkKernel</code> as kernel function.</p>"},{"location":"how-to-guides/selfsupervised/#training","title":"Training","text":"<p>Train the neural operator.</p>"},{"location":"how-to-guides/selfsupervised/#plotting","title":"Plotting","text":"<p>Plot model predictions for training data.</p>"},{"location":"how-to-guides/selfsupervised/#generalization","title":"Generalization","text":"<p>Plot prediction on a test sample which was not part of the training set.</p>"},{"location":"how-to-guides/superresolution/","title":"Super-resolution","text":"<pre><code>import torch\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom continuiti.benchmarks.flame import FlameDataset\nfrom continuiti.operators import DeepONet\nfrom continuiti.trainer import Trainer\nfrom continuiti.trainer.callbacks import LearningCurve\n</code></pre> <pre><code>N = 4\nflame_dir = pathlib.Path.cwd().joinpath(\"..\", \"data\", \"flame\")\nflame = FlameDataset(flame_dir=flame_dir, size=N, split=\"val\", channels=[\"ux\"])\n</code></pre> <pre><code>operator = DeepONet(shapes=flame.shapes, trunk_width=128, trunk_depth=64)\n</code></pre> <pre><code>trainer = Trainer(operator, lr=1e-4)\ntrainer.fit(flame, epochs=3000, callbacks=[LearningCurve()])\n</code></pre> <pre>\n<code>Parameters: 1052272  Device: mps\nEpoch 3000/3000  Step 1/1  [====================]  362ms/step  ETA 0:00min - loss/train = 5.1608e-03   </code>\n</pre> <pre>\n<code>\n</code>\n</pre>"},{"location":"how-to-guides/superresolution/#super-resolution","title":"Super-resolution","text":"<p>In this example, we expand upon the basics of operator learning and demonstrate the use of neural operators for super-resolution.</p> <p>We will employ the FLAME dataset, a set of flow samples of resolution 32x32 that should be up-sampled to 128x128. You can download the data set from Kaggle and put it into the <code>data/flame</code> directory to reproduce this example.</p>"},{"location":"how-to-guides/superresolution/#setup","title":"Setup","text":""},{"location":"how-to-guides/superresolution/#flame-dataset","title":"Flame Dataset","text":"<p>continuiti provides the <code>Flame</code> class (a special <code>OperatorDataset</code>) that reads and exports samples from the FLAME data. The data set contains train/val splits and has four channels <code>ux</code>, <code>uy</code>, <code>uz</code>, and <code>rho</code>. In this example, we only use channel <code>ux</code> from the first four samples of the val split, and we visualize the provided data using matplotlib.</p>"},{"location":"how-to-guides/superresolution/#operator","title":"Operator","text":"<p>We define a <code>DeepONet</code> to map the low-resolution data to a continuous function. Note that we increase the expressivity of the trunk network by increasing the width and depth.</p>"},{"location":"how-to-guides/superresolution/#training","title":"Training","text":"<p>With an <code>OperatorDataset</code> at hand, training is straightforward using the <code>Trainer.fit</code> method. Here, we add the <code>LearningCurve</code> callback to monitor the training loss.</p>"},{"location":"how-to-guides/superresolution/#evaluation","title":"Evaluation","text":"<p>As we can evaluate the trained operator at arbitrary positions, we can plot the mapped function on a fine mesh with 128 positions (or any other resolution), aka super-resolution!</p>"},{"location":"how-to-guides/timeseries/","title":"Time Series","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuiti.operators import BelNet\nfrom continuiti.data import OperatorDataset\nfrom continuiti.trainer import Trainer\n</code></pre> <pre><code>f = lambda t: torch.sin(2 * torch.pi * t)\ndef random_locations(num_sensors):\nreturn torch.sort(torch.rand(num_sensors))[0]\n# History will be given as input\nnum_sensors = 32\nt_hist = random_locations(num_sensors)\n# Future will be given as labels\nnum_labels = 16\nt_fut = 1 + 0.5 * random_locations(num_labels)\nf_hist = f(t_hist)\nf_fut = f(t_fut)\n</code></pre> <pre><code>n_samples = 32\nx_dim = u_dim = y_dim = v_dim = 1\nx = torch.zeros(n_samples, x_dim, num_sensors)\nu = torch.zeros(n_samples, u_dim, num_sensors)\ny = torch.zeros(n_samples, y_dim, num_labels)\nv = torch.zeros(n_samples, v_dim, num_labels)\nfor i in range(n_samples):\nt_hist = random_locations(num_sensors)\nt_fut = 1 + 0.5 * random_locations(num_labels)\nf_hist = f(t_hist)\nf_fut = f(t_fut)\nx[i, 0, :] = t_hist\nu[i, 0, :] = f_hist\ny[i, 0, :] = t_fut\nv[i, 0, :] = f_fut\ndataset = OperatorDataset(x, u, y, v)\n</code></pre> <pre><code>operator = BelNet(dataset.shapes, D_1=32, D_2=32)\n</code></pre> <pre><code>Trainer(operator).fit(dataset, tol=1e-4)\n</code></pre> <pre><code>t_plot = torch.linspace(1, 1.5, 100).reshape(1, 1, -1)\n# Some time steps used for training\nx, u, t_fut, f_fut = dataset[0:1]\nf_pred = operator(x, u, t_plot)    # x, u = t_hist, f_hist\n# Different time steps\nt_hist2 = random_locations(num_sensors)\nf_hist2 = f(t_hist2)\nx2 = t_hist2.reshape(1, 1, -1)\nu2 = f_hist2.reshape(1, 1, -1)\nf_pred2 = operator(x2, u2, t_plot) # x2, u2 = t_hist2, f_hist2\n</code></pre> <p>This example demonstrates how useful it can be to consider functional data as functions and apply machine learning to these functions directly. This is what operator learning is about!</p>"},{"location":"how-to-guides/timeseries/#time-series","title":"Time Series","text":"<p>This example demonstrates operator learning for a time series with non-uniform time steps. Actually, a discretization-invariant neural operator can learn from an arbitrary discretization and generalize to a different one!</p>"},{"location":"how-to-guides/timeseries/#setup","title":"Setup","text":""},{"location":"how-to-guides/timeseries/#problem","title":"Problem","text":"<p>As a simple demonstration of the concepts, we will consider sequences of \\(n\\) observations \\(f_i\\) with \\(i \\in \\{1, \\dots, n\\}\\). The index \\(i\\) corresponds to samples of a function \\(f\\) at times \\(t_i\\). For these observations, we would like to predict \\(m\\) future values \\(f_j\\) where \\(j \\in \\{n+1, \\dots, n+m\\}\\).</p> <p>In this example, we choose a simple sine function</p> \\[ f(t) = \\sin(2\\pi t), \\] <p>and choose \\(n=32\\) and \\(m = 16\\). Both \\(t_i\\) and \\(t_j\\) are sampled random uniformly from the intervals \\(t_i\\in [0, 1)\\) and \\(t_j\\in [1, 1.5)\\).</p> <p>The trained neural operator is supposed to predict the future of the time series by only having access to \\(n\\) evaluations of the historical function.</p>"},{"location":"how-to-guides/timeseries/#dataset","title":"Dataset","text":"<p>For training an operator, we first construct the corresponding <code>OperatorDataset</code>.</p>"},{"location":"how-to-guides/timeseries/#operator","title":"Operator","text":"<p>In this example, we use BelNet, a discretization-invariant neural operator that can interpolate between different input discretizations. It can also learn mappings of functions that are defined on different domains, this is what is referred to as domain-independence.</p>"},{"location":"how-to-guides/timeseries/#training","title":"Training","text":"<p>We train the operator on the given input discretization.</p>"},{"location":"how-to-guides/timeseries/#evaluation","title":"Evaluation","text":"<p>Let's plot the predictions of the trained BelNet on the interval \\([1, 1.5)\\). Note that the operator makes a good prediction even if we sample the sine wave in new random time steps!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the continuiti tutorials! These tutorials give an introduction to operator learning: from the basics of functions and discretization to the training of neural operators in continuiti.</p> <p>If you instead just want implement specific problems, directly jump to our How-to Guides.</p> <p>First Steps</p><p>First steps and installation</p> <p>Operators</p><p>What is a function operator?</p> <p>Functions</p><p>How to implement functions</p> <p>Training</p><p>Learn to train neural operators in continuiti</p> <p>FNO</p><p>Use Fourier Neural Operators</p>"},{"location":"tutorials/firststeps/","title":"First Steps","text":"<p>continuiti aims to implement recent advances of operator learning. It provides cutting-edge deep learning approaches to learn mappings of (continuous) functions.</p> <p>Operator learning is motivated by the fact that many problems in science and engineering are formulated in terms of functions. While conventional neural networks learn between vector-spaces, neural operators learn mappings between infinite-dimensional function spaces.</p> <p>In the following, we will kindly introduce you step-by-step to the concepts of operator learning. We start with installing continuiti and then move on to the basics of operator learning with simple examples.</p>"},{"location":"tutorials/firststeps/#installing-continuiti","title":"Installing continuiti","text":"<p>Install the latest version of continuiti using pip: <pre><code>pip install continuiti\n</code></pre></p> <p>continuiti requires Python&gt;=3.9 and is built on top of PyTorch.</p>"},{"location":"tutorials/fno/","title":"FNO","text":"<pre><code>import torch\nfrom continuiti.data import OperatorDataset\nfrom continuiti.discrete import RegularGridSampler\nfrom continuiti.operators import FourierNeuralOperator\nfrom continuiti.operators.fourierlayer import FourierLayer\nfrom continuiti.trainer import Trainer\n</code></pre> <pre><code># Input function\nu_func = lambda x: torch.exp(-x**2)*10\n# Target function\nv_func = lambda y: torch.exp(-y**2)*2*10 * (-y)\ndef get_equidistant(N):\nreturn torch.arange(-N/2, N/2) / N * torch.pi * 2\nnum_sensors = 31\nnum_evaluations = 31\nx = get_equidistant(num_sensors)\ny = get_equidistant(num_evaluations)\nu = u_func(x)\nv = v_func(y)\n</code></pre> <pre><code>n_observations = 1\nu_dim = x_dim = y_dim = v_dim = 1\nx = x.reshape(n_observations, x_dim, num_sensors)\nu = u.reshape(n_observations, u_dim, num_sensors)\ny = y.reshape(n_observations, y_dim, num_evaluations)\nv = v.reshape(n_observations, v_dim, num_evaluations)\ndataset = OperatorDataset(x, u, y, v)\n</code></pre> <pre><code>fourier = FourierLayer(dataset.shapes)\n</code></pre> <pre><code>trainer = Trainer(fourier)\ntrainer.fit(dataset, epochs=10_000)\n</code></pre> <p>We can evaluate the trained <code>FourierLayer</code> with any other resolution, e.g., to plot the output function an a finer resolution.</p> <pre><code>num_plot = 1000\ny_plot = get_equidistant(num_plot).reshape(1, 1, -1)\nv_pred = fourier(x, u, y_plot)\n</code></pre> <pre><code>x_fine = get_equidistant(128).reshape(1, 1, -1)\nu_fine = u_func(x_fine)\nv_pred = fourier(x_fine, u_fine, y_plot)\n</code></pre> <p>Per default, the Fourier layer considers as many modes as are suggested by the <code>shapes.u.num</code> parameter. However, it can be set to any other value and determines the number of weights in the layer.</p> <pre><code>vs = {}\nfor modes in [3, 4, 10, 100]:\nfourier = FourierLayer(dataset.shapes, num_modes=(modes,))\nTrainer(fourier).fit(dataset, epochs=10_000)\nv_pred = fourier(x, u, y_plot)\nvs[modes] = v_pred\n</code></pre> <pre><code>sampler = RegularGridSampler([-3., -3.], [3., 3.])\nN = 100\nx = y = sampler(N**2)\nu = multivariate_normal_2d(x)\nv = double_multivariate_normal_2d(y)\n</code></pre> <p>Note</p> <p>The definitions of <code>multivariate_normal_2d</code> and <code>double_multivariate_normal_2d</code> are hidden for readability, but both functions are visualized below.</p> <pre><code>x = x.reshape(1, 2, N, N)\nu = u.reshape(1, 1, N, N)\ny = y.reshape(1, 2, N, N)\nv = v.reshape(1, 1, N, N)\ndataset = OperatorDataset(x, u, y, v)\n</code></pre> <pre><code>model = FourierNeuralOperator(shapes=dataset.shapes, depth=3, width=3)\n</code></pre> <pre><code>Trainer(model).fit(dataset, epochs=10_000)\nv_pred = model(x, u, y)\n</code></pre>"},{"location":"tutorials/fno/#fourier-neural-operator-fno","title":"Fourier Neural Operator (FNO)","text":"<p>A very popular architecture in the field of neural operators is the Fourier Neural Operator (FNO). This notebook demonstrates the use of <code>FourierLayer</code> and <code>FourierNeuralOperator</code> in continuiti.</p> <p>The FNO architecture was proposed in Z. Li et al., 2020 and gained a lot of attention because it is computationally very efficient. However, due to the Fast Fourier Transform (FFT) being used, it is in general restricted to simple geometries like rectangles.</p>"},{"location":"tutorials/fno/#1d","title":"1D","text":"<p>Let's start with a simple one-dimensional function training a single <code>FourierLayer</code> on it.</p>"},{"location":"tutorials/fno/#dataset","title":"Dataset","text":""},{"location":"tutorials/fno/#fourier-layer","title":"Fourier Layer","text":"<p>We instantiate the Fourier layer with the shapes determined by the dataset.</p>"},{"location":"tutorials/fno/#training","title":"Training","text":""},{"location":"tutorials/fno/#modes","title":"Modes","text":"<p>In the same way as the <code>FourierLayer</code> allows you to change the number of evaluation points of \\(y\\) for each forward pass, the dimensionality of the input \\(u\\) can also be changed.</p> <p>In the case of \\(u\\) being larger than what was specified during initialization, the <code>FourierLayer</code> removes the high frequencies (or modes). In case \\(u\\) is smaller, zero-values large frequencies are added.</p>"},{"location":"tutorials/fno/#2d","title":"2D","text":"<p>The FourierLayer also works on multi-dimensional data and we demonstrate these capabilities along with the multi-layer <code>FourierNeuralOperator</code> (FNO) in the following.</p>"},{"location":"tutorials/fno/#dataset_1","title":"Dataset","text":""},{"location":"tutorials/fno/#fourier-neural-operator","title":"Fourier Neural Operator","text":"<p>Let's use an FNO! It contains <code>depth</code> many <code>FourierLayers</code> with a latent dimension of <code>width</code>.</p>"},{"location":"tutorials/fno/#training_1","title":"Training","text":""},{"location":"tutorials/functions/","title":"Functions","text":"<p>In continuiti, functions are handled by a <code>Function</code> object that takes a callable as input. As an example, we define two functions \\(f\\) and \\(g\\): </p> \\[ f(x) = \\sin(\\pi x), \\qquad g(x) = x^3, \\qquad x\\in \\mathbb{R}. \\] <pre><code>from continuiti.data.function import Function\nf = Function(lambda x: torch.sin(torch.pi * x))\ng = Function(lambda x: x ** 3)\n</code></pre> <p>Functions are evaluated by calling them.</p> <pre><code>x = torch.linspace(-1, 1, 100)\ny = f(x)\nz = g(x)\n</code></pre> <p>Functions can be added to other function instances and multiplied by scalars. For instance:</p> <pre><code>h_1 = f + g\nh_2 = (-1) * (f + g)\n</code></pre> <p>In practice, we consider subsets of function spaces (where in general, these properties are not respected) and,     therefore, this class implements a (parametrized) function set. Here, we define the following set of sine waves $$ y = \\beta \\sin(k\\pi x),\\quad A=[\\beta, k]^\\top\\in\\mathbb{R}^2,\\quad x, y\\in\\mathbb{R}. $$ In continuiti, function sets are handled by a <code>FunctionSet</code> that takes a nested callable as input. When <code>FunctionSet</code> is called with a set of parameters it returns a list of <code>Function</code> instances.</p> <pre><code>from continuiti.data.function import FunctionSet\nsine_set = FunctionSet(lambda a: Function(lambda x: a[0] * torch.sin(a[1] * torch.pi * x)))\nparameters = 5 * torch.rand(2, 4)  # 2 parameters in 4 sets\nsines = sine_set(parameters)\n</code></pre> <pre><code>from continuiti.discrete import RegularGridSampler, UniformBoxSampler\nsampler_a = RegularGridSampler([1., 1.], [1.5, 2.5])\nsampler_b = UniformBoxSampler([1.75, 1.25], [3., 1.75])\nn_samples = 102\nsamples_a = sampler_a(n_samples)\nsamples_b = sampler_b(n_samples)\nprint(f\"Shape of sample A: {samples_a.shape} (as 6 x 17 = 102), Shape of sample B: {samples_b.shape}\")\n</code></pre> <pre>\n<code>Shape of sample A: torch.Size([2, 6, 17]) (as 6 x 17 = 102), Shape of sample B: torch.Size([2, 102])\n</code>\n</pre> <p>Now, we can use these samplers to draw samples from our previously defined sine wave \\(y = \\beta \\sin(k\\pi x)\\). In a first step, we will draw from the parameter space. In a second step, we draw from the domain.</p> <pre><code>parameter_sampler = RegularGridSampler([1., 1.], [2., 2.])\ndomain_sampler = UniformBoxSampler([-1.], [1.])\n# sample parameter space\nn_parameter_samples = 9\nparameter_samples = parameter_sampler(n_parameter_samples).reshape(2, -1)\nsines = sine_set(parameter_samples)\n# sample domain\nn_sensors = 32\nx = []\ny = []\nfor sine in sines:\nxi = domain_sampler(n_sensors)\nx.append(xi)\nyi = sine(xi)\ny.append(yi)\nx = torch.stack(x)\ny = torch.stack(y)\n</code></pre> <pre><code>from continuiti.data.function import FunctionOperatorDataset\nu_set = FunctionSet(lambda a: Function(lambda xi: torch.sin(a * xi)))\nv_set = FunctionSet(lambda a: Function(lambda xi: a * torch.cos(a * xi)))\nx_sampler = RegularGridSampler([-1], [1.])\ny_sampler = UniformBoxSampler([-2], [.5])\nparameter_sampler = RegularGridSampler([torch.pi], [2 * torch.pi])\ndset = FunctionOperatorDataset(\ninput_function_set=u_set,\nx_sampler=x_sampler,\nn_sensors=64,\noutput_function_set=v_set,\ny_sampler=y_sampler,\nn_evaluations=128,\nparameter_sampler=parameter_sampler,\nn_observations=4,\n)\n</code></pre>"},{"location":"tutorials/functions/#functions","title":"Functions","text":"<p>This notebook gives an introduction into functions, function spaces (or function sets), and how these concepts are implemented in continuiti.</p>"},{"location":"tutorials/functions/#functions_1","title":"Functions","text":"<p>A function is a mapping between a set \\(X\\) (domain) and a set \\(Y\\) (co-domain), denoted by</p> \\[\\begin{align*} f: X &amp;\\rightarrow Y, \\\\ x &amp;\\mapsto f(x). \\end{align*}\\] <p>For example, the function \\(f(x)=x^2\\) maps each element \\(x\\in X = [-1,1] \\subset \\mathbb{R}\\) to an element of \\(Y = [0,1] \\subset \\mathbb{R}\\).</p>"},{"location":"tutorials/functions/#function-sets","title":"Function Sets","text":"<p>Let \\(X\\) be a domain supported on a field \\(K_1\\), and let \\(Y\\) be the codomain supported on a field \\(K_2\\). A function space is the set of functions \\(F(X, Y)\\) that map from \\(X\\) to \\(Y\\) and are closed with respect to addition  $$  (f+g)(x):X \\rightarrow Y, x\\mapsto f(x)+g(x),  $$  and scalar multiplication  $$  (c\\cdot f)(x) \\rightarrow Y, x\\mapsto c\\cdot f(x).  $$</p>"},{"location":"tutorials/functions/#sampler","title":"Sampler","text":"<p>In continuiti, there are samplers to create discrete representations of continuous functions. The following example shows how the <code>RegularGridSampler</code> and the <code>UniformBoxSampler</code> handle this. Both take two corner points of a \\(n\\)-dimensional rectangle as inputs. </p>"},{"location":"tutorials/functions/#function-operator-datasets","title":"Function Operator Datasets","text":"<p>In continuiti, the concepts of function sets and samplers are combined to form <code>FunctionOperatorDataset</code> instances. This class is used to handle scenarios where both the input function set \\(U\\) and the output function set \\(V\\) of an operator \\(G\\) are known. It utilizes samplers to generate samples to generate a training set.</p> <p>Note</p> <p>The <code>FunctionOperatorDataset</code> assumes that both function sets are defined on the same parameters.</p> <p>In this example, we show how to generate a derivative operator dataset $$ G:u\\mapsto \\partial_x u, $$ for the parameterized sine function $$ u_a: x\\mapsto \\sin(ax), \\quad a, x\\in \\mathbb{R}, $$ and its derivative $$ v_a: x\\mapsto a\\cos(ax), \\quad a,x \\in \\mathbb{R}. $$</p>"},{"location":"tutorials/operators/","title":"Operators","text":"<p>Function operators are ubiquitous in mathematics and physics: They are used to describe dynamics of physical systems, such as the Navier-Stokes equations in fluid dynamics. As solutions of these systems are functions, it is natural to transfer the concept of function mapping into machine learning.</p> <p>In mathematics, operators are function mappings: they map functions to functions. Let</p> \\[ U = \\{ u: X \\subset \\mathbb{R}^{d_x} \\to \\mathbb{R}^{d_u} \\} \\] <p>be a set of functions that map a \\(d_x\\)-dimensional input to an \\(d_u\\)-dimensional output, and</p> \\[ V = \\{ v: Y \\subset \\mathbb{R}^{d_y} \\to \\mathbb{R}^{d_v} \\} \\] <p>be a set of functions that map a \\(d_y\\)-dimensional input to a \\(d_v\\)-dimensional output.</p> <p>An operator</p> \\[\\begin{align*}   G: U &amp;\\to V, \\\\      u &amp;\\mapsto v, \\end{align*}\\] <p>maps functions \\(u \\in U\\) to functions \\(v \\in V\\).</p> <p>Example</p> <p>The operator \\(G(u) = \\partial_x u\\) maps functions \\(u\\) to their partial derivative \\(\\partial_x u\\).</p>"},{"location":"tutorials/operators/#learning-operators","title":"Learning Operators","text":"<p>Operator learning is the task of learning the mapping \\(G\\) from data. In the context of neural networks, we want to train a neural network \\(G_\\theta\\) with parameters \\(\\theta\\) that, given a set of input-output pairs \\((u_k, v_k) \\in U \\times V\\), maps \\(u_k\\) to \\(v_k\\). We generally refer to such a neural network \\(G_\\theta\\) as a neural operator.</p>"},{"location":"tutorials/operators/#discretization","title":"Discretization","text":"<p>In continuiti, we use the general approach of mapping function evaluations to represent both input and output functions \\(u\\) and \\(v\\) in a discretized form.</p> <p>Let \\(x_i \\in X\\) be <code>num_sensors</code> many sensor positions (or collocation points) in the input domain \\(X\\) of \\(u\\). We represent the function \\(u\\) by its evaluations at these sensors and write \\(\\mathbf{x} = (x_i)_i\\) and \\(\\mathbf{u} = (u(x_i))_i\\). This finite dimensional representation is fed into the neural operator.</p> <p>The mapped function \\(v = G(u)\\), on the other hand, is also represented by function evaluations only. Let \\(y_j \\in Y\\) be <code>num_evaluations</code> many evaluation points (or query points) in the input domain \\(Y\\) of \\(v\\) and \\(\\mathbf{y} = (y_j)_j\\). Then, the output values \\(\\mathbf{v} = (v(y_j))_j\\) are approximated by the neural operator $$ v(\\mathbf{y}) = G(u)(\\mathbf{y}) \\approx G_\\theta(\\mathbf{x}, \\mathbf{u}, \\mathbf{y}) = \\mathbf{v}. $$</p> <p>In Python, we will write the operator call as <pre><code>v = operator(x, u, y)\n</code></pre> with tensors of shapes</p> <pre><code>x: (batch_size, x_dim, num_sensors...)\nu: (batch_size, u_dim, num_sensors...)\ny: (batch_size, y_dim, num_evaluations...)\nv: (batch_size, v_dim, num_evaluations...)\n</code></pre> <p>This is to provide the most general case for implementing operators, as some neural operators differ in the way they handle input and output values.</p>"},{"location":"tutorials/operators/#wrapping","title":"Wrapping","text":"<p>For convenience, the call can be wrapped to mimic the mathematical syntax. For instance, for a fixed set of collocation points <code>x</code>, we could define <pre><code>G = lambda y: lambda u: operator(x, u(x), y)\nv = G(u)(y)\n</code></pre></p>"},{"location":"tutorials/training/","title":"Training","text":"<pre><code>import torch\nfrom continuiti.discrete import RegularGridSampler\nfrom continuiti.data.function import FunctionSet\nU = FunctionSet(lambda a: lambda x: torch.sin(a * torch.pi * x))\nV = FunctionSet(lambda a: lambda y: a * torch.pi * torch.cos(a * torch.pi * y))\na = torch.Tensor([[1., 1.5, 2.]])\nu_a = U(a)\nv_a = V(a)\nprint(f\"len(u) = {len(u_a)}  \", f\"len(v) = {len(v_a)}\")\n</code></pre> <pre>\n<code>len(u) = 3   len(v) = 3\n</code>\n</pre> <p>Note</p> <p>In these examples, we hide the code for visualization, but you can find it in the source code of this notebook.</p> <pre><code>from continuiti.data.function import FunctionOperatorDataset\na_sampler = RegularGridSampler([1.], [2.])\nx_sampler = RegularGridSampler([0.], [1.])\nn_sensors = 32\nn_observations = 128\ndataset = FunctionOperatorDataset(\nU, x_sampler, n_sensors,\nV, x_sampler, n_sensors,\na_sampler, n_observations,\n)\n</code></pre> <pre>\n<code>OperatorShapes(x=TensorShape(dim=1, size=torch.Size([32])), u=TensorShape(dim=1, size=torch.Size([32])), y=TensorShape(dim=1, size=torch.Size([32])), v=TensorShape(dim=1, size=torch.Size([32])))\n</code>\n</pre> <p>Split data set into training, validation and test set.</p> <pre><code>from continuiti.data.utility import split\ntrain_dataset, test_val_dataset = split(dataset, 0.75)\nval_dataset, test_dataset = split(test_val_dataset, 0.5)\n</code></pre> <pre>\n<code>len(train_dataset) = 96  len(val_dataset) = 16  len(test_dataset) = 16\n</code>\n</pre> <pre><code>from continuiti.operators import DeepONet\noperator = DeepONet(shapes=dataset.shapes, trunk_depth=8)\n</code></pre> <pre><code>from continuiti.trainer import Trainer\nfrom continuiti.trainer.callbacks import LearningCurve\ntrainer = Trainer(operator)\ntrainer.fit(\ntrain_dataset,\ntol=1e-3,\ncallbacks=[LearningCurve()],\ntest_dataset=val_dataset,\n)\n</code></pre> <pre>\n<code>Parameters: 11152  Device: mps\nEpoch 610/1000  Step 3/3  [====================]  8ms/step  ETA 0:10min - loss/train = 8.5058e-04  loss/test = 9.7409e-04 - stopping criterion met\n</code>\n</pre> <pre><code>x, u, y, v = val_dataset[0:1]\ny_plot = torch.linspace(0, 1, 100).reshape(1, 1, -1)\nv_pred = operator(x, u, y_plot)\n</code></pre> <p>Let us evaluate some training metrics, e.g., a validation error.</p> <pre><code>from continuiti.data import dataset_loss\nloss_train = dataset_loss(train_dataset, operator)\nloss_val = dataset_loss(val_dataset, operator)\nloss_test = dataset_loss(test_dataset, operator)\n</code></pre> <pre>\n<code>loss/train = 8.0337e-04\nloss/val   = 9.7409e-04\nloss/test  = 6.4642e-04\n</code>\n</pre> <p>As you can observe, the neural operator is able to learn the operator \\(G\\) and generalizes well to unseen data. That's the basics!</p>"},{"location":"tutorials/training/#training","title":"Training","text":"<p>This notebook demonstrates the basics of training a neural operator in continuiti.</p>"},{"location":"tutorials/training/#operator","title":"Operator","text":"<p>Given two sets of functions \\(U\\) and \\(V\\), assume we want to learn an operator</p> \\[\\begin{align*} G: U &amp;\\to V, \\\\    u &amp;\\mapsto v, \\end{align*}\\] <p>that maps functions \\(u \\in U\\) to functions \\(v \\in V\\).</p> <p>In this example, we choose to learn the operator that maps the set of functions</p> \\[ U = \\{ u_a(x) = \\sin(a \\pi x) \\mid a \\in [1, 2] \\} \\] <p>to the set of functions</p> \\[ V = \\{ v_a(x) = a \\pi \\cos(a \\pi y) \\mid a \\in [1, 2] \\}, \\] <p>such that \\(G(u_a) = v_a\\). Let's consider \\(x, y \\in [0, 1]\\) and start with the visualization of some functions in \\(U\\) and \\(V\\).</p>"},{"location":"tutorials/training/#discretization","title":"Discretization","text":"<p>Operator learning is about learning mappings between infinite dimensional spaces. To work with infinite-dimensional objects numerically, we have to discretize the input and output function somehow. In continuiti, this is done by point-wise evaluation.</p> <p>Discretized functions can be collected in an <code>OperatorDataset</code> for operator learning. The <code>OperatorDataset</code> is a container of discretizations of input-output functions. It contains tuples (<code>x</code>, <code>u</code>, <code>y</code>, <code>v</code>) of tensors, where every sample consists of</p> <ul> <li>the sensor positions <code>x</code>,</li> <li>the values <code>u</code> of the input function at the sensor positions,</li> <li>the evaluation points <code>y</code>, and</li> <li>the values <code>v</code> of the output functions at the evaluation points.</li> </ul> <p>If we already have a <code>FunctionSet</code>, we can use the <code>FunctionOperatorDataset</code> to sample elements \\(u \\in U\\) and \\(v \\in V\\) evaluated at sampled positions.</p>"},{"location":"tutorials/training/#neural-operator","title":"Neural Operator","text":"<p>In order to learn the operator \\(G\\) with a neural network, we can train a neural operator.</p> <p>A neural operator \\(G_\\theta\\) takes an input function \\(u\\), evaluated at sensor positions \\(x\\), and maps it to a function \\(v\\) evaluated at (possibly different) evaluation points \\(y\\), such that $$ v(y) = G(u)(y) \\approx G_\\theta\\left(x, u(x), y\\right). $$</p> <p>In this example, we train a DeepONet, a common neural operator architecture motivated by the universal approximation theorem for operators.</p>"},{"location":"tutorials/training/#training_1","title":"Training","text":"<p>continuiti provides the <code>Trainer</code> class which implements a default training loop for neural operators. It is instantiated with an <code>Operator</code>, an optimizer (<code>Adam(lr=1e-3)</code> by default),  and a loss function (<code>MSELoss</code> by default).</p> <p>The <code>fit</code> method takes an <code>OperatorDataset</code> and trains the neural operator  up to a given tolerance on the training data (but at most for a given number of <code>epochs</code>, 1000 by default).</p>"},{"location":"tutorials/training/#evaluation","title":"Evaluation","text":"<p>The mapping of the trained operator can be evaluated at arbitrary positions, so let's plot the prediction of \\(G_\\theta\\) on a fine resolution along with the target function.</p>"}]}